%!TEX root = forallxyyc.tex
\graphicspath{{figures--proofs/}}
\part{Natural deduction for TFL}
\label{ch.NDTFL}
\addtocontents{toc}{\protect\mbox{}\protect\hrulefill\par}

\chapter{Natural deduction}\label{s:NDVeryIdea}

\section{Natural deduction versus truth tables}

In \S\ref{s:Valid-def}, we said that an argument is valid if and only if it is impossible for all of the premises to be true and the conclusion to be false. 

In the case of TFL, this led us to develop truth tables. Each line of a complete truth table corresponds to a valuation. So, given an argument in TFL, we have a very direct way to assess whether it is possible to make all of the premises true and the conclusion false: just investigate the truth table.

However, truth tables do not necessarily give us much \emph{insight}. Consider this argument:
%\begin{align*}
$$(P \eand Q) \eor R, \enot R \proves Q$$
%W \eif (T \eand S), W & \therefore T
%\end{align*}
This is a valid argument, and you can confirm that it is by constructing a four-line truth table. But we might want to know \textit{why} it is valid---that is, why (or how) the conclusion follows from the premises. 

One aim of a \emph{natural deduction system} is to show that particular arguments are valid and why they are valid. That is to say, the system allows us to make explicit the reasoning process that get us from the premises to the conclusion. We begin with very basic rules of inference. These rules can be combined, and with just a small number of them, we hope to be able to explicate all of the valid arguments that can be represented in TFL. There are different deduction systems that can be used with TFL. A \textit{natural} deduction system is one that, for the most part, reflects the reasoning processes that we all typically use---at least insofar as the reasoning involves`and', `or', `not', `if \ldots, then \ldots', and `if and only if'.

This is a different way of thinking about arguments. With truth tables, we directly consider different scenarios where the atomic sentences are true or false and see what that means for the premises and conclusion. With natural deduction systems, we manipulate the sentences in accordance with rules that we have set down. This gives us a better insight---or at least, a different insight---into how arguments work.

In addition to giving us insight, at least some of the time, using a natural deduction system to demonstrate that an argument is valid is much easier than using a truth table. Take, for instance, this argument:
$$A \eand B \proves (A \eor C) \eand (B \eor D)$$
To test this argument for validity with a truth table, you need 16 lines. If you do it correctly, then you will see that there is no line on which all the premises are true and on which the conclusion is false. So you will know that the argument is valid. (But, as just mentioned, there is a sense in which you will not know \emph{why} the argument is valid.) On the other hand, using our natural deduction system, you can demonstrate that this argument is valid in six lines. (And after reading \S\ref{s:conj-rule} and \S\ref{s:disj-rule}, you'll be able to do it easily.) 

When an argument contains more letters, it gets even more difficult to use truth tables (since the number of lines needed is $2^{n}$ where $n=$ the number of letters). In principle, we can set a computer to grind through truth tables and report back when it is finished. But, in practice, complicated arguments in TFL can become \emph{intractable} if we use truth tables. %(Naturally, the reverse is also sometimes true. We can show that $\enot(P \eif Q) \therefore (P \eand \enot Q)$ is valid with a four-line truth table, but it takes 17 not-so-easy lines to show that it is valid with our natural deduction system.)


\section{Truth functional propositional logic}

As you know, the symbols of TFL are the sentence letters that represent atomic sentences, the logical operators $\enot$, $\eand$, $\eor$, $\eif$, and $\eiff$, and brackets. These, then, can be combined into sentences using the rules given in chapter \ref{s:TFLSentences}. 

Truth tables are a method for setting the meaning of the logical operators. That begins with the characteristic truth table for each logical operator and then can be expanded for any sentence in TFL. Truth tables, as it works out, also give us a method for verifying that an argument satisfies the definition of \textit{valid}. (\textit{Valid} is a concept and is not, strictly speaking a part of TFL. Rather it is a concept---or a property of arguments---that can, to an extent, be studied and explicated using TFL. Similarly, as you have seen, \textit{tautology}, \textit{contradiction}, \textit{contingent}, \textit{equivalent}, and \textit{jointly consistent} can be explicated using TFL.) 

Whereas truth tables set the meaning for each logical operator, a system of natural deduction sets the rules for how sentences containing the logical operators can be combined or taken apart. (This, in effect, establishes the syntax for the logical operators.) This gives us a method for (1) confirming that an argument is valid and (2) showing why it is valid in a manner that, more or less, conforms to our natural reasoning processes. 


%\begin{center}
	%$\diamondsuit\quad\diamondsuit\quad\diamondsuit$
    %$\blacksquare\quad\blacksquare\quad\blacksquare$
%\end{center}

\section{Fitch} The modern development of natural deduction dates from simultaneous but unrelated papers by Gerhard Gentzen and Stanisław Jaśkowski that were published in 1934. The natural deduction system that we will use, however, is based largely on work by Frederic Fitch, which was first published in 1952. Consequently, the format that we will use for writing proofs is called \textit{Fitch notation}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\chapter{The rules of derivation}\label{s:BasicTFL}

\section{Proofs}

A \define{proof} is a list of sentences. The sentence or sentences at the beginning of the list are assumptions. These are the premises of the argument. (We call them \textit{assumptions} because, at least within the proof, they do not require any justification. We just state them.) Every other sentence in the sequence follows from earlier sentences by a specific rule. The final sentence is the conclusion of the argument.

As an illustration, consider this argument:
	$$\enot (A \eor B) \proves \enot A \eand \enot B$$
We start the proof by numbering the line and writing the premise:
\begin{proof}
	\hypo{a1}{\enot (A \eor B)} \pr{}
\end{proof}
Every line in a proof is numbered so that we can refer to it later if we need to do so. We have also indicated that this is a premise by putting `PR' at the end of the line. And we have drawn a line underneath the premise. Everything written above the line is an \emph{initial assumption} (i.e., a premise). Everything written below the line will either be a sentence that can be derived from that assumption, or it will be a new assumption that we introduce. The colon that is right before `PR' is, technically, optional, but it has to be used in Carnap to separate the TFL sentence from the `PR' (or the rule) that is written at the end of each line.


The conclusion of this argument is `$\enot A \eand \enot B$'; and so we want our proof to end---on some line, we'll call it $n$---with that sentence:
\begin{proof}
	\hypo{a1}{\enot (A \eor B)} \pr{}
	\have{a2}{\ldots}
	\have[ ]{}{\ldots}
	\have[ ]{}{\ldots}
	\have[n]{con}{\enot A \eand \enot B}
\end{proof}
It doesn't matter what line number we end on, but, all things considered, we would prefer a shorter proof to a longer one.

Suppose we have this argument:
$$A\eor B, \enot (A\eand C), \enot (B \eand \enot D) \proves \enot C\eor D$$
This argument has three premises, and so we start by listing them, numbering each line, and drawing a line under the final premise:
\begin{proof}
	\hypo{a1}{A \eor B} \pr{}
	\hypo{a2}{\enot (A\eand C)} \pr{}
	\hypo{a3}{\enot (B \eand \enot D)} \pr{}
\end{proof}
This, meanwhile, will be the final line of the proof:
\begin{proof}
	\have[n]{con}{\enot C \eor D}
\end{proof}
Setting up the premises and the conclusion is, however, the easy part. The real task---and the interesting part---is explaining each of the steps that get us from the premises to the conclusion. 

To construct proofs, we will develop a \define{natural deduction} system. In the natural deduction system, there are two rules for each logical operator: an \define{introduction} rule, which allows us to derive a new sentence that has the logical operator as the main connective, and an \define{elimination} rule, which allows us to extract a subsentence from a sentence that has that logical operator as the main connective. (Table \ref{table.TFL-rules} contains a list of the rules.) These rules can then be combined to demonstrate each step that must be taken to get from the premises to the conclusion. All of the rules introduced in this chapter are also summarized on pp.~\pageref{ProofRules} - \pageref{ProofRules-end}.


\begin{table*}\centering\sffamily\footnotesize
\ra{1.25}
\begin{tabular}{@{}l l@{}}\toprule
\textsc{The rules for truth functional logic} & \\\midrule
conjunction introduction rule & conjuction elimination rule \\
disjunction introduction rule & disjunction elimination rule \\
conditional introduction rule & conditional elimination rule \\
biconditional introduction rule & biconditional elimination rule \\
negation introduction rule & negation elimination rule \\
reiteration rule & double negation rule\\
\bottomrule
\end{tabular}
\caption{}\label{table.TFL-rules}
\end{table*}



\section{Conjunction introduction and elimination}\label{s:conj-rule}

Suppose we want to show that \textit{Sarah is swimming and Amy is reading}. One way to do this would be as follows: first we show that Sarah is swimming; next, we show that Amy is reading; then we put the two together to obtain the conjunction \textit{Sarah is swimming and Amy is reading}.

This reasoning process, which we all do naturally, is part of our natural deduction system. It is implemented with the \define{conjunction introduction rule}. 
\begin{factboxy}{conjunction introduction rule}
\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[n]{b}{\meta{B}}
	\have[\ ]{c}{\meta{A}\eand\meta{B}} \ai{a, b}
\end{proof}
\tcblower
\footnotesize{Note that `$m$' and `$n$' will never appear in an actual proof. In a proof, the lines are numbered `$1$', `$2$', `$3$', and so forth. When we define the rule, we use variables to emphasize that the $\meta{A}$ and the $\meta{B}$ can be on any lines in the proof (and in any order), as long as they are before the application of the rule.}
\end{factboxy}

For the example above, we can adopt this symbolization key:
	\begin{ekey}
		\item[S] Sarah is swimming.
		\item[R] Amy is reading.
	\end{ekey}
Although, as you will see, the lines that we use to apply the rules can be anywhere in the proof, let's say that `$S$' and `$R$' are our premises, and so they are on lines 1 and 2. Then on any subsequent line---but, in this case, it will be line 3---we can get `$S \eand R$' by using the conjunction introduction rule ($\eand$I).
\begin{proof}
	\hypo{a}{S} \pr{}
	\hypo{b}{R} \pr{}
	\have{c}{S \eand R} \ai{a, b}
\end{proof}

Every line of our proof must either be an assumption (and a premise is an assumption), or it must be justified by some rule. Therefore, on line 3, we put the citation `$\eand$I 1, 2' to indicate that `$S \eand R$' was obtained by applying the conjunction introduction rule to lines 1 and 2. The `$\meta{A}$' and `$\meta{B}$', which were on lines 1 and 2, can occur in either order, and the conjunction that we derive can be `$\meta{A} \eand \meta{B}$' or `$\meta{B} \eand \meta{A}$'.

As you saw, the conjunction \emph{introduction} rule introduces the connective `$\eand$' into our proof. Correspondingly, we have a rule that \emph{eliminates} that connective.  Suppose you have shown that \textit{Jeff is eating and Mary is sleeping}. You are entitled to conclude that Jeff is eating. Equally, you are entitled to conclude that Mary is sleeping. Putting this together, we obtain our \define{conjunction elimination rule} (which is actually two similar rules).

\begin{factboxy}{conjunction elimination rule}
\begin{proof}
	\have[m]{ab}{\meta{A}\eand\meta{B}}
	\have[\ ]{a}{\meta{A}} \ae{ab}
\end{proof}
\textit{and equally:}
\begin{proof}
	\have[m]{ab}{\meta{A}\eand\meta{B}}
	\have[\ ]{b}{\meta{B}} \ae{ab}
\end{proof}
\end{factboxy}
\noindent When you have a conjunction on some line of a proof, you can use the conjunction elimination rule to obtain either of the conjuncts on a new line. You can only, however, apply this rule when the `$\eand$' is the main logical operator. So, for instance, you cannot use the conjunction elimination rule to obtain `$D$' from `$C \eor (D \eand E)$'.

With just these two rules, we can start to see how our formal proof system works. Consider:
\begin{earg}
\item[] $(A\eor B) \eand (G \eand H) \proves (A\eor B)\eand H$
\end{earg}
The main logical operator in both the premise and conclusion of this argument is `$\eand$', and so we will use both of our conjunction rules in this proof. We begin by writing down the premise, and we draw a line below it. Everything after this line must follow from our premise by the application of our rules. 
\begin{proof}
	\hypo{ab}{(A\eor B) \eand (G\eand H)} \pr{}
\end{proof}
From the premise, we can eliminate the main connective (and only the main connective) using {\eand}E. Using $\eand$E twice gives us this:
\begin{proof}
	\hypo{ab}{(A\eor B) \eand (G\eand H)} \pr{}
	\have{a}{(A\eor B)} \ae{ab}
	\have{b}{(G\eand H)} \ae{ab}
\end{proof}
Now that $(G\eand H)$ is on its own line, we can use $\eand$E again to get $H$ on a line by itself. 
\begin{proof}
	\hypo{ab}{(A\eor B) \eand (G\eif H)} \pr{}
	\have{a}{(A\eor B)} \ae{ab}
	\have{b}{(G\eand H)} \ae{ab}
	\have{b2}{H} \ae{b}
\end{proof}
In our final step we use the conjunction introduction rule to get the conclusion, $(A\eor B)\eand H$.
\begin{proof}
	\hypo{ab}{(A\eor B) \eand (G\eif H)} \pr{}
	\have{a}{(A\eor B)} \ae{ab}
	\have{b}{(G\eand H)} \ae{ab}
	\have{b2}{H} \ae{b}
	\have{c}{(A\eor B)\eand H} \ai{a,b2}
\end{proof}
And we're done. Notice that there is nothing in this representation of the proof to indicate that the last line is the conclusion. It's only because we began with `$(A\eor B) \eand (G \eand H) \proves (A\eor B)\eand H$' that we know that we have arrived at the conclusion that we wanted.
 

\section{Disjunction introduction and elimination}\label{s:disj-rule}
Suppose that Sarah is swimming. Then the sentence `Sarah is swimming or Sarah is working' is true. After all, to say that `Sarah is swimming or she is working' is to say something weaker than `Sarah is swimming'. (For the sentence, `Sarah is swimming' to be true, it must be the case that Sarah is swimming. For the sentence `Sarah is swimming or she is working' to be true, it must be the case that Sarah is swimming \textit{or} it must be the case that Sarah is working. It doesn't have to be the case, although it can be, that she is, somehow, swimming and working.)

Let's emphasize this point. Suppose Sarah is swimming. It follows that \emph{either} Sarah is swimming \emph{or} she is a witch. Equally, it follows that \emph{either} Sarah is swimming \emph{or} the Queen of England is on the moon. These are strange inferences to draw from `Sarah is swimming', but there is nothing logically wrong with them. (They may violate some implicit conversational norms, but they don't violate the truth conditions for `or'. Just check the characteristic truth table for the disjunction.)

The idea is that if we know that a sentence is true, we can create a longer sentence by adding `or \ldots \textit{whatever we want}' and this new sentence will also be true. This feature of the disjunction gives us the \define{disjunction introduction rule} (which, again, is two similar rules).

\begin{factboxy}{disjunction introduction rule}
\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[\ ]{ab}{\meta{A}\eor\meta{B}}\oi{a}
\end{proof}

\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[\ ]{ba}{\meta{B}\eor\meta{A}}\oi{a}
\end{proof}
\end{factboxy}


\noindent \meta{B} can be \emph{any} sentence whatsoever. The only line that we need before we use this rule is the one containing \meta{A}. Hence, the following is a perfectly acceptable proof:
\begin{proof}
	\hypo{m}{M} \pr{}
	\have{mmm}{M \eor [(A\eiff B) \eif (C \eand D)]}\oi{m}
\end{proof}

The \define{disjunction elimination rule}, on the other hand, requires citing more than one line. Let's say that this sentence is true: \textit{Amy is a chef or she is a rock climber}. Do we know that Amy is a chef? No. The sentence \textit{Amy is a chef or she is a rock climber} might be true because \textit{Amy is a chef} is true, but it might be true because only \textit{Amy is a rock climber} is true. Or, since this is the inclusive-or, it's also possible that \textit{Amy is a chef} and \textit{Amy is a rock climber} are both true. The problem is that we don't know anything except that Amy is a chef or she is a rock climber. 

To make an inference from \textit{Amy is a chef or she is a rock climber}, we also need the denial of one of the disjuncts---for instance, \textit{Amy is \textbf{not} a chef}. If we know that \textit{Amy is a chef or she is a rock climber} and that \textit{Amy is not a chef}, then we can safely conclude that \textit{Amy is a rock climber}. That is an application of the disjunction elimination rule ({\eor}E).

\begin{factboxy}{disjunction elimination rule}
\begin{proof}
	\have[m]{ab}{\meta{A}\eor\meta{B}}
	\have[n]{nb}{\enot\meta{B}}
	\have[\ ]{a}{\meta{A}} \oe{ab,nb}
\end{proof}

\begin{proof}
	\have[m]{ab}{\meta{A}\eor\meta{B}}
	\have[n]{na}{\enot\meta{A}}
	\have[\ ]{b}{\meta{B}} \oe{ab,nb}
\end{proof}
\end{factboxy}

\section{Conditional elimination}

For the conditional, we will begin with the elimination rule since it is simpler than the conditional introduction rule. Consider the following argument:
	\begin{quote}
		If Jane is smart, then she is fast.\\
		Jane is smart.\\ 
		Therefore, Jane is fast.
	\end{quote}
In this argument---which is valid---we have a conditional and then, on a separate line, the antecedent of that conditional (`Jane is smart'). That allows us to infer the antecedent (`Jane is fast'). In short, if we have a conditional and we know that the antecedent of the conditional is true (or has happened), then we know that the consequent has to be true. (See also the discussion of the conditional on p.~\pageref{characteristic-tt-conditional}.) Deriving the consequent of the conditional in this way is an application of the conditional elimination rule ($\eif$E).
\begin{factboxy}{conditional elimination rule}\label{ce-rule}
\begin{proof}
	\have[m]{ab}{\meta{A}\eif\meta{B}}
	\have[n]{a}{\meta{A}}
	\have[\ ]{b}{\meta{B}} \ce{ab,a}
\end{proof}
\end{factboxy}
This rule is also sometimes called \emph{modus ponens}. When we use the rule, the conditional and the antecedent of the conditional can be separated from one another, and they can appear in any order.


\section{Biconditional introduction and elimination} 

The \define{biconditional elimination rule} ({\eiff}E) is similar to the conditional elimination rule but a bit more flexible. If you have the left-hand subsentence of the biconditional, you can derive the right-hand subsentence. If you have the right-hand subsentence, you can derive the left-hand subsentence.
\begin{factboxy}{biconditional elimination rule}\label{be-rule}
\begin{proof}
	\have[m]{ab}{\meta{A}\eiff\meta{B}}
	\have[n]{a}{\meta{A}}
	\have[\ ]{b}{\meta{B}} \be{ab,a}
\end{proof}

\begin{proof}
	\have[m]{ab}{\meta{A}\eiff\meta{B}}
	\have[n]{a}{\meta{B}}
	\have[\ ]{b}{\meta{A}} \be{ab,a}
\end{proof}
\end{factboxy}

The \define{biconditional introduction rule} ($\eiff$I) is based on the fact, mentioned in chapter \ref{s:CharacteristicTruthTables}, that the biconditional is ``the conjunction of a conditional running in each direction.'' The rule then is basically this: from $\meta{A} \eif \meta{B}$ and $\meta{B} \eif \meta{A}$, infer $\meta{A} \eiff \meta{B}$.

\begin{factboxy}{biconditional introduction rule}
\begin{proof}\label{eiff-I}
    \have[m]{ab}{\meta{A}\eif\meta{B}} 
    \have[n]{ba}{\meta{B}\eif\meta{A}}
	\have[\ ]{b}{\meta{A}\eiff\meta{B}} \bi{ab,ba}
\end{proof}
\end{factboxy}

%The order of the $\meta{A}$ and $\meta{B}$ in $\meta{A}\eiff\meta{B}$ has to match their order in the first conditional in the conjunction, but since, typically, that line will have to be generated using the conjunction introduction rule, those conjuncts  can be put in either order then. \textbf{Also, notice that when we cite the rule, we use $\eiff$ex, not $\eiff$I.}


\section{Some examples}

We will now review some proofs that use the rules that we have learned so far. 

\begin{earg}
\item[\ex{14.6.1}] First, a proof of $P \eand Q, \enot R \proves Q \eand \enot R$ requires the conjunction introduction rule and the conjunction elimination rule.
\begin{proof}
	\hypo{p1}{P \eand Q} \pr{}
	\hypo{p2}{\enot R} \pr{}
	\have{q}{Q} \ae{p1}
	\have{c}{Q \eand \enot R} \ai{p2,q}
\end{proof}\medskip

\item[\ex{14.6.2}]For a proof of $P \eor Q, R \eand \enot Q \proves P \eor S$, we need the conjunction elimination rule, disjunction introduction rule, and the disjunction elimination rule.
\begin{proof}
	\hypo{p1}{P \eor Q} \pr{}
	\hypo{p2}{R \eand \enot Q} \pr{}
	\have{q}{\enot Q} \ae{p2}
	\have{p}{P} \oe{p1,q}
	\have{c}{P \eor S} \oi{p}
\end{proof}\medskip

\item[\ex{14.6.3}] For a proof of $P \eif Q, R \eand P \proves Q \eand R$, we use the conjunction introduction rule, the conjunction elimination rule, and the conditional elimination rule.
\begin{proof}
	\hypo{p1}{P \eif Q} \pr{}
	\hypo{p2}{R \eand P} \pr{}
	\have{p}{P} \ae{p2}
	\have{q}{Q} \ce{p1,p}
	\have{c}{Q \eand R} \ai{p,q}
\end{proof}\medskip

\item[\ex{14.6.4}] For a proof of $R \eiff T, P \eor T, \enot P \proves R$, we the disjunction elimination rule and the biconditional elimination rule.
\begin{proof}
	\hypo{p1}{R \eiff T} \pr{}
	\hypo{p2}{P \eor T} \pr{}
	\hypo{p3}{\enot P} \pr{}
	\have{t}{T} \oe{p2,p3}
	\have{c}{R} \be{p1,t}
\end{proof}\medskip

\item[\ex{14.6.5}] And last, a proof for $$(S \eif T) \eor R, (T \eif S) \eor Q, \enot R \eand \enot Q \proves T \eiff S$$ requires the conjunction elimination rule, the disjunction elimination rule, and the biconditional introduction rule.
\begin{proof}
	\hypo{p1}{(S \eif T) \eor R} \pr{}
	\hypo{p2}{(T \eif S) \eor Q} \pr{}
	\hypo{p3}{\enot R \eand \enot Q} \pr{}
	\have{r}{\enot R} \ae{p3}
	\have{q}{\enot Q} \ae{p3}
	\have{st}{S \eif T} \oe{p1,r}
	\have{ts}{T \eif S} \oe{p2,q}
%	\have{con}{(T \eif S) \eand (S \eif T)} \ai{st,ts}
	\have{c}{T \eiff S} \bi{st,ts}
\end{proof}\medskip
\end{earg}

\section{Conditional introduction}

The \define{conditional introduction rule} is a little bit more complicated than the conditional elimination rule, but, with some thought (and maybe some time), it is easily grasped. We'll start with this symbolization key for the sentence letters \textit{L}, \textit{R}, and \textit{T}:
	\begin{ekey}
		\item[T] Today is Tuesday.
		\item[L] Kate has logic class today.
		\item[R] It is raining.
	\end{ekey}
And this is our argument: 
$$T \eand L \proves R \eif L$$
 
Maybe you can see that this argument is valid. (That is, you can see that if the premise is true, then the conclusion has to be true.) But if you can't right now, that's ok. We will go through the proof for this argument, and in the process explain the conditional introduction rule. We start by listing the premise.
	\begin{proof}
		\hypo{r}{T \eand L} \pr{}
	\end{proof}
Next, we need to make a new assumption: `It is raining'. (We might say that we're making this assumption ``for the sake of argument'' or to see where it leads). To indicate that this is an assumption that we have supplied, we put `$R$' on line 2 this way:
	\begin{proof}
		\hypo{r}{T \eand L} \pr{}
		\open
			\hypo{l}{R} \as{}
	\end{proof}
You will notice right away that the `$R$' is indented and not against the same vertical line as the `$T \eand L$'. Whenever we make an assumption ourselves, we must indent it and the lines that follow. This creates a \define{subproof} that is set off from the rest of the proof. The assumption is cited with `AS', and we put a line under the assumption just as we do with the premises.

With this extra assumption in place, we next use $\eand$E to get $L$ on a line by itself.
	\begin{proof}
		\hypo{r}{T \eand L} \pr{}
		\open
			\hypo{l}{R} \as{}
			\have{rl}{L}\ae{r}
	\end{proof}
The idea for the first three lines of this proof are, first, we know that \textit{today is Tuesday and Kate has logic class today} (or, at least, we are assuming that `$ T \eand L$' is true). Next, on line two, we are in effect saying, ``What if \textit{it is raining} is true?'' That is, what will follow if we make that assumption? Well, one thing that will follow is that Kate still has logic class today. That was true before we made our assumption about it raining. Nothing in our proof tells us that if \textit{it is raining} is true, then Kate won't have logic class today. Hence, \textit{Kate has logic class today} is true and can go on line 3. (Notice that \textit{today is Tuesday} is also true and could go on line 3, but that won't get us any closer to the conclusion that we want.)

So, again, on line 2, we are, in a sense, asking, What if \textit{it is raining}? On line 3, we have one answer: \textit{Kate has logic class today}. Therefore, on line 4, we can put these two together as `if it is raining, then Kate has logic class today' with the conditional introduction rule.
	\begin{proof}
		\hypo{r}{T \eand L} \pr{}
		\open
			\hypo{l}{R} \as{}
			\have{rl}{L}\ae{r}
			\close
		\have{con}{R \eif L}\ci{l-rl}
	\end{proof}
For this final step, we have dropped back to the original vertical line. When we introduce the conditional, we are \emph{discharging} the assumption that we made (`$R$') and closing the subproof.

That is a simple argument, but it illustrates how we create a conditional in a proof. When we use the conditional introduction rule, the assumption that make will always be the antecedent of the conditional. The last line of the subproof, meanwhile, will always be the consequent of the conditional. 
% Similarly, we could have put `$\enot R$' on line 2, in which case our conditional would have been `if it is not raining, then Kate has logic today'. The point is that, on line 2, we are making an assumption, and then we see what follows. Since `Kate has logic class today' is in the conjunction on line 1, it will follow no matter what the assumption is.
To summarize, first, we make an assumption, \meta{A}. From that assumption, we derive \meta{B}. Once we've done that, we know that \textit{if} \meta{A}, then \meta{B}, and we have our conditional. 
\begin{factboxy}{conditional introduction rule}
	\begin{proof}
		\open
			\hypo[i]{a}{\meta{A}} \as{}
			\have[j]{b}{\meta{B}}
		\close
		\have[\ ]{ab}{\meta{A}\eif\meta{B}}\ci{a-b}
	\end{proof}
\footnotesize{There can be as many or as few lines as needed between lines $i$ and $j$.} 
\end{factboxy}

Lines $i$ through $j$ are called a \define{subproof}, and once a subproof has been closed, none of the lines in the subproof can be used again. The conditional $\meta{A}\eif\meta{B}$ can be used later in the proof (if it's needed) because it is outside of the subproof. Also, a proof is not complete until every assumption that has been introduced has been discharged. That is to say, every subproof must be closed by the application of the $\eif$I rule (or, as we will see shortly, the $\enot$I or $\enot$E rules).
Thus, we stipulate the following.
\begin{earg}
\item[1.] To cite individual lines when applying a rule, those lines must (a) come before the application of the rule, but (b) not occur within a closed subproof.
\item[2.] A proof is not complete until every additional assumption (not counting the premises) is discharged.
\end{earg}


\section{Some more examples}

Let's go through a few more examples, each of which uses the conditional introduction rule.

\begin{earg}
\item Suppose we want a proof of this argument:
	$$P \eif Q, Q \eif R \proves P \eif R$$
We start by listing both of our premises. Next, since we want ($P \eif R$), we assume the antecedent of that conditional. 
\begin{proof}
	\hypo{pq}{P \eif Q} \pr{}
	\hypo{qr}{Q \eif R} \pr{}
	\open
		\hypo{p}{P} \as{}
	\close
\end{proof}
Now, even though it is an assumption that we've introduced, since `$P$' is on a line by itself (and the subproof has not yet been closed), we can use it for our next step. With `$P$' and the `$P \eif Q$' on line 1, we can use {\eif}E to get `$Q$'. 
\begin{proof}
	\hypo{pq}{P \eif Q} \pr{}
	\hypo{qr}{Q \eif R} \pr{}
	\open
		\hypo{p}{P}\as{}
		\have{q}{Q}\ce{pq,p}
%		\have{r}{R}\ce{qr,q}
	\close
%	\have{pr}{P \eif R}\ci{p-r}
\end{proof}
With the $Q$ on line 4 and $Q \eif R$ on line 2, we can use {\eif}E and get $R$. So, by assuming `$P$', we were able to get `$R$'. Last, we apply the {\eif}I rule, which discharges our assumption and completes the proof.
\label{HSproof}
\begin{proof}
	\hypo{pq}{P \eif Q} \pr{}
	\hypo{qr}{Q \eif R} \pr{}
	\open
		\hypo{p}{P}\as{}
		\have{q}{Q}\ce{pq,p}
		\have{r}{R}\ce{qr,q}
	\close
	\have{pr}{P \eif R}\ci{p-r}
\end{proof}

\item Consider how we would prove: $F \eif (G \eand H) \proves F \eif G$. Perhaps it is tempting to write down the premise and then apply the {\eand}E rule to the conjunction $(G \eand H)$. This is not allowed, however. \textbf{The rules of proof can only be applied to the main connective of a sentence.} (That's the `$\eif$' in this sentence, not the `$\eand$'.) To use $\eand$E, we need to get $(G \eand H)$ on a line by itself, and so we proceed this way:

\begin{proof}
	\hypo{fgh}{F\eif(G\eand H)} \pr{}
	\open
		\hypo{f}{F}\as{}
		\have{gh}{G \eand H}\ce{fgh,f}
		\have{g}{G}\ae{gh}
	\close
	\have{fg}{F \eif G}\ci{f-g}
\end{proof}

\item As you know, the biconditional elimination rule is similar to the conditional elimination rule. (But they are not the same. See p.~\pageref{ce-rule} and p.~\pageref{be-rule} to compare them.) We should also, however, be able to go from a biconditional, say `$B \eiff C$', to a conditional: `$B \eif C$' or `$C \eif B$'. This is easily done with the conditional introduction rule.
\begin{proof}
	\hypo{bc}{B \eiff C} \pr{}
	\open
		\hypo{b}{B}\as{}
		\have{c}{C}\be{bc,b}
	\close
	\have{bc2}{B \eif C}\ci{b-c}
\end{proof}
\smallskip
\noindent And, with a similar proof, we can also derive `$C \eif B$'.

%\begin{notebox}
%In fact, in some logic textbooks, this is biconditional elimination rule:
%\begin{proof}\label{eiff-I}
%    \have[m]{ab}{\meta{A} \eiff \meta{B}} 
%	\have[\ ]{b}{\meta{A}\eif\meta{B}} \be{ab}
%\end{proof}
%\begin{proof}\label{eiff-I}
%    \have[m]{ab}{\meta{A} \eiff \meta{B}}
%	\have[\ ]{b}{\meta{B}\eif\meta{A}}\be{ab}
%\end{proof}
%\end{notebox}

\end{earg}


\section{Negation introduction and elimination}
Here is a simple mathematical argument in English:
\begin{earg}
\item[1.] Assume that there is some greatest natural number. Call it $G$.
\item[2.] That number plus one is also a natural number.
\item[3.] $G+1$ is greater than $G$.
\item[4.] Thus, $G$ is the greatest natural number (accordingt to 1), and there is a natural number greater than $G$ (according to 3).
\item[5.] The previous line is a contradiction.
\item[6.] Therefore, the assumption that we began with is false. There is no greatest natural number.
\end{earg}
This type of argument is traditionally called a \emph{reductio}. Its full Latin name is \emph{reductio ad absurdum}, which means `reduction to absurdity' (although \textit{absurdity} in the sense that we often use it today isn't part of this). In a reductio, we assume something for the sake of argument---for example, that there is a greatest natural number. Then we show that the assumption leads to two contradictory sentences---for example, that $G$ is the greatest natural number and that it is not. In this way, we show that the original assumption must be false, which means that the denial of the assumption is true. 

Our two negation rules (which are basically the same rule) formalize this reasoning process.

\begin{factboxy}{negation introduction rule}
\begin{proof}
\open
	\hypo[m]{na}{\meta{A}}\as{}
	\have[n]{b}{\meta{B}}
	\have[p]{nb}{\enot\meta{B}}
\close
\have[\ ]{a}{\enot\meta{A}}\ni{na-nb}
\end{proof}
\end{factboxy}

\noindent Notice that just as we do when using the conditional introduction rule, we begin by making an assumption. The subproof that follows is indented, and the assumption that we made must be discharged by applying either $\enot$I or $\enot$E.

\begin{factboxy}{negation elimination rule}
\begin{proof}
\open
	\hypo[m]{na}{\enot\meta{A}}\as{}
	\have[n]{b}{\meta{B}}
	\have[p]{nb}{\enot\meta{B}}
\close
\have[\ ]{a}{\meta{A}}\ne{na-nb}
\end{proof}
\end{factboxy}

When using either of the negation rules, the last two lines of the subproof must be an explicit contradiction: \meta{B} on one line and its negation, $\enot\meta{B}$, on the next line (or vice versa). Those two lines cannot be separated. When you cite the rule, however, the lines that you give are the lines for the whole subproof (starting with the assumption), not just the two lines containing the contradiction. 

%To see how the negation ... rule works, suppose we want to prove the law of non-contradiction: $\enot(G \eand \enot G)$. We can prove this without any premises by immediately starting a subproof. We want to apply {\enot}I to the subproof, so we assume $(G \eand \enot G)$. We then get an explicit contradiction by {\eand}E. The proof looks like this:

%\begin{proof}
%	\open
%		\hypo{gng}{G\eand \enot G}\as{}
%		\have{g}{G}\ae{gng}
%		\have{ng}{\enot G}\ae{gng}
%	\close
%	\have{ngng}{\enot(G \eand \enot G)}\ni{gng-ng}
%\end{proof}

\section{Reiteration and double negation}
In addition to the rules for each logical operator, we also have the \define{reiteration rule} and a \define{double negation rule}. These rules, like the other rules, preserve the truth value of the sentences in a proof. (I.e., if the sentence on a certain line is true and you apply either one of these rules to that sentence, what you get on the new line will still be true.) But, compared to the others, these are minor rules. Their purpose is just to help us produce proofs more easily. 

 If you already have shown something in the course of a proof, the reiteration rule allows you to repeat it on a new line.  

\begin{factboxy}{reiteration rule}
\begin{proof}
	\have[m]{a1}{\meta{A}}
	\have[n]{a2}{\meta{A}} \reit{a1}
\end{proof}
\end{factboxy}

To demonstrate both the reiteration rule and the negation elimination rule, we will go through the proof for this argument: $\enot P \eif \enot Q, Q \proves P$. Looking at the argument, you'll notice that our conclusion is $P$, but $P$ itself does not occur anywhere in the premises. Hence, we cannot get $P$ by using $\eand$E, $\eor$E, $\eif$E, or $\eiff$E. That tells us that we will need to use one of our negation rules.

After the premises, we make the assumption that we need for negation elimination. Since, ultimately, we want `$P$', we will assume `$\enot P$', knowing, of course, that once we discharge that assumption (and close the subproof), we will have `$P$'.

\begin{proof}
	\hypo{p1}{\enot P \eif \enot Q} \pr{}	
	\hypo{p2}{Q} \pr{}
	\open
		\hypo{np}{\enot P}\as{}
\end{proof}
We then use the conditional elimination rule to get $\enot Q$ on line 4. 
\begin{proof}
	\hypo{p1}{\enot P \eif \enot Q} \pr{}	
	\hypo{p2}{Q} \pr{}
	\open
		\hypo{np}{\enot P}\as{}
		\have{nq}{\enot Q}\ce{p1,np}
%		\have{ng}{Q}\reit{p2}
\end{proof}
The $Q$ on line 2 and $\enot Q$ on line 4 are a contradiction, but to use $\enot$E we need to have $Q$ on line 5. To move it down to line 5, we use the reiteration rule. 

\begin{proof}
	\hypo{p1}{\enot P \eif \enot Q} \pr{}	
	\hypo{p2}{Q} \pr{}
	\open
		\hypo{np}{\enot P}\as{}
		\have{nq}{\enot Q}\ce{p1,np}
		\have{ng}{Q}\reit{p2}
\end{proof}
Now that $\enot Q$ and $Q$ are on consecutive lines, we can use $\enot$E to discharge the assumption that we made, and that gives us the conclusion we are after: $P$.

\begin{proof}
	\hypo{p1}{\enot P \eif \enot Q} \pr{}	
	\hypo{p2}{Q} \pr{}
	\open
		\hypo{np}{\enot P}\as{}
		\have{nq}{\enot Q}\ce{p1,np}
		\have{ng}{Q}\reit{p2}
	\close
	\have{c}{P}\ni{np-ng}
\end{proof}

Our final rule is the double negation rule. The first version of this rule allows us to add two \textit{not}s (i.e., \textit{not not}) to an atomic sentence---which, of course, will not change the sentence's truth value. This is sometimes useful, especially when you need to use the disjunction elimination rule. The second version of the double negation rule allows us to remove two \textit{not}s, although needing to do this is less common.

\begin{factboxy}{double negation rule}
\begin{proof}
	\have[m]{a1}{\meta{A}}
	\have[n]{a2}{\enot\enot\meta{A}} \dn{a1}
\end{proof}

\begin{proof}
	\have[m]{a1}{\enot\enot\meta{A}}
	\have[n]{a2}{\meta{A}} \dn{a1}
\end{proof}
\end{factboxy}

To illustrate the double negation rule---and also to review the $\eor$E and $\eif$I rules---we will go through the proof for this argument: $\enot P \eor (R \eand Q) \proves P \eif Q$. We begin by listing the premise and making the assumption for $\eif$I.

\begin{proof}
	\hypo{p1}{\enot P \eor (R \eand Q)} \pr{}	
	\open
		\hypo{p}{P}\as{}
\end{proof}
The next thing that we want is `$(R \eand Q)$' on a line by itself. Now, you might think that, with `$\enot P \eor (R \eand Q)$' and `$P$', we can use `$\eor$E' to infer `$(R \eand Q)$'. After all, `$P$' is the \textit{denial} of `$\enot P$'. Recall, however, the disjunction elimination rule.
\begin{factboxy}{disjunction elimination rule}
\begin{proof}
	\have[m]{ab}{\meta{A}\eor\meta{B}}
	\have[n]{nb}{\enot\meta{B}}
	\have[\ ]{a}{\meta{A}} \oe{ab,nb}
\end{proof}
\end{factboxy}  
\noindent To use this rule, we need the \textit{negation} of one of the disjuncts. The negation of a sentence in TFL is the sentences with a `$\enot$' in front of it. The negation of `$\meta{B}$' is `$\enot\meta{B}$', and the negation of `$\enot\meta{B}$' is `$\enot\enot\meta{B}$' (and so on by the same pattern). Hence, we need to transform `$P$' into `$\enot\enot P$' with the double negation rule. Then we can use the disjunction elimination rule with `$\enot P \eor (R \eand Q)$' and `$\enot\enot P$'.

\begin{proof}
	\hypo{p1}{\enot P \eor (R \eand Q)} \pr{}	
	\open
		\hypo{p}{P}\as{}
		\have{dn}{\enot\enot P} \dn{p}
\end{proof}
Once we have `$R \eand Q$', we use `$\eand$E' to get `$Q$'. Then we  can use `$\eif$I' to get the conclusion.

\begin{proof}
	\hypo{p1}{\enot P \eor (R \eand Q)} \pr{}	
	\open
		\hypo{p}{P}\as{}
		\have{dn}{\enot\enot P} \dn{p}
		\have{rq}{R \eand Q} \oe{p1,dn}
		\have{q}{Q} \ae{rq}
	\close
	\have{c}{P \eif Q}\ci{p-q}
\end{proof}



\section{Invalid arguments}

In this chapter, we have taken it for granted that each argument that we have encountered has been valid. The purpose of providing a proof, then, is (1) to confirm that it is valid and (2) to show why it is valid---that is, to lay out each very simple step that takes us from the premises to the conclusion. If an argument is invalid, however, we are stuck. It is impossible to provide a correct proof of an invalid argument using the rules introduced in this chapter. At the same time, just because we can't provide a proof for an argument doesn't mean that the argument is invalid. Perhaps the proof is just too complicated for us to figure out. 

In chapter \ref{s:NDVeryIdea}, we discussed some reasons to prefer natural deduction to truth tables for checking that an argument is valid. To show that an argument is invalid, however, creating a truth table is not merely a superior method, it is our only option.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercises for chapter on proofs

\section{Practice exercises}
\setcounter{ProbPart}{0}

\problempart
Give a proof for each argument.
\begin{earg}
\item $\enot P \eif (Q \eor P), \enot P \proves Q$\smallskip
\item $D \eand H, H \eiff J  \proves J \eor N$\smallskip 
\item $G \eand (H \eand J), (H \eor J) \eif K \proves K$\smallskip
\item $P \eand (Q \eor R), P \eif \enot R \proves Q \eor S$\smallskip
\end{earg}


\problempart
Give a proof for each argument.
\begin{earg}
\item $P \eif (Q \eif R) \proves (P \eand Q) \eif R$\smallskip
\item $Q \eif R \proves (Q \eand S) \eif (R \eor T)$\smallskip 
\item $M \eand (\enot N \eif \enot M) \proves (N \eand M) \eor \enot M$\smallskip
\item $(Z\eand K)\eiff(Y\eand M), D\eand(D\eif M) \proves Y\eif Z$\smallskip
\item $C\eif(E\eand G), \enot C \eif G \proves G$\smallskip
\item $\enot(P \eif Q) \proves \enot Q$\smallskip
\item $S \eiff T \proves S \eiff (T \eor S)$\smallskip 
\item $D \eor F, D \eif G, F \eif H \proves G \eor H$\smallskip
\item $(W \eor X) \eor (Y \eor Z), X\eif Y, \enot Z \proves W\eor Y$
\end{earg}


\problempart
If you know that a proof can be given for $\meta{A}\proves\meta{B}$ (that is, you know that the argument is valid), then is it possible to know if $(\meta{A} \eand \meta{C}) \proves \meta{B}$ is valid? Is it possible to know if $(\meta{A} \eif \meta{B}) \eif \meta{B} \proves \meta{B}$ is valid? Explain your answers.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  answers

\section{Answers}
\setcounter{ProbPart}{0}

\problempart
\begin{earg}
\item $\enot P \eif (Q \eor P), \enot P \proves Q$
\begin{proof}
	\hypo{p1}{\enot P \eif (Q \eor P)} \pr{}
	\hypo{p2}{\enot P} \pr{}
	\have{qp}{Q \eor P} \ce{p1,p2}
	\have{q}{Q} \oe{p2,qp}
\end{proof}
\medskip

\item $D \eand H, H \eiff J  \proves J \eor N$ 
\begin{proof}
	\hypo{p1}{D \eand H} \pr{}
	\hypo{p2}{H \eiff J} \pr{}
	\have{h}{H} \ae{p1}
	\have{j}{J} \be{p2,h}
	\have{jn}{J \eor N} \oi{j}
\end{proof}
\medskip

\filbreak
\item $G \eand (H \eand J), (H \eor J) \eif K \proves K$
\begin{proof}
	\hypo{p1}{G \eand (H \eand J)} \pr{}
	\hypo{p2}{(H \eor J) \eif K} \pr{}
	\have{hj}{H \eand J} \ae{p1}
	\have{h}{H} \ae{hj}
	\have{hj2}{H \eor J} \oi{h}
	\have{k}{K} \ci{p2,hj2}
\end{proof}
\medskip


\item $P \eand (Q \eor R), P \eif \enot R \proves Q \eor S$
\begin{proof}
	\hypo{p1}{P \eand (Q \eor R)} \pr{}
	\hypo{p2}{P \eif \enot R} \pr{}
	\have{p}{P} \ae{p1}
	\have{nr}{\enot R} \ci{p2,p}
	\have{qr}{Q \eor R} \ae{p1}
	\have{q}{Q} \oe{nr,qr}
	\have{qs}{Q \eor S} \oi{q}
\end{proof}
\medskip
\end{earg}

\filbreak
\problempart
\begin{earg}
\item $P \eif (Q \eif R) \proves (P \eand Q) \eif R$
\begin{proof}
	\hypo{p1}{P \eif (Q \eif R)} \pr{}
	\open
	\hypo{as}{P \eand Q} \as{}
	\have{p}{P} \ae{as}
	\have{qr}{Q \eif R} \ce{p1,p}
	\have{q}{Q} \ae{as}
	\have{r}{R} \ce{qr,q}
	\close
	\have{con}{(P \eand Q) \eif R} \ci{as-r}
\end{proof}
\medskip


\item $Q \eif R \proves (Q \eand S) \eif (R \eor T)$ 
\begin{proof}
	\hypo{p1}{Q \eif R} \pr{}
	\open
	\hypo{as}{Q \eand S} \as{}
	\have{q}{Q} \ae{as}
	\have{r}{R} \ce{p1,q}
	\have{rt}{R \eor T} \oi{r}
	\close
	\have{con}{(Q \eand S) \eif (R \eor T)} \ci{as-rt}
\end{proof}
\medskip

\filbreak
\item $M \eand (\enot N \eif \enot M) \proves (N \eand M) \eor \enot M$
\begin{proof}
\hypo{MnNnM}{M \eand (\enot N \eif \enot M)} \pr{}
\have{M}{M}\ae{MnNnM}
\have{nNnM}{\enot N \eif \enot M}\ae{MnNnM}
\open
	\hypo{nN}{\enot N} \as{}
	\have{nM}{\enot M}\ce{nNnM, nN}
	\have{M2}{M} \reit{M}
\close
\have{N}{N} \ne{nN-M2}
\have{NM}{N \eand M}\ai{M,N}
\have{con}{(N \eand M) \eor \enot M}\oi{NM}	
\end{proof}
\medskip

\filbreak
\item $(Z\eand K)\eiff(Y\eand M), D\eand(D\eif M) \proves Y\eif Z$
\begin{proof}
\hypo{zkym}{(Z \eand K) \eiff (Y \eand M)} \pr{}
\hypo{ddm}{D \eand (D \eif M)} \pr{}
\have{d}{D}\ae{ddm}
\have{dm}{D \eif M}\ae{ddm}
\have{m}{M}\ce{d,dm}
\open
	\hypo{y}{Y} \as{}
	\have{ym}{Y \eand M}\ai{m,y}
	\have{zk}{Z \eand K}\be{zkym, ym}
	\have{z}{Z}\ae{zk}
\close
\have{yz}{Y \eif Z}\ci{y-z}
\end{proof}
\medskip

\filbreak
\item $C\eif(E\eand G), \enot C \eif G \proves G$
\begin{proof}
\hypo{ceg}{C \eif (E \eand G)} \pr{}
\hypo{ncg}{\enot C \eif G} \pr{}
\open
        \hypo{ng}{\enot G} \as{}
        \open
                \hypo{c}{C} \as{}
                \have{eg}{E \eand G}\ce{ceg, c}
                \have{g1}{G}\ae{eg}
                \have{ng2}{\enot G} \reit{ng}
        \close
        \have{nc}{\enot C}\ni{c-ng2}
        \have{g2}{G}\ce{ncg, nc}
        \have{ng3}{\enot G} \reit{ng}
\close
\have{con}{G}\ne{ng-ng3}
\end{proof}
\medskip

\item $\enot(P \eif Q) \proves \enot Q$
\begin{proof}
	\hypo{p1}{\enot(P \eif Q)} \pr{}
	\open
	\hypo{as}{Q} \as{}
		\open
		\hypo{as2}{P} \as{}
		\have{q}{Q} \reit{as}
		\close
	\have{pq}{P \eif Q} \ci{as2-q}
	\have{npq}{\enot(P \eif Q)} \reit{p1}
	\close
	\have{con}{\enot Q} \ni{as-npq}
\end{proof}
\medskip


\filbreak
\item $S \eiff T \proves S \eiff (T \eor S)$ 
\begin{proof}
	\hypo{p1}{S \eiff T} \pr{}
	\open
	\hypo{as1}{S} \as{}
	\have{t}{T} \be{p1,as1}
	\have{ts}{T \eor S} \oi{t}
	\close
	\have{c1}{S \eif (T \eor S)} \ci{as1-ts}
	\open
	\hypo{as2}{T \eor S} \as{}
		\open
		\hypo{as3}{\enot S} \as{}
		\have{t2}{T} \oe{as2,as3}
		\have{s2}{S} \be{p1,t2}
		\have{ns}{\enot S} \reit{as3}
		\close
	\have{s}{S} \ne{as3-ns}
	\close
	\have{c2}{(T \eor S) \eif S} \ci{as2-s}
%	\have{con1}{(S \eif (T \eor S)) \eand ((T \eor S) \eif S)} \ai{c1,c2}
	\have{con2}{S \eiff (T \eor S)} \bi{c1,c2}
\end{proof}
\medskip


\filbreak
\item $D \eor F, D \eif G, F \eif H \proves G \eor H$ 
\begin{proof}
	\hypo{p1}{D \eor F} \pr{}
	\hypo{p2}{D \eif G} \pr{}
	\hypo{p3}{F \eif H} \pr{}
	\open
	\hypo{as}{\enot(G \eor H)} \as{}
		\open
		\hypo{as2}{\enot D} \as{}
		\have{f}{F} \oe{p1,as2}
		\have{h}{H} \ce{p3,f}
		\have{gh}{G \eor H} \oi{h}
		\have{ngh}{\enot (G \eor H)} \reit{as}
		\close
	\have{d}{D} \ne{as2-ngh}
	\have{g}{G} \ce{p2,d}
	\have{gh2}{G \eor H} \oi{g}
	\have{ngh2}{\enot (G \eor H)} \reit{as}
	\close
	\have{con}{G \eor H} \ne{as-ngh2}
\end{proof}

\filbreak
\item $(W \eor X) \eor (Y \eor Z), X\eif Y, \enot Z \proves W\eor Y$
\begin{proof}
\hypo{p1}{(W \eor X ) \eor (Y \eor Z )} \pr{}
\hypo{p2}{X \eif Y} \pr{}
\hypo{p3}{\enot Z} \pr{}
\open
	\hypo{nwy}{\enot (W \eor Y)} \as{}
		\open
		\hypo{nwx}{\enot (W \eor X)} \as{}
		\have{yz}{Y \eor Z} \oe{p1,nwx}
		\have{y}{Y} \oe{p3,yz}
		\have{wy}{W \eor Y} \oi{y}
		\have{nwy2}{\enot (W \eor Y)} \reit{nwy}
		\close
	\have{wx}{W \eor X} \ne{nwx-nwy2}
		\open
		\hypo{x}{X} \as{}
		\have{y2}{Y} \ce{p2,x}
		\have{wy}{W \eor Y} \oi{y2}
		\have{nwy3}{\enot (W \eor Y)} \reit{nwy}
		\close
	\have{nx}{\enot X} \ni{x-nwy3}
	\have{w}{W} \oe{wx,nx}
	\have{wy2}{W \eor Y} \oi{w}
	\have{nwy4}{\enot (W \eor Y)} \reit{nwy}
	\close
\have{con}{W \eor Y} \ne{nwy-nwy4}
\end{proof}

\end{earg}
\bigskip

\problempart
If $\meta{A} \proves \meta{B}$ is valid, then $(\meta{A} \eand \meta{C})\proves \meta{B}$ is valid. We know this because if $\meta{A} \proves \meta{B}$, then there is some proof with assumption $\meta{A}$ that ends with $\meta{B}$, and no undischarged assumptions other than $\meta{A}$. Now, if we start a proof with assumption $(\meta{A} \eand \meta{C})$, we can obtain $\meta{A}$ by $\eand$E. We can now copy and paste the original proof of $\meta{B}$ from $\meta{A}$, adding 1 to every line number and line number citation. The result will be a proof of $\meta{B}$ from assumption $\meta{A}$.

We also know that $(\meta{A} \eif \meta{B}) \eif \meta{A} \proves \meta{B}$ is valid. Since, there is some proof with assumption $\meta{A}$ that ends with $\meta{B}$, we can assume \meta{A}, derive \meta{B}, and then use the conditional-introduction rule to get $(\meta{A} \eif \meta{B})$. We use the condtional-elimination rule on $(\meta{A} \eif \meta{B})$ and the premise, and get \meta{B}, the conclusion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% CHAPTER 16

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% CHAPTER 17

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Proofs in Carnap}\label{s:Carnap-proofs}

Creating proofs in Carnap is not difficult. To type the connectives, use the symbols on the right in table \ref{symbols-Carnap}. 

\begin{table*}\centering\sffamily\footnotesize
\ra{1.25}
\begin{tabular}{@{}l l@{}}\toprule
\textsc{TFL connective} & \textsc{in Carnap} \\\midrule
	\enot & $ \sim$\\
	\eand & \&\\
	\eor & v (lowercase v)\\
	\eif & $-$$>$ (dash, greater than sign)\\
	\eiff & $<$$-$$>$\\
\bottomrule
\end{tabular}
\caption{}\label{symbols-Carnap}
\end{table*}


Carnap will number the lines automatically. After the sentence on each line, there has to be a colon (`:') before the `PR', `AS', or the rule. Carnap is flexible with the spacing on a line, but as a guideline, put a tab space between the sentence and `:PR', `:AS', or the rule (:$\eif$E, :$\eor$I, etc.). Also indent subproofs with a tab space. (Carnap will let you use more or fewer spaces, but a subproof has to be indented some amount.)

To produce a proof, you are given an interface like the one shown in figure \ref{fig:proof-1a}. As you can see, the argument is given at the top. In this case, the premises are $P \eif Q$ and $R \eand P$, and the conclusion is $Q$. (The premises are separated by commas. The premises and the conclusion are separated by the turnstile (`$\proves$').)

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{SCfigure}
\centering
\includegraphics[height=4cm]{textbook--1a.PNG}
\caption{}
\label{fig:proof-1a}
\end{SCfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Begin by listing the premises, and don't forget to put `:PR' after each one. If there is a problem with a line---either the sentence isn't formed correctly, the rule you've cited isn't being used correctly, or there's some other mistake---Carnap will put \textsf{?} or {\fontencoding{U}\fontfamily{futs}\selectfont\char 66\relax}  at the end of the line. When the line is ok, you will get a `+'.

Once you complete each line, Carnap will give you the typographically correct version on the right (figure \ref{fig:proof-1b}). We finish this proof using the $\eand$E and $\eif$E rules (figure \ref{fig:proof-1c}). When the proof is correct, the box containing the argument will turn green, and the proof can be submitted. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
%\centering
\includegraphics[width=13cm]{textbook--1b.PNG}
\caption{}
\label{fig:proof-1b}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
%\centering
\includegraphics[width=13cm]{textbook--1c.PNG}
\caption{}
\label{fig:proof-1c}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our next example, $(A \eor B) \proves (\enot A \eif B)$, requires a subproof. We begin as before. To create the subproof, put a tab space before $\enot A$ and put `:AS' at the end of the line (figure \ref{fig:proof-2a}). Since the next line is also part of the subproof, we again need a tab before the $B$. We end the subproof (and discharge the assumption) with the $\eif$I rule. $\enot A \eif B$ is not indented (so no tabs or spaces before the $\enot A$). That's the conclusion, and so if everything is correct, Carnap will give you the green bar and you can submit the proof (figure \ref{fig:proof-2b}). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
%\centering
\includegraphics[width=13cm]{textbook--2a.PNG}
\caption{}
\label{fig:proof-2a}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
%\centering
\includegraphics[width=13cm]{textbook--2b.PNG}
\caption{}
\label{fig:proof-2b}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

As I said at the beginning of this chapter, creating proofs in Carnap is not difficult. You do have to be careful, however. Programming a language like TFL is relatively simple because there are only a small number of rules and, to produce proofs of valid arguments, we follow those rules very strictly. But as a consequence, Carnap is not designed to understand what you are trying to do if you deviate from the rules, even if it is a minor deviation or an innocent mistake. So, some reminders:
\begin{enumerate}
\itemsep-.3mm
	\item As long as `$\enot$' is not the main logical operator, you can drop the outermost parentheses. All other parentheses have to be used. 
	\item Capitalize `PR', `AS',`E', `I' (in the rules), and all atomic sentences.
	\item Don't forget the `:' right before PR, AS, or the rule that you are citing. 
	\item There is no space between the $\eand$, $\eor$, $\eif$, $\eiff$, or $\enot$ and the `E' or `I'.  
	\item There is a space (and no punctuation) after the `E' or `I'. 
	\item There is a comma between the two lines that have to be cited for $\eand$I, $\eor$E, $\eif$E, and $\eiff$E (e.g., `$\eif$E 2,4').
	\item There is a dash between the two lines that have to be cited for $\eif$I, $\enot$I, and $\enot$E (e.g., `$\enot$E 4-6').
%	\item The $\eiff$I rule requires dashes and a comma. (Check the format on p.~\pageref{eiff-I}.)
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\chapter{Proof strategies}
There is no simple recipe for proofs, and there is no substitute for practice. Here, though, are some questions and strategies to keep in mind.

\begin{earg}
\item[\ex{18-1}] Do you know all of the rules? \textbf{If you don’t have them memorized yet, then they should be written on a sheet of paper that you have next to you while you’re working.}
\medskip

\item[\ex{18-2}] Are there steps that you can take without making an assumption? If yes, is it worth taking those steps?
\medskip

\item[\ex{18-3}] If you’re not sure how to proceed, but you can do conjunction-elimination, conditional-elimination, disjunction-elimination, or biconditional-elimination, then do them just to see what happens.
\medskip

\item[\ex{18-4}] If an assumption is needed, is it for $\eif$I, $\enot$I, or $\enot$E? \textbf{Don't make an assumption if you don't know which of these rules you plan to use when you close the sub-proof.}
\medskip

\item[\ex{18-5}] If an assumption is needed, what should it be? (If you want to get $A \eif B$, then you’re going to use $\eif$I and your assumption should be $A$.)
\medskip

\item[\ex{18-6}] If you make an assumption, then you should know what you want on either the last line of your sub-proof (if you’re doing $\eif$I) or the last two lines of your sub-proof (if you’re doing $\enot$I or $\enot$E).
\medskip

\item[\ex{18-3a}] Sometimes it is useful to work backwards from the conclusion. The conclusion, of course, will be the last line of your proof, and you can, if you wish, put it at the bottom of the proof anytime. For example, let's say that you need to provide a proof for this argument: $P \eif (\enot Q \eif R) \proves (P \eand \enot Q) \eif R$. You can begin this way:
\begin{proof}
	\hypo{pqr}{P \eif (\enot Q \eif R)} \pr{}
		\have[\ ]{pq}{}
		\have[\ ]{p}{}
		\have[\ ]{qr}{}
		\have[\ ]{q}{}
	\have[\ ]{con}{(P \eand \enot Q) \eif R}
\end{proof}
\medskip

Knowing that you need to arrive at a conditional, you also know that you need to use the conditional-introduction rule, what your assumption should be, and what will be on the last line of your sub-proof.

\begin{proof}
	\hypo{pqr}{P \eif (\enot Q \eif R)} \pr{}
	\open
		\hypo{pq}{P \eand \enot Q}\as{}
		\have[\ ]{p}{}
		\have[\ ]{qr}{}
		\have[\ ]{r}{R}
	\close
	\have[\ ]{con}{(P \eand \enot Q) \eif R}\ci{}
\end{proof}

\medskip

\item[\ex{18-7}] When you use $\enot$I or $\enot$E, you are using them as a last resort. Use them when you can't use any of the other rules.
\medskip

\item[\ex{18-8}] \textbf{Persist}. Try different things. If one approach fails, then try something else.
\end{earg}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Proof-theoretic concepts}\label{s:ProofTheoreticConcepts}

\section{Theorems}

You are familiar with arguments that have this form:
$$\meta{A}_1, \meta{A}_2, \ldots, \meta{A}_n \proves \meta{C}$$
We may also, however, have a sentence for which it is possible to give a proof with no premises: ${} \proves \meta{C}$. In this case, we say that $\meta{C}$ is a \define{theorem}.

\begin{factboxy}{Theorem}\label{def:syntactic_tautology_in_sl}
$\meta{C}$ is a \define{theorem} iff ~ $\proves \meta{C}$
\end{factboxy}

One such sentence is `$\enot (A \eand \enot A)$'. To show that this sentence is a theorem, we give a proof that has no premises and no undischarged assumptions. To get started, we do, however, have to make an assumption. We will assume `$A \eand \enot A$'. Once we show that this assumption leads to contradiction, we can discharge it and we will have `$\enot (A \eand \enot A)$'. This is the proof:
	\begin{proof}
		\open
			\hypo{con}{A \eand \enot A}
			\have{a}{A}\ae{con}
			\have{na}{\enot A}\ae{con}
		\close
		\have{lnc}{\enot (A \eand \enot A)}\ni{con-na}
	\end{proof}
This theorem, `$\proves \enot (A \eand \enot A)$' is an instance of what is sometimes called \emph{the Law of Non-Contradiction}.

To show that a sentence is a theorem, we just have to find a suitable proof. On the other hand, it is not possible to show that a sentence is \emph{not} a theorem this same way. To show that a sentence is not a theorem with our natural deduction system, we would have to demonstrate, not just that certain proof strategies fail, but that \emph{no} proof is possible. Even if we fail in trying to give a proof for a sentence in a thousand different ways, perhaps the proof is just too long and complex for us to figure out. 

\section{Equivalent, consistent, and inconsistent}

In \S\ref{equivalence--tt}, we defined \textit{equivalent} in terms of truth tables, namely, if two sentences have the same truth value on every line of a truth table, then they are equivalent. We can also show that two sentences are equivalent using our natural deduction system. To indicate that we have shown that the two sentences are equivalent with a derivation (or actually with two derivations), we will call this equivalence \define{provably equivalent}. 

\begin{factboxy}{Provably equivalent}
Two sentences \meta{A} and \meta{B} are \define{provably equivalent} iff each can be derived from the other. I.e., $\meta{A}\proves\meta{B}$ and $\meta{B}\proves\meta{A}$.\\
(Equivalently, \meta{A} and \meta{B} are \define{provably equivalent} if $\proves \meta{A} \eiff \meta{B}$.)
\end{factboxy}
        
As in the case of showing that a sentence is a theorem, it is relatively easy to show that two sentences are provably equivalent: it just requires a pair of proofs. Showing that sentences are \emph{not} provably equivalent is not possible for t he same reason that it isn't possible to show that a sentence is not a theorem. Even if we fail to produce two proofs showing that two sentences are provably equivalent, that doesn't mean that the proofs don't exist. It just means that we've failed to figure out what they are. 

We also, in \S\ref{consistency--tt}, defined \textit{jointly inconsistent} using truth tables: sentences are jointly inconsistent if there is no line on a truth table where they are all true. Again, we can show that two or more sentences are jointly inconsistent with our natural deduction system. 

\begin{factboxy}{Provably inconsistent}
The sentences $\meta{A}_1,\meta{A}_2,\ldots, \meta{A}_n$ are \define{provably inconsistent} iff, from them, a contradiction can be derived. I.e.\ $\meta{A}_1,\meta{A}_2,\ldots, \meta{A}_n \proves (\meta{B} \eand \enot \meta{B})$.
%\tcblower
%If they are not \define{inconsistent}, then $\meta{A}_1,\meta{A}_2,\ldots, \meta{A}_n$ are \define{provably consistent}.
\end{factboxy}
        
To show that a set of sentences are provably inconsistent, we use the sentences as premises and then derive a contradiction. (Any contradiction will do.) For instance, this proof demonstrates that $P \eand Q$ and $\enot P \eor \enot Q$ are provably inconsistent.

	\begin{proof}
	\hypo{p1}{P \eand Q} \pr{}
	\hypo{p2}{\enot P \eor \enot Q} \pr{}
	\have{p}{P} \ae{p1}
	\have{np}{\enot \enot P} \dn{p}
	\have{nq}{\enot Q} \oe{p2,np}
	\have{q}{Q} \ae{p1}
	\end{proof}

Showing that some set of sentences are \textit{not} provably inconsistent is, as you might guess at this point, not possible. Doing so would require showing, not just that we have failed to derive a contradiction from a set a sentences, but that no such derivation is possible.

Table \ref{table.one-mult-proofs} summarizes what we have covered in this chapter. As we will discuss in the next chapter, when the presence (or the absence) of a logical property cannot be demonstrated using our natural deduction system, we have to resort to using a truth table.

\begin{table*}\centering\sffamily\footnotesize
\ra{1.25}
\begin{tabular}{@{}l l l@{}}\toprule
To check & that it is & that it is not\\\midrule
theorem & one proof & \textit{not possible with proofs}\\
equivalent & two proofs & \textit{not possible with proofs}\\
inconsistent &  one proof  & \textit{not possible with proofs}\\
consistent & \textit{not possible with proofs} & one proof\\
\bottomrule
\end{tabular}
\caption{This table summarizes what is required to check each of these logical notions.}\label{table.one-mult-proofs}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercises for chapter on proof theoretic concepts

\section{Practice exercises}
\setcounter{ProbPart}{0}


\problempart
Give a proof for each of these theorems.
\begin{earg}
\item $\proves O \eif O$\smallskip
\item $\proves N \eor \enot N$\smallskip
\item $\proves (P \eif Q) \eor (Q \eif P)$\smallskip
\item $\proves J \eiff [J\eor (L\eand\enot L)]$\smallskip
\item $\proves ((A \eif B) \eif A) \eif A$\smallskip 
\end{earg}


\problempart
Show that each of the following pairs of sentences are provably equivalent. (To indicate that the inference from the premise to the conclusion goes from the first sentence to the second and vice versa, we use the symbols $\leftproves\proves$.)
\begin{earg}
\item $T\eif S \leftproves\proves \enot S \eif \enot T$
\item $R \eif Q \leftproves\proves \enot(R \eand \enot Q)$
\end{earg}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  answers

\section{Answers}
\setcounter{ProbPart}{0}


\problempart
\begin{earg}
\item $\proves O \eif O$
\myanswer{\begin{proof}
\open
	\hypo{o}{O} \as{}
	\have{o2}{O}\by{R}{o}
\close
\have{con}{O \eif O}\ci{o-o2}
\end{proof}}
\bigskip

\item $\proves N \eor \enot N$
\begin{proof}
\open
        \hypo{ncon}{\enot (N \eor \enot N)} \as{}
        \open
                \hypo{n}{N} \as{}
                \have{nnn1}{N \eor \enot N}\oi{n}
                \have{ncon2}{\enot (N \eor \enot N)} \reit{ncon}
        \close
        \have{nn}{\enot N} \ni{n-ncon2}
        \have{nnn2}{N \eor \enot N} \oi{nn}
        \have{ncon3}{\enot (N \eor \enot N)} \reit{ncon}
\close
\have{con}{N \eor \enot N} \ne{ncon-ncon3}
\end{proof}
\bigskip

\filbreak
\item $\proves (P \eif Q) \eor (Q \eif P)$
\begin{proof}
\open
        \hypo{ncon}{\enot ((P \eif Q) \eor (Q \eif P))} \as{}
        \open
        		\hypo{p}{P} \as{}
        		\open
        			\hypo{nq}{\enot Q} \as{}
        			\open
        				\hypo{q}{Q} \as{}
        				\have{p2}{P} \reit{p}
				\close
				\have{qp}{Q \eif P} \ci{q-p2}
				\have{pqqp}{(P \eif Q) \eor (Q \eif P)} \oi{qp}
				\have{ncon2}{\enot ((P \eif Q) \eor (Q \eif P))} \reit{ncon}
			\close
			\have{q}{Q} \ne{nq-ncon2}
		\close
		\have{pq}{P \eif Q} \ci{p-q}
		\have{pqqp2}{(P \eif Q) \eor (Q \eif P)} \oi{pq}
		\have{ncon2}{\enot ((P \eif Q) \eor (Q \eif P))} \reit{ncon}
\close
\have{con}{(P \eif Q) \eor (Q \eif P)} \ne{ncon-ncon2}
\end{proof}
\bigskip

\filbreak
\item $\proves J \eiff (J\eor (L \eand \enot L))$
\myanswer{\begin{proof}
\open
	\hypo{j1}{J} \as{}
	\have{jlnl1}{J \eor (L \eand \enot L)}\oi{j1}
\close
\have{cond1}{J \eif (J\eor (L \eand \enot L))} \ci{j1-jlnl1}
\open
	\hypo{jlnl2}{J \eor (L \eand \enot L)} \as{}
	\open
		\hypo{lnl}{L \eand \enot L} \as{}
		\have{l}{L}\ae{lnl}
		\have{nl}{\enot L}\ae{lnl}
	\close
	\have{nlnl}{\enot (L \eand \enot L)}\ne{lnl-nl}
	\have{j5}{J}\oe{jlnl2, nlnl}
\close
\have{cond2}{(J \eor (L \eand \enot L)) \eif J} \ci{jlnl2-j5}
%\have{conj}{(J \eif (J\eor (L \eand \enot L))) \eand ((J \eor (L \eand \enot L)) \eif J)} \ai{cond1,cond2}
\have{con}{J \eiff (J\eor (L \eand \enot L))}\bi{cond1,cond2}
\end{proof}}
\bigskip


\filbreak
\item $((A \eif B) \eif A) \eif A$ 
\begin{proof}
\open
	\hypo{aba}{(A \eif B) \eif A} \as{}
	\open 
		\hypo{na}{\enot A} \as{}
		\open
		\have{nab}{\enot (A \eif B)} \as{}
			\open
				\hypo{a}{A} \as{}
					\open
					\hypo{nb}{\enot B} \as{}
					\have{a2}{A} \reit{a}
					\have{na2}{\enot A} \reit{na}
				\close
				\have{b}{B} \ne{nb-na2}
			\close
		\have{ab}{A \eif B}\ci{a-b}
		\have{nab2}{\enot (A \eif B)}\reit{nab}
	\close
	\have{ab2}{A \eif B} \ne{nab-nab2}
	\have{a}{A} \ce{aba,ab2}
	\have{na3}{\enot A} \reit{na}
\close
\have{a2}{A} \ne{aba-na3}
\close
\have{con}{((A \eif B) \eif A) \eif A}\ci{aba-a2}
\end{proof}
\end{earg}


\filbreak
\problempart

\begin{earg}
\item $T\eif S \leftproves\proves \enot S \eif \enot T$
\begin{proof}
\hypo{ts}{T \eif S} \pr{}
\open
	\hypo{ns}{\enot S} \as{}
		\open
		\hypo{t}{T} \as{}
		\have{s}{S} \ce{ts,t}
		\have{ns2}{\enot S} \reit{ns}
		\close
	\have{nt}{\enot T} \ni{ns-ns2}
\close
\have{nsnt}{\enot S \eif \enot T}\ci{ns-nt}
\end{proof}

\begin{proof}
\hypo{nsnt}{\enot S \eif \enot T} \pr{}
\open
	\hypo{t}{T} \as{}
	\open
		\hypo{ns}{\enot S} \as{}
		\have{nt}{\enot T}\ce{nsnt, ns}
		\have{t2}{T}\reit{t}
	\close
	\have{s}{S}\ne{ns-t2}
\close
\have{ts}{T \eif S} \ci{t-s}
\end{proof}
\medskip


\filbreak
\item $R \eif Q \leftproves\proves \enot(R \eand \enot Q)$
\begin{proof}
\hypo{ui}{R \eif Q} \pr{}
\open
	\hypo{uni}{R \eand \enot Q} \as{}
	\have{u}{R}\ae{uni}
	\have{ni}{\enot Q}\ae{uni}
	\have{i}{Q}\ce{ui, u}
\close
\have{con}{\enot (R \eand \enot Q)}\ni{uni-i}
\end{proof}

\begin{proof}
\hypo{con}{\enot (R \eand \enot Q)} \pr{}
\open
	\hypo{u}{R} \as{}
	\open
		\hypo{ni}{\enot Q} \as{}
		\have{uni}{R \eand \enot Q}\ai{u, ni}
		\have{red}{\enot (R \eand \enot Q)}\reit{con}
	\close
	\have{i}{Q}\ne{ni-red}
\close
\have{ui}{R \eif Q}\ci{u-i}
\end{proof}

\end{earg}