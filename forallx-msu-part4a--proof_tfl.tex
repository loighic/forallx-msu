%!TEX root = forallxyyc.tex
\graphicspath{{figures--proofs/}}
\part{Natural deduction for TFL}
\label{ch.NDTFL}
\addtocontents{toc}{\protect\mbox{}\protect\hrulefill\par}

\chapter{Natural deduction}\label{s:NDVeryIdea}

\section{Natural deduction versus truth tables}

An argument is valid if and only if it is impossible for all of the premises to be true and the conclusion to be false. And we have seen that truth tables can be used to determine whether an argument is valid. In the next chapter, you will learn another method for verifying that an argument is valid. Before we turn to this new method, however, let’s review the strengths and weakness of truth tables.

\begin{earg}
\item The truth table method of determining if an argument is valid focuses directly on the definition of \textit{valid}. Each line of a complete truth table corresponds to a valuation. So, given an argument in TFL, truth tables reveal whether or not the conclusion is true when all of the premises true. 
\medskip

\item Truth tables also allow us to easily and rigorously set meaning for each logical operator. As we discussed in \S\ref{s:ConnectiveDisjunction}, in English, `or’ can take the inclusive-or meaning (one or the other, or both) or the exclusive-or meaning (one or the other, but not both), and, at different times, both meanings are used in English. We can discuss which English meaning is closest to the meaning of `$\eor$’ in TFL (it's the inclusive-or), but, in the end, we just set the meaning of the symbol `$\eor$’ with this truth table:

\begin{center}
\begin{tabular}{d d | f}
\meta{A} & \meta{B} & $\meta{A}\eor\meta{B}$ \\
\hline
T & T & T\Tstrut\\
T & F & T\\
F & T & T\\
F & F & F
\end{tabular}
\end{center}

Hence, it's definition is this: with respect to the sentence being true or false, `$\eor$’ is the symbol that connects $\meta{A}$ and $\meta{B}$ in the ways shown in the truth table. And, then, the same goes for the other logical operators. 
\medskip

\item To create a truth table, the number of lines needed is $2^n$ (where \textit{n} = the number of different letters). So, an argument with four different sentence letters will require a 16 line truth table, one with five letters will require 32 lines, one with six different letters will require 64 lines, and so on. Hence, while a truth table can be used to determine if any argument is valid or invalid, one of the weakness of this method is that it is difficult to use when the argument contains more than four different sentence letters.
\medskip

\item But what is typically seen as the biggest weakness of using truth tables to determine whether an argument is valid or not is that it doesn't reveal to us \textit{why} the argument is valid. It doesn’t, in other words, lay out the reasoning that demonstrates why (and how) the conclusion follows from the premises.
\end{earg}
 
As an alternative to truth tables, we use a \textit{natural deduction system}. Such a system allows us to verify that an argument is valid and to see why it is valid. We do this by making explicit the reasoning process that takes us from the premises to the conclusion. We begin with very basic rules. (For instance, one is this: if we know that `\textit{P or Q}' is true; and we also know that `\textit{not P}' is true, then we can assert that `\textit{Q}' is true.) These rules can be combined, and with just a small number of them, we hope to be able to show how we get from the premises to the conclusion for all of the valid arguments that can be represented in TFL. There are different deduction systems that can be used with TFL. A \textit{natural} deduction system is one that, for the most part, reflects the ways that we naturally reason---at least insofar as the reasoning involves ‘and’, ‘or’, ‘not’, ‘if \ldots, then \ldots’, and ‘if and only if’. 


\section{Truth functional propositional logic}

We have reached a point where it is useful to summarize what TFL is. As you know, the symbols of TFL are the sentence letters that represent atomic sentences, the logical operators $\enot$, $\eand$, $\eor$, $\eif$, and $\eiff$, and brackets. These, then, can be combined into sentences using the rules given in chapter \ref{s:TFLSentences}. And, then, in chapter \ref{s:CharacteristicTruthTables}, truth tables were used to set the meaning of the logical operators. 

Truth tables also give us a method for verifying that an argument satisfies the definition of \textit{valid}. \textit{Valid} is a concept and is not, strictly speaking a part of TFL. Rather it is a property of some arguments that can, to an extent, be studied and explicated using TFL. Similarly, as you have seen, \textit{tautology}, \textit{contradiction}, \textit{contingent}, \textit{equivalent},  \textit{jointly consistent}, and \textit{jointly inconsistent} are concepts that can be explicated using TFL.

The final part of TFL is a system of natural deduction, which sets the rules for how sentences containing the logical operators can be combined or taken apart. The natural deduction system introduced in the next chapter consists of twelve rules of derivation. (That is, twelve rules for deriving new statements from those that we already have.)
% This, in effect, establishes the syntax for sentences in TFL.

\section{Fitch} 

The modern development of natural deduction dates from simultaneous but unrelated papers by Gerhard Gentzen and Stanisław Jaśkowski that were published in 1934. The natural deduction system that we will use, however, is based largely on work by Frederic Fitch that was first published in 1952. Consequently, the format that is used, in the next chapter, for writing proofs is called \textit{Fitch notation}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\chapter{The rules of derivation}\label{s:BasicTFL}

\section{Proofs}

A \define{proof} is a list of sentences. The sentence or sentences at the beginning of the list are assumptions. These are the premises of the argument. (We call them \textit{assumptions} because, at least within the proof, they do not require any justification. We just state them.) Every other sentence in the list follows from earlier sentences by a specific rule. The final sentence is the conclusion of the argument.

As an illustration, consider this argument:
	$$\enot (A \eor B) \proves \enot A \eand \enot B$$
We start the proof by numbering the line and writing the premise:
\begin{proof}
	\hypo{a1}{\enot (A \eor B)} \pr{}
\end{proof}
Every line in a proof is numbered so that we can refer to it later if we need to do so. We have also indicated that this is a premise by putting `PR' at the end of the line. And we have drawn a line underneath the premise. Everything written above the line is an \emph{initial assumption} (i.e., a premise). Everything written below the line will either be a sentence that can be derived from that assumption, or it will be a new assumption that we introduce. The colon that is right before `PR' is, technically, optional, but it has to be used in Carnap to separate the TFL sentence from the `PR' (or the rule) that is written at the end of each line.

The conclusion of this argument is `$\enot A \eand \enot B$'; and so we want our proof to end---on some line, we'll call it $n$---with that sentence:
\begin{proof}
	\hypo{a1}{\enot (A \eor B)} \pr{}
	\have{a2}{\ldots}
	\have[ ]{}{\ldots}
	\have[ ]{}{\ldots}
	\have[n]{con}{\enot A \eand \enot B}
\end{proof}
It doesn't matter how many lines it takes to arrive at the conclusion, although, generally, we would prefer a shorter proof over a longer one.

Suppose we have this argument:
$$A\eor B, \enot (A\eand C), \enot (B \eand \enot D) \proves \enot C\eor D$$
This argument has three premises, and so we start by listing them, numbering each line, and drawing a line under the final premise:
\begin{proof}
	\hypo{a1}{A \eor B} \pr{}
	\hypo{a2}{\enot (A\eand C)} \pr{}
	\hypo{a3}{\enot (B \eand \enot D)} \pr{}
\end{proof}
This, meanwhile, will be the final line of the proof:
\begin{proof}
	\have[n]{con}{\enot C \eor D}
\end{proof}
Setting up the premises and the conclusion is, however, the easy part. The real task---and the interesting part---is determining each of the steps that get us from the premises to the conclusion. 

To do that, we will use a \define{natural deduction} system. In this system, there are two rules for each logical operator: an \define{introduction} rule, which allows us to derive a new sentence that has the logical operator as the main connective, and an \define{elimination} rule, which allows us to extract a sub-sentence from a sentence that has that logical operator as the main connective. (Table \ref{table.TFL-rules} contains a list of the rules.) These rules can then be combined to demonstrate each step that must be taken to get from the premises to the conclusion. All of the rules introduced in this chapter are also summarized on pp.~\pageref{ProofRules} - \pageref{ProofRules-end}.


\begin{table*}\centering\sffamily\footnotesize
\ra{1.25}
\begin{tabular}{@{}l l@{}}\toprule
\multicolumn{2}{@{}l}{\textth{The rules of derivation}} \\\midrule
conjunction introduction rule & conjuction elimination rule \\
disjunction introduction rule & disjunction elimination rule \\
conditional introduction rule & conditional elimination rule \\
biconditional introduction rule~~ & biconditional elimination rule \\
negation introduction rule & negation elimination rule \\
reiteration rule &\\
double negation rule &\\ 
\bottomrule
\end{tabular}
\caption{}\label{table.TFL-rules}
\end{table*}



\section{Conjunction intro and elim}\label{s:conj-rule}

Let's say that we know that Sarah is swimming. We also, as it happens, know that Amy is reading. We are, therefore, justified in stating, ``Sarah is swimming and Amy is reading.'' 
This reasoning process, which we all do naturally, is part of our natural deduction system. It is called the \define{conjunction introduction rule}. 
\begin{factboxy}{conjunction introduction rule}
\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[n]{b}{\meta{B}}
	\have[\ ]{c}{\meta{A}\eand\meta{B}} \ai{a, b}
\end{proof}
\tcblower
\footnotesize{Note that `$m$' and `$n$' will never appear in an actual proof. In a proof, the lines are numbered $1$, $2$, $3$, etc. We define this rule with `$m$' and `$n$' to emphasize that the $\meta{A}$ and the $\meta{B}$ can be on any lines in the proof (and in any order), as long as they are before the application of the rule.}
\end{factboxy}

\noindent The `$\meta{A}$' and `$\meta{B}$' can occur in either order, and the conjunction can be `$\meta{A} \eand \meta{B}$' or `$\meta{B} \eand \meta{A}$'.

For the example about Sarah and Amy, we can use this symbolization key:
	\begin{ekey}
		\item[S] Sarah is swimming.
		\item[R] Amy is reading.
	\end{ekey}
Let's say that `$S$' and `$R$' are our premises (although they don't have to be to use the conjunction introduction rule), and so they are on lines 1 and 2. Then on any subsequent line---but, in this case, it will be line 3---we can get `$S \eand R$' by using the conjunction introduction rule ($\eand$I).
\begin{proof}
	\hypo{a}{S} \pr{}
	\hypo{b}{R} \pr{}
	\have{c}{S \eand R} \ai{a, b}
\end{proof}

Every line of our proof must either be an assumption (and remember that a premise is an assumption), or it must be justified by some rule. Therefore, on line 3, we put `$\eand$I 1, 2' to indicate that `$S \eand R$' was obtained by applying the conjunction introduction rule to lines 1 and 2. 

The conjunction introduction rule introduces a sentence with `$\eand$' as the main connective. We also have a rule that eliminates that connective. Suppose someone tells you that \textit{Jeff is eating and Mary is sleeping}. Assuming that whoever told you this is reliable, you are entitled to infer simply that \textit{Jeff is eating}. You are also entitled to infer that \textit{Mary is sleeping}. These are applications of the \define{conjunction elimination rule} (which is actually two similar rules).

\begin{factboxy-width}[width=7.5cm]{conjunction elimination rule}
\begin{proof}
	\have[m]{ab}{\meta{A}\eand\meta{B}}
	\have[\ ]{a}{\meta{A}} \ae{ab}
\end{proof}
\textit{and equally:}
\begin{proof}
	\have[m]{ab}{\meta{A}\eand\meta{B}}
	\have[\ ]{b}{\meta{B}} \ae{ab}
\end{proof}
\end{factboxy-width}
\noindent When you have a conjunction on one line of a proof, you can use the conjunction elimination rule to obtain either of the conjuncts on a new line. You can only, however, apply this rule when the `$\eand$' is the main logical operator. So, for instance, you cannot use the conjunction elimination rule to obtain `$D$' from `$C \eor (D \eand E)$'. \textbf{Each of the rules of derivation can only be applied to the main logical operator of a sentence.}

With just these two rules, we can start to see how our natural deduction system works. Let's construct a proof for this argument: 
\begin{earg}
\item[] $(A\eor B) \eand (G \eand H) \proves (A\eor B)\eand H$
\end{earg}
The main logical operator in both the premise and conclusion of this argument is `$\eand$', and so we will use both of our conjunction rules in the proof. We begin by writing down the premise, and we draw a line below it. Everything after this line must follow from our premise by the application of our rules. 
\begin{proof}
	\hypo{ab}{(A\eor B) \eand (G\eand H)} \pr{}
\end{proof}
From the premise, we can eliminate the main connective (and only the main connective) using the conjunction elimination rule. Using this rule twice gives us this:
\begin{proof}
	\hypo{ab}{(A\eor B) \eand (G\eand H)} \pr{}
	\have{a}{(A\eor B)} \ae{ab}
	\have{b}{(G\eand H)} \ae{ab}
\end{proof}
Now that `$G\eand H$' is on its own line, we can use the conjunction elimination rule again to get $H$ on a line by itself. 
\begin{proof}
	\hypo{ab}{(A\eor B) \eand (G\eand H)} \pr{}
	\have{a}{(A\eor B)} \ae{ab}
	\have{b}{(G\eand H)} \ae{ab}
	\have{b2}{H} \ae{b}
\end{proof}
In our final step, we use the conjunction introduction rule to get the conclusion, `$(A\eor B)\eand H$'.
\begin{proof}
	\hypo{ab}{(A\eor B) \eand (G\eand H)} \pr{}
	\have{a}{(A\eor B)} \ae{ab}
	\have{b}{(G\eand H)} \ae{ab}
	\have{b2}{H} \ae{b}
	\have{c}{(A\eor B)\eand H} \ai{a,b2}
\end{proof}
And we're done. Notice that there is nothing in this representation of the proof to indicate that the last line is the conclusion. It's only because we began with `$(A\eor B) \eand (G \eand H) \proves (A\eor B)\eand H$' that we know that we have arrived at the conclusion that we wanted.
 

\section{Disjunction intro and elim}\label{s:disj-rule}

% Hopefully, grasping the conjunction introduction and conjunction elimination rules was not too difficult, and you saw that you already understood the logic of each rule. As we proceed, it may surprise you to be told that you already, more or less, understand the logic of the rest of the rules of derivation (with the possible exception of the biconditional introduction and elimination rules, since the biconditional isn't often used precisely in English). These are, after all, \textit{natural} deduction rules. Seeing them in this context makes it easy to forget this, but you should keep it in mind. We have also, already, discussed the logical operators in chapter \ref{s:TFLConnectives} and chapter \ref{s:CharacteristicTruthTables} and you should to try to integrate what you learned in those chapters with the rules of derivation.

\paragraph{Disjunction elimination}
For the disjunction rules, let's start with this example: 
\begin{ebullet}
	\item[] \textit{Sarah is swimming or Jeff is eating a burrito}. 
\end{ebullet}
When is this true? Recall from section \ref{s:ConnectiveDisjunction} that we are using the inclusive-or. So, \textit{Sarah is swimming or Jeff is eating a burrito} is true when
\begin{ebullet}
	\item[(a)] \textit{Sarah is swimming} is true, or 
	\item[(b)] \textit{Jeff is eating a burrito} is true, or 
	\item[(c)] both are true. 
\end{ebullet}
That's a lot of options, but if all we know is that Sarah is swimming or Jeff is eating a burrito, then we don't know precisely what either one of them are doing. But let's say that someone (whom we trust completely) tells us that, actually, Sarah is \textit{not} swimming. That piece of information about Sarah, then, let's us safely infer that Jeff is eating a burrito. This reasoning process is captured by the \define{disjunction elimination rule}.

\medskip
\begin{tcbraster}[raster columns=2, raster valign=top]
\begin{factboxy}{disjunction elimination rule}
\begin{proof}
	\have[m]{ab}{\meta{A}\eor\meta{B}}
	\have[n]{nb}{\enot\meta{B}}
	\have[\ ]{a}{\meta{A}} \oe{ab,nb}
\end{proof}

\begin{proof}
	\have[m]{ab}{\meta{A}\eor\meta{B}}
	\have[n]{na}{\enot\meta{A}}
	\have[\ ]{b}{\meta{B}} \oe{ab,nb}
\end{proof}
\end{factboxy}					% Don't put a blank line after this.
\begin{factboxy}{disjunction introduction rule}
\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[\ ]{ab}{\meta{A}\eor\meta{B}}\oi{a}
\end{proof}

\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[\ ]{ba}{\meta{B}\eor\meta{A}}\oi{a}
\end{proof}
\end{factboxy}
\end{tcbraster}


\paragraph{Disjunction introduction}
\textit{Sarah is swimming or Jeff is eating a burrito} is true when \textit{Sarah is swimming} is false (she isn't swimming) and \textit{Jeff is eating a burrito} is true. This quality of disjunctions lets us make an inference that we don't use often in our everyday lives. It is a very simple inference, however. Take any sentence. We'll use \textit{you are studying logic}. That's true. Since \textit{you are studying logic} is true, each one of these sentences is also true:

\begin{ebullet}
	\item[] You are studying logic, or you are studying German.
	\item[] You are studying logic, or your mother is in Fiji.
	\item[] You are studying logic, or a dragon is on the moon.
\end{ebullet}

\noindent The idea is that, if we know that a sentence is true, we can create a longer sentence by adding `or \textit{any sentence whatsoever}' and the disjunction will also be true. This feature of the disjunction underlies the \define{disjunction introduction rule} (which, again, is two similar rules).

%\meta{B} can be any sentence whatsoever. All that we need to use this rule is the line containing \meta{A} on some earlier line of the proof. Hence, the following is a perfectly acceptable:
%\begin{proof}
%	\hypo{m}{M} \pr{}
%	\have{mmm}{M \eor [(A\eiff B) \eif (C \eand D)]}\oi{m}
%\end{proof}


\subsection{Double negation}

The \define{double negation rule} is a rule of convenience that sometimes compliments the disjunction-elimination rule. (There are also times when it will be used in the proofs that are discussed in \ref{s:theorems}, but, for the material in this chapter, it will only be used with the disjunction-elimination rule.) First, notice that the disjunction-elimination rule is very specific. To use this rule, we need, on one line of our proof, a sentence of the form `$\meta{A}\eor\meta{B}$', and on another line of our proof, we need one side of the disjunction (either $\meta{A}$ or $\meta{B}$) with a `$\enot$' in front of it---that is, either $\enot\meta{A}$ or $\enot\meta{B}$. 

This presents a problem if we have these two sentences somewhere in a proof:
\begin{proof}
	\have[m]{np}{\enot P \eor Q}
	\have[n]{p}{P}
\end{proof}
You might think that, given those two lines, we can put `$Q$' on a new line like this:
\begin{proof}
	\have[m]{np}{\enot P \eor Q}
	\have[n]{p}{P}
	\have[\ ]{q}{Q} \oe{np,p}
\end{proof}
In a sense, this is the right idea for the disjunction elimination rule---one side of the disjunction `$\enot P \eor Q$' has to be true and the `$P$' on line $n$ means that `$\enot P$' is false. Hence, we should be allowed to put `$Q$' on a line by itself. The disjunction-elimination rule, however, does not permit this. To see why, let's distinguish between \define{negation} and \define{denial}.

\begin{factboxy}{negation and denial}
The \define{negation} of a sentence is the sentence with a `not' added to it.\\ 
The \define{denial} of a sentence is the sentence with either a `not' added or a `not' removed.
\end{factboxy}

\noindent For example, 
\begin{earg}
\item[1.] the negation of \textit{today is Tuesday} is \textit{today is not Tuesday}. 
\end{earg}
In TFL, 
\begin{earg}
\item[2.] the negation of $\meta{A}$ is $\enot\meta{A}$. The negation of $\enot\meta{A}$ is $\enot\enot\meta{A}$.
\end{earg}
Meanwhile,
\begin{earg}
\item[3.] the denial of \textit{it is not raining} is either (a) \textit{it is raining} or (b) \textit{it is not not raining}. 
\end{earg}
In TFL,
\begin{earg}
\item[4.] the denial of $\enot\meta{A}$ is either $\meta{A}$ or $\enot\enot\meta{A}$.
\end{earg}
\textbf{To use the disjunction elimination rule, we must have the \textit{negation} of one side of the disjunction on a separate line.} In the example above, we have, on line \textit{n}, the denial of $\enot P$, not its negation. The \define{double negation rule} helps us correct this so that we can use the disjunction elimination rule more often. 

Before we see how the double negation rule can help us with our derivation, let's introduce the rule. The first version of the double negation rule allows us to add two \textit{not}s (i.e., \textit{not not}) to a sentence in TFL---which, of course, will not change the sentence's truth value. The second version of the double negation rule allows us to remove two \textit{not}s, although needing to do this is less common. 

\begin{factboxy-width}[width=7.5cm]{double negation rule}
\begin{proof}
	\have[m]{a1}{\meta{A}}
	\have[n]{a2}{\enot\enot\meta{A}} \dn{a1}
\end{proof}

\begin{proof}
	\have[m]{a1}{\enot\enot\meta{A}}
	\have[n]{a2}{\meta{A}} \dn{a1}
\end{proof}
\end{factboxy-width}

Let's say that this is the argument for which we need to provide a proof: `$\enot P \eor Q, P \proves Q$'. After the premises, we use the double negation rule to get `$\enot \enot P$' from line 2.

\begin{proof}
	\hypo{ab}{\enot P \eor Q} \pr{}
	\hypo{nb}{P} \pr{} 
	\have{nnb}{\enot\enot P} \dn{nb}
\end{proof}
The `$P$' on line 2 and the `$\enot \enot P$' on line 3 have exactly the same meaning. The only difference between `$P$' and `$\enot \enot P$' is their form. But now that we have `$\enot \enot P$', we have the negation of what is on the left side of the disjunction (i.e., `$\enot P$'). That allows us to use the disjunction elimination rule, and we can get the conclusion.

\begin{proof}
	\hypo{ab}{\enot P \eor Q} \pr{}
	\hypo{nb}{P} \pr{}
	\have{nnb}{\enot \enot P} \dn{nb}
	\have{q}{Q} \oe{ab,nnb}
\end{proof}

Remember that, in this chapter, you will only use the double negation rule with the disjunction elimination rule, and you will only use it some of the time with that rule.

\begin{factboxy-side}{when not to use (left) and when to use (right) the double negation rule}
\begin{proof}
	\have[m]{ab}{\meta{A}\eor\meta{B}}
	\have[n]{nb}{\enot\meta{B}}
	\have[\ ]{a}{\meta{A}} \oe{ab,nb}
\end{proof}
\tcblower
\begin{proof}
	\have[m]{ab}{\meta{A}\eor\enot\meta{B}}
	\have[n]{b}{\meta{B}}
	\have[p]{nnb}{\enot\enot\meta{B}} \dn{b}
	\have[\ ]{a}{\meta{A}} \oe{ab,nnb}
\end{proof}
\end{factboxy-side}


%\tcbsidebyside[sidebyside adapt=both,
%enhanced,center,
%title=when to use the double negation rule,
%attach boxed title to top center={yshift=-2mm},
%coltitle=black,boxed title style={colback=red!25},
%segmentation style=solid,colback=red!5,colframe=red!50
%]{%
%\begin{proof}
%	\have[m]{ab}{\meta{A}\eor\meta{B}}
%	\have[n]{nb}{\enot\meta{B}}
%	\have[\ ]{a}{\meta{A}} \oe{ab,nb}
%\end{proof}
%}{%
%\begin{proof}
%	\have[m]{ab}{\meta{A}\eor\enot\meta{B}}
%	\have[n]{b}{\meta{B}}
%	\have[p]{nnb}{\enot\enot\meta{B}} \dn{b}
%	\have[\ ]{a}{\meta{A}} \oe{ab,nnb}
%\end{proof}
%}


\section{Conditional elimination}

For the conditional, we will cover the elimination rule now and the introduction rule in \S \ref{s:CI-rule}. Consider the following argument:
	\begin{earg}
	\item	If the envelope is on the table, then Aleksander is in the safe house.
	\item The envelope is on the table.
	\item Therefore, Aleksander is in the safe house.
	\end{earg}
In this argument---which is valid---we have a conditional and then, on a separate line, the antecedent of that conditional (`the envelope is on the table'). That allows us to safely infer the antecedent (`Aleksander is in the safe house'). In short, if we have a conditional and we know that the antecedent of the conditional is true, then we know that the consequent has to be true. (See also the discussion of the conditional on p.~\pageref{characteristic-tt-conditional}.) Deriving the consequent of the conditional in this way is an application of the \define{conditional elimination rule}.
This rule is also sometimes called \emph{modus ponens}. When we use the rule, the conditional and the antecedent of the conditional can be separated from one another, and they can appear in any order.

\medskip
\begin{tcbraster}[raster columns=2, raster valign=top]
\begin{factboxy}{conditional elimination rule}\label{ce-rule}
\begin{proof}
	\have[m]{ab}{\meta{A}\eif\meta{B}}
	\have[n]{a}{\meta{A}}
	\have[\ ]{b}{\meta{B}} \ce{ab,a}
\end{proof}
\end{factboxy}
\begin{factboxy}{biconditional elimination rule}\label{be-rule}
\begin{proof}
	\have[m]{ab}{\meta{A}\eiff\meta{B}}
	\have[n]{a}{\meta{A}}
	\have[\ ]{b}{\meta{B}} \be{ab,a}
\end{proof}

\begin{proof}
	\have[m]{ab}{\meta{A}\eiff\meta{B}}
	\have[n]{a}{\meta{B}}
	\have[\ ]{b}{\meta{A}} \be{ab,a}
\end{proof}
\end{factboxy}
\end{tcbraster}


\section{Biconditional intro and elim}\label{s:bi-rules}

The \define{biconditional elimination rule} is similar to the conditional elimination rule but a bit more flexible. If you have a biconditional and the left-hand side of the biconditional on another line, you can put the right-hand side on a new line. Similarly, if you have the right-hand side, you can put the left-hand side on a new line. Notice the difference between the conditional elimination and the biconditional elimination rule. There are two ways to use the biconditional elimination rule. There is only one way to use the conditional elimination rule.

In chapter \ref{s:CharacteristicTruthTables}, we said that the biconditional is ``the conjunction of a conditional running in each direction.'' This is the foundation for the \define{biconditional introduction rule}. If we have both conditionals, $\meta{A} \eif \meta{B}$ and $\meta{B} \eif \meta{A}$, then we can put $\meta{A} \eiff \meta{B}$ on a new line.

\begin{factboxy-width}[width=7.5cm]{biconditional introduction rule}
\begin{proof}\label{eiff-I}
    \have[m]{ab}{\meta{A}\eif\meta{B}} 
    \have[n]{ba}{\meta{B}\eif\meta{A}}
	\have[\ ]{b}{\meta{A}\eiff\meta{B}} \bi{ab,ba}
\end{proof}
\end{factboxy-width}



%The order of the $\meta{A}$ and $\meta{B}$ in $\meta{A}\eiff\meta{B}$ has to match their order in the first conditional in the conjunction, but since, typically, that line will have to be generated using the conjunction introduction rule, those conjuncts  can be put in either order then. \textbf{Also, notice that when we cite the rule, we use $\eiff$ex, not $\eiff$I.}


\section{Some examples}

We will now review some proofs that use the rules that are covered in \S\S \ref{s:conj-rule} -- \ref{s:bi-rules}. 

\begin{earg}
\item[\ex{14.6.1}] A proof of `$P \eand Q, \enot R \proves Q \eand \enot R$' requires the conjunction introduction rule and the conjunction elimination rule.
\begin{proof}
	\hypo{p1}{P \eand Q} \pr{}
	\hypo{p2}{\enot R} \pr{}
	\have{q}{Q} \ae{p1}
	\have{c}{Q \eand \enot R} \ai{p2,q}
\end{proof}\medskip

\item[\ex{14.6.2}] For a proof of `$P \eor Q, R \eand \enot Q \proves P \eor S$', we need the conjunction elimination rule, disjunction introduction rule, and the disjunction elimination rule.
\begin{proof}
	\hypo{p1}{P \eor Q} \pr{}
	\hypo{p2}{R \eand \enot Q} \pr{}
	\have{q}{\enot Q} \ae{p2}
	\have{p}{P} \oe{p1,q}
	\have{c}{P \eor S} \oi{p}
\end{proof}\medskip

\item[\ex{14.6.2b}] For `$C \eand (D \eor \enot F), F \eand G \proves C \eand (D \eor H)$', we use all five of the rules introduced in sections \ref{s:conj-rule} and \ref{s:disj-rule}.
\begin{proof}
	\hypo{p1}{C \eand (D \eor \enot F)} \pr{}
	\hypo{p2}{F \eand G} \pr{}
	\have{c}{C} \ae{p1}
	\have{dnf}{D \eor \enot F} \ae{p1}
	\have{f}{F} \ae{p2}
	\have{nnf}{\enot \enot F} \dn{f}
	\have{d}{D} \oe{dnf, nnf}
	\have{dh}{D \eor H} \oi{d}
	\have{con}{C \eand (D \eor H)} \ai{c,dh}
\end{proof}\medskip

\item[\ex{14.6.3}] For a proof of `$P \eif Q, R \eand P \proves Q \eand R$', we use the conjunction introduction rule, the conjunction elimination rule, and the conditional elimination rule.
\begin{proof}
	\hypo{p1}{P \eif Q} \pr{}
	\hypo{p2}{R \eand P} \pr{}
	\have{p}{P} \ae{p2}
	\have{r}{R} \ae{p2}
	\have{q}{Q} \ce{p1,p}
	\have{c}{Q \eand R} \ai{r,q}
\end{proof}\medskip

\item[\ex{14.6.4}] For a proof of `$R \eiff T, P \eor T, \enot P \proves R$', we use the disjunction elimination rule and the biconditional elimination rule.
\begin{proof}
	\hypo{p1}{R \eiff T} \pr{}
	\hypo{p2}{P \eor T} \pr{}
	\hypo{p3}{\enot P} \pr{}
	\have{t}{T} \oe{p2,p3}
	\have{c}{R} \be{p1,t}
\end{proof}\medskip

\item[\ex{14.6.5}] And last, a proof for $$(S \eif T) \eor \enot R, (T \eif S) \eor Q, R \eand \enot Q \proves T \eiff S$$ requires the conjunction elimination rule, the double negation rule, the disjunction elimination rule, and the biconditional introduction rule.
\begin{proof}
	\hypo{p1}{(S \eif T) \eor \enot R} \pr{}
	\hypo{p2}{(T \eif S) \eor Q} \pr{}
	\hypo{p3}{R \eand \enot Q} \pr{}
	\have{r}{R} \ae{p3}
	\have{q}{\enot Q} \ae{p3}
	\have{nnr}{\enot \enot R} \dn{r}
	\have{st}{S \eif T} \oe{p1,nnr}
	\have{ts}{T \eif S} \oe{p2,q}
%	\have{con}{(T \eif S) \eand (S \eif T)} \ai{st,ts}
	\have{c}{T \eiff S} \bi{st,ts}
\end{proof}\medskip
\end{earg}


\section{Conditional introduction}
\label{s:CI-rule}

The \define{conditional introduction rule} is a little bit more complicated than the conditional elimination rule, but, with some thought (and some practice), it is easily grasped. We'll start with this symbolization key for the sentence letters \textit{L}, \textit{R}, and \textit{T}:
	\begin{ekey}
		\item[T] Today is Tuesday.
		\item[L] Kate has logic class today.
		\item[R] It is raining.
	\end{ekey}
And this is our argument: 
$$T \eand L \proves R \eif L$$
 
Maybe you can see that this argument is valid. But if you can't right now, that's fine. We will go through the proof for this argument, and in the process explain the conditional introduction rule. We start by listing the premise.
	\begin{proof}
		\hypo{r}{T \eand L} \pr{}
	\end{proof}
Next, we need to make a new assumption: `It is raining'. We might say that we're making this assumption ``for the sake of argument'' or to see where it leads. To indicate that this is an assumption that we have supplied, we put `$R$' on line 2 this way:
	\begin{proof}
		\hypo{r}{T \eand L} \pr{}
		\open
			\hypo{l}{R} \as{}
	\end{proof}
You will notice right away that the `$R$' is indented. Whenever we make an assumption ourselves, we must indent it and the lines that follow. This creates a \define{subproof} that is set off from the rest of the proof. The assumption is cited with `AS', and we put a line under the assumption just as we do with the premises.

With this extra assumption in place, we next use $\eand$E to get $L$ on a line by itself.
	\begin{proof}
		\hypo{r}{T \eand L} \pr{}
		\open
			\hypo{l}{R} \as{}
			\have{rl}{L}\ae{r}
	\end{proof}
The idea for the first three lines of this proof are, first, we know that \textit{today is Tuesday and Kate has logic class today} (or, at least, we are assuming that `$ T \eand L$' is true). Next, on line 2, we are in effect asking, ``What if \textit{it is raining} is true?'' That is, what will follow if we make that assumption? Well, one thing that will follow is that Kate still has logic class today. 
In other words, using the conjunction elimination rule, we can put `$L$' on line 3. (\textit{Today is Tuesday} (i.e., `$T$') could go on line 3, but that won't get us any closer to the conclusion that we want.)

So, on line 2, we have asked, What if \textit{it is raining}? On line 3, we have one answer: \textit{Kate has logic class today}. Therefore, on line 4, we can put these two together as `if it is raining, then Kate has logic class today' with the conditional introduction rule.
	\begin{proof}
		\hypo{r}{T \eand L} \pr{}
		\open
			\hypo{l}{R} \as{}
			\have{rl}{L}\ae{r}
			\close
		\have{con}{R \eif L}\ci{l-rl}
	\end{proof}
For this final step, we have gone back to the original vertical line of the proof. 

When we use the conditional introduction rule, the assumption that we make will always be the antecedent of the conditional. The last line of the subproof, meanwhile, will always be the consequent of the conditional. 

\begin{factboxy}{conditional introduction rule}
\footnotesize{Begin by making an assumption, \meta{A}. From that assumption, derive \meta{B}. Once that is done, you know that \textit{if} \meta{A}, then \meta{B}, and you can put the conditional on the line after the subproof.} 
	\begin{proof}
		\open
			\hypo[i]{a}{\meta{A}} \as{}
			\have[j]{b}{\meta{B}}
		\close
		\have[\ ]{ab}{\meta{A}\eif\meta{B}}\ci{a-b}
	\end{proof}
\footnotesize{There can be as many or as few lines as needed between lines $i$ and $j$.} 
\end{factboxy}

Lines $i$ through $j$ are called a \define{subproof}. These are the rules for subproofs:

\begin{earg}
\item[1.] Once a subproof has been closed, none of the lines in the subproof can be used again. (The conditional $\meta{A}\eif\meta{B}$ can be used later in the proof because it is outside of the subproof.)
\item[2.] A subproof is closed by the application of the conditional introduction rule---or, as you will see shortly, the negation introduction or the negation elimination rules.
\item[3.] When we close a subproof, the assumption made at the beginning of the subproof has been \textit{discharged}.
\item[4.] A proof is not complete until every assumption that we have made (and so not counting the premises) is discharged.
\end{earg}


\section{Some more examples}

Let's go through a few more examples, each of which uses the conditional introduction rule.

\begin{earg}
\item Suppose we want a proof of this argument:
	$$P \eif Q, Q \eif R \proves P \eif R$$
We start by listing both of our premises. Next, since we want ($P \eif R$), we assume the antecedent of that conditional. 
\begin{proof}
	\hypo{pq}{P \eif Q} \pr{}
	\hypo{qr}{Q \eif R} \pr{}
	\open
		\hypo{p}{P} \as{}
	\close
\end{proof}
Now, even though it is an assumption that we've introduced, since `$P$' is on a line by itself (and the subproof has not yet been closed), we can use it for our next step. With `$P$' and the `$P \eif Q$' on line 1, we can use {\eif}E to get `$Q$'. 
\begin{proof}
	\hypo{pq}{P \eif Q} \pr{}
	\hypo{qr}{Q \eif R} \pr{}
	\open
		\hypo{p}{P}\as{}
		\have{q}{Q}\ce{pq,p}
%		\have{r}{R}\ce{qr,q}
	\close
%	\have{pr}{P \eif R}\ci{p-r}
\end{proof}
With the $Q$ on line 4 and $Q \eif R$ on line 2, we can use {\eif}E and get $R$. So, by assuming `$P$', we were able to get `$R$'. Last, we apply the {\eif}I rule, which discharges our assumption and completes the proof.
\label{HSproof}
\begin{proof}
	\hypo{pq}{P \eif Q} \pr{}
	\hypo{qr}{Q \eif R} \pr{}
	\open
		\hypo{p}{P}\as{}
		\have{q}{Q}\ce{pq,p}
		\have{r}{R}\ce{qr,q}
	\close
	\have{pr}{P \eif R}\ci{p-r}
\end{proof}

\item Next, let's construct a proof for this argument: `$F \eif (G \eand H) \proves F \eif G$'. We proceed this way:

\begin{proof}
	\hypo{fgh}{F\eif(G\eand H)} \pr{}
	\open
		\hypo{f}{F}\as{}
		\have{gh}{G \eand H}\ce{fgh,f}
		\have{g}{G}\ae{gh}
	\close
	\have{fg}{F \eif G}\ci{f-g}
\end{proof}

\item As you know, the biconditional elimination rule is similar to the conditional elimination rule. (But they are not the same. See p.~\pageref{ce-rule} to compare them.) We should also, however, be able to start with a biconditional, say `$B \eiff C$' and derive either of the conditionals: `$B \eif C$' or `$C \eif B$'. This is easily done with the conditional introduction rule.
\begin{proof}
	\hypo{bc}{B \eiff C} \pr{}
	\open
		\hypo{b}{B}\as{}
		\have{c}{C}\be{bc,b}
	\close
	\have{bc2}{B \eif C}\ci{b-c}
\end{proof}
\smallskip
\noindent And, with a similar proof, we can also derive `$C \eif B$'.

\item In the proof for `$\enot P \eor (R \eand Q) \proves P \eif Q$', we will use the conditional introduction rule as well as double negation rule and disjunction-elimination rule. 

\begin{proof}
	\hypo{p1}{\enot P \eor (R \eand Q)} \pr{}	
	\open
		\hypo{p}{P}\as{}
		\have{dn}{\enot\enot P} \dn{p}
		\have{rq}{R \eand Q} \oe{p1,dn}
		\have{q}{Q} \ae{rq}
	\close
	\have{c}{P \eif Q}\ci{p-q}
\end{proof}

%\begin{notebox}
%In fact, in some logic textbooks, this is biconditional elimination rule:
%\begin{proof}\label{eiff-I}
%    \have[m]{ab}{\meta{A} \eiff \meta{B}} 
%	\have[\ ]{b}{\meta{A}\eif\meta{B}} \be{ab}
%\end{proof}
%\begin{proof}\label{eiff-I}
%    \have[m]{ab}{\meta{A} \eiff \meta{B}}
%	\have[\ ]{b}{\meta{B}\eif\meta{A}}\be{ab}
%\end{proof}
%\end{notebox}

\end{earg}


\section{Negation introduction and elimination}

Here is a simple mathematical argument in English:
\begin{earg}
\item[1.] Assume that there is some greatest natural number. Call it $G$.
\item[2.] That number plus one is also a natural number.
\item[3.] $G+1$ is greater than $G$.
\item[4.] Thus, $G$ is the greatest natural number (accordingt to 1), and there is a natural number greater than $G$ (according to 3).
\item[5.] The previous line is a contradiction.
\item[6.] Therefore, the assumption that we made on line 1 is false. There is no greatest natural number.
\end{earg}
This type of argument is traditionally called a \emph{reductio}. Its full Latin name is \emph{reductio ad absurdum}, which means `reduction to absurdity' (although \textit{absurdity} in the sense that we often use it today isn't part of this). In a reductio, we assume something for the sake of argument---for example, that there is a greatest natural number. Then we show that the assumption leads to two contradictory sentences---for example, `$G$ is the greatest natural number' and `$G$ is not the greatest natural number.' In this way, we have shown that the original assumption must be false, which means that the denial of the assumption is true. 

Our two negation rules (which are basically the same rule) formalize this reasoning process.

\medskip
\begin{tcbraster}[raster columns=2, raster valign=top]
\begin{factboxy}{negation introduction rule}
\begin{proof}
\open
	\hypo[m]{na}{\meta{A}}\as{}
	\have[n]{b}{\meta{B}}
	\have[p]{nb}{\enot\meta{B}}
\close
\have[\ ]{a}{\enot\meta{A}}\ni{na-nb}
\end{proof}
\end{factboxy}
\begin{factboxy}{negation elimination rule}
\begin{proof}
\open
	\hypo[m]{na}{\enot\meta{A}}\as{}
	\have[n]{b}{\meta{B}}
	\have[p]{nb}{\enot\meta{B}}
\close
\have[\ ]{a}{\meta{A}}\ne{na-nb}
\end{proof}
\end{factboxy}
\end{tcbraster}
\medskip

\noindent Notice that just as we do when using the conditional introduction rule, we begin by making an assumption. The subproof that follows is indented, and the assumption that we made must be discharged by applying either the negation introduction rule or the negation elimination rule. 

When using either of the negation rules, the last two lines of the subproof must be an explicit contradiction: \meta{B} on one line and its negation, $\enot\meta{B}$, on the next line (or vice versa). Those two lines cannot be separated. When you cite the rule, however, the lines that you give are the lines for the whole subproof (starting with the assumption), not just the two lines containing the contradiction. 

%To see how the negation ... rule works, suppose we want to prove the law of non-contradiction: $\enot(G \eand \enot G)$. We can prove this without any premises by immediately starting a subproof. We want to apply {\enot}I to the subproof, so we assume $(G \eand \enot G)$. We then get an explicit contradiction by {\eand}E. The proof looks like this:

%\begin{proof}
%	\open
%		\hypo{gng}{G\eand \enot G}\as{}
%		\have{g}{G}\ae{gng}
%		\have{ng}{\enot G}\ae{gng}
%	\close
%	\have{ngng}{\enot(G \eand \enot G)}\ni{gng-ng}
%\end{proof}

\subsection{Reiteration}

To get a contradiction on the last two lines of a subproof, you will usually have to move a sentence that is on an earlier line to the last or second-to-last line of the subproof. This is done with the \define{reiteration rule}. Just as the double negation rule is a rule of convenience that sometimes compliments the disjunction-elimination rule, the reiteration rule is a rule of convenience that usually compliments the negation elimination and negation introduction rules.  

\begin{factboxy-width}[width=7.5cm]{reiteration rule}
\begin{proof}
	\have[m]{a1}{\meta{A}}
	\have[n]{a2}{\meta{A}} \reit{a1}
\end{proof}
\end{factboxy-width}

To demonstrate both the negation elimination rule and the reiteration rule, we will go through the proof for this argument: `$\enot P \eif \enot Q, Q \proves P$'. Looking at the argument, you'll notice that our conclusion is `$P$', but we cannot get `$P$' by using $\eand$E, $\eor$E, $\eif$E, or $\eiff$E. That tells us that we will need to use one of our negation rules.

After the premises, we make the assumption that we need for negation elimination. Since, ultimately, we want `$P$', we will assume `$\enot P$', knowing, of course, that once we discharge that assumption (and close the subproof), we will have `$P$'.

\begin{proof}
	\hypo{p1}{\enot P \eif \enot Q} \pr{}	
	\hypo{p2}{Q} \pr{}
	\open
		\hypo{np}{\enot P}\as{}
\end{proof}
We then use the conditional elimination rule to get $\enot Q$ on line 4. 
\begin{proof}
	\hypo{p1}{\enot P \eif \enot Q} \pr{}	
	\hypo{p2}{Q} \pr{}
	\open
		\hypo{np}{\enot P}\as{}
		\have{nq}{\enot Q}\ce{p1,np}
%		\have{ng}{Q}\reit{p2}
\end{proof}
The $Q$ on line 2 and $\enot Q$ on line 4 are a contradiction, but to use the negation elimination rule we need to have $Q$ on line 5. To get it there, we use the reiteration rule. 

\begin{proof}
	\hypo{p1}{\enot P \eif \enot Q} \pr{}	
	\hypo{p2}{Q} \pr{}
	\open
		\hypo{np}{\enot P}\as{}
		\have{nq}{\enot Q}\ce{p1,np}
		\have{ng}{Q}\reit{p2}
\end{proof}
Now that $\enot Q$ and $Q$ are on consecutive lines, we can use $\enot$E to discharge the assumption that we made, and that gives us the conclusion we are after: $P$.

\begin{proof}
	\hypo{p1}{\enot P \eif \enot Q} \pr{}	
	\hypo{p2}{Q} \pr{}
	\open
		\hypo{np}{\enot P}\as{}
		\have{nq}{\enot Q}\ce{p1,np}
		\have{ng}{Q}\reit{p2}
	\close
	\have{c}{P}\ne{np-ng}
\end{proof}

We just used the negation elimination rule. The negation introduction rule is, essentially, the same. Whether you use the negation introduction rule or negation elimination rule is just a function of whether you want `$\enot \meta{A}$' or `$\meta{A}$' on the line after the subproof.


\newpage

\section{Even more examples}

The negation introduction rule or the negation elimination rule is used in each of these proofs.

\begin{earg}

\item $P \eif Q, \enot Q \proves \enot P$

\begin{proof}
	\hypo{p1}{P \eif Q} \pr{}
	\hypo{p2}{\enot Q} \pr{}	
	\open
		\hypo{p}{P} \as{}
		\have{q}{Q} \ce{p1,p}
		\have{nq}{\enot Q} \reit{p2}
	\close
	\have{c}{\enot P} \ni{p-nq}
\end{proof}
\medskip


\item $P \eif \enot Q \proves \enot(P \eand Q)$

\begin{proof}
	\hypo{p1}{P \eif \enot Q} \pr{}	
	\open
		\hypo{pq}{P \eand Q} \as{}
		\have{p}{P} \ae{pq}
		\have{nq}{\enot Q} \ce{p1,p}
		\have{q}{Q} \ae{pq}
	\close
	\have{c}{\enot(P \eand Q)} \ni{pq-q}
\end{proof}

\filbreak
\item $B \eand C, \enot(B \eand D) \proves \enot D$

\begin{proof}
	\hypo{p1}{B \eand C} \pr{}	
	\hypo{p2}{\enot(B \eand D)} \pr{}
	\open
		\hypo{a}{D} \as{}
		\have{b}{B} \ae{p1}
		\have{ba}{B \eand D} \ai{a,b}
		\have{p2b}{\enot(B \eand D)} \reit{p2}
	\close
	\have{c}{\enot D} \ni{a-p2b}
\end{proof}
\medskip

\item A proof for `$\enot P \proves P \eif Q$' requires two subproofs. First, we assume `$P$' so that we can use the conditional introduction rule at the end of the proof. Then, we assume `$\enot Q$' so that we can use the negation elimination rule and get `$Q$' on the last line of the first subproof.

\begin{proof}
	\hypo{p1}{\enot P} \pr{}	
	\open
		\hypo{p}{P}	\as{}
		\open
			\hypo{nq}{\enot Q} \as{}
			\have{p2}{P} \reit{p}
			\have{np}{\enot P} \reit{p1}
		\close
		\have{q}{Q} \ne{nq-np}
	\close
	\have{c}{P \eif Q} \ci{p-q}
\end{proof}


\end{earg}



\section{Invalid arguments}

In this chapter, we have taken it for granted that each argument that we have encountered has been valid. The purpose of providing a proof is (1) to confirm that it is valid and (2) to show why it is valid---that is, to lay out each step that takes us from the premises to the conclusion. If an argument is invalid, however, we are stuck. It is impossible to provide a correct proof of an invalid argument using the rules introduced in this chapter. At the same time, not being able to provide a proof for an argument doesn't mean that the argument is invalid. Perhaps the proof is just too complicated for us to figure out. 

In chapter \ref{s:NDVeryIdea}, we discussed some reasons to prefer natural deduction to truth tables for checking that an argument is valid. To show that an argument is invalid, however, creating a truth table is not merely a superior method, it is our only option.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercises for chapter on proofs

\section{Practice exercises}
\setcounter{ProbPart}{0}

\problempart
Give a proof for each argument.
\begin{earg}
\item $\enot P \eif (Q \eor P), \enot P \proves Q$\smallskip
\item $D \eand H, H \eiff J  \proves J \eor N$\smallskip 
\item $G \eand (H \eand J), (H \eor J) \eif K \proves K$\smallskip
\item $P \eand (Q \eor R), P \eif \enot R \proves Q \eor S$\smallskip
\end{earg}


\problempart
Give a proof for each argument.
\begin{earg}
\item $P \eif (Q \eif R) \proves (P \eand Q) \eif R$\smallskip
\item $Q \eif R \proves (Q \eand S) \eif (R \eor T)$\smallskip 
\item $M \eand (\enot N \eif \enot M) \proves (N \eand M) \eor \enot M$\smallskip
\item $(Z\eand K)\eiff(Y\eand M), D\eand(D\eif M) \proves Y\eif Z$\smallskip
\item $C\eif(E\eand G), \enot C \eif G \proves G$\smallskip
\item $\enot(P \eif Q) \proves \enot Q$\smallskip
\item $S \eiff T \proves S \eiff (T \eor S)$\smallskip 
\item $D \eor F, D \eif G, F \eif H \proves G \eor H$\smallskip
\item $(W \eor X) \eor (Y \eor Z), X\eif Y, \enot Z \proves W\eor Y$
\end{earg}


\problempart
If you know that a proof can be given for $\meta{A}\proves\meta{B}$ (that is, you know that the argument is valid), then is it possible to know if $(\meta{A} \eand \meta{C}) \proves \meta{B}$ is valid? Is it possible to know if $(\meta{A} \eif \meta{B}) \eif \meta{B} \proves \meta{B}$ is valid? Explain your answers.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  answers

\section{Answers}
\setcounter{ProbPart}{0}

\problempart
\begin{earg}
\item $\enot P \eif (Q \eor P), \enot P \proves Q$
\begin{proof}
	\hypo{p1}{\enot P \eif (Q \eor P)} \pr{}
	\hypo{p2}{\enot P} \pr{}
	\have{qp}{Q \eor P} \ce{p1,p2}
	\have{q}{Q} \oe{p2,qp}
\end{proof}
\medskip

\item $D \eand H, H \eiff J  \proves J \eor N$ 
\begin{proof}
	\hypo{p1}{D \eand H} \pr{}
	\hypo{p2}{H \eiff J} \pr{}
	\have{h}{H} \ae{p1}
	\have{j}{J} \be{p2,h}
	\have{jn}{J \eor N} \oi{j}
\end{proof}
\medskip

\filbreak
\item $G \eand (H \eand J), (H \eor J) \eif K \proves K$
\begin{proof}
	\hypo{p1}{G \eand (H \eand J)} \pr{}
	\hypo{p2}{(H \eor J) \eif K} \pr{}
	\have{hj}{H \eand J} \ae{p1}
	\have{h}{H} \ae{hj}
	\have{hj2}{H \eor J} \oi{h}
	\have{k}{K} \ci{p2,hj2}
\end{proof}
\medskip


\item $P \eand (Q \eor R), P \eif \enot R \proves Q \eor S$
\begin{proof}
	\hypo{p1}{P \eand (Q \eor R)} \pr{}
	\hypo{p2}{P \eif \enot R} \pr{}
	\have{p}{P} \ae{p1}
	\have{nr}{\enot R} \ci{p2,p}
	\have{qr}{Q \eor R} \ae{p1}
	\have{q}{Q} \oe{nr,qr}
	\have{qs}{Q \eor S} \oi{q}
\end{proof}
\medskip
\end{earg}

\filbreak
\problempart
\begin{earg}
\item $P \eif (Q \eif R) \proves (P \eand Q) \eif R$
\begin{proof}
	\hypo{p1}{P \eif (Q \eif R)} \pr{}
	\open
	\hypo{as}{P \eand Q} \as{}
	\have{p}{P} \ae{as}
	\have{qr}{Q \eif R} \ce{p1,p}
	\have{q}{Q} \ae{as}
	\have{r}{R} \ce{qr,q}
	\close
	\have{con}{(P \eand Q) \eif R} \ci{as-r}
\end{proof}
\medskip


\item $Q \eif R \proves (Q \eand S) \eif (R \eor T)$ 
\begin{proof}
	\hypo{p1}{Q \eif R} \pr{}
	\open
	\hypo{as}{Q \eand S} \as{}
	\have{q}{Q} \ae{as}
	\have{r}{R} \ce{p1,q}
	\have{rt}{R \eor T} \oi{r}
	\close
	\have{con}{(Q \eand S) \eif (R \eor T)} \ci{as-rt}
\end{proof}
\medskip

\filbreak
\item $M \eand (\enot N \eif \enot M) \proves (N \eand M) \eor \enot M$
\begin{proof}
\hypo{MnNnM}{M \eand (\enot N \eif \enot M)} \pr{}
\have{M}{M}\ae{MnNnM}
\have{nNnM}{\enot N \eif \enot M}\ae{MnNnM}
\open
	\hypo{nN}{\enot N} \as{}
	\have{nM}{\enot M}\ce{nNnM, nN}
	\have{M2}{M} \reit{M}
\close
\have{N}{N} \ne{nN-M2}
\have{NM}{N \eand M}\ai{M,N}
\have{con}{(N \eand M) \eor \enot M}\oi{NM}	
\end{proof}
\medskip

\filbreak
\item $(Z\eand K)\eiff(Y\eand M), D\eand(D\eif M) \proves Y\eif Z$
\begin{proof}
\hypo{zkym}{(Z \eand K) \eiff (Y \eand M)} \pr{}
\hypo{ddm}{D \eand (D \eif M)} \pr{}
\have{d}{D}\ae{ddm}
\have{dm}{D \eif M}\ae{ddm}
\have{m}{M}\ce{d,dm}
\open
	\hypo{y}{Y} \as{}
	\have{ym}{Y \eand M}\ai{m,y}
	\have{zk}{Z \eand K}\be{zkym, ym}
	\have{z}{Z}\ae{zk}
\close
\have{yz}{Y \eif Z}\ci{y-z}
\end{proof}
\medskip

\filbreak
\item $C\eif(E\eand G), \enot C \eif G \proves G$
\begin{proof}
\hypo{ceg}{C \eif (E \eand G)} \pr{}
\hypo{ncg}{\enot C \eif G} \pr{}
\open
        \hypo{ng}{\enot G} \as{}
        \open
                \hypo{c}{C} \as{}
                \have{eg}{E \eand G}\ce{ceg, c}
                \have{g1}{G}\ae{eg}
                \have{ng2}{\enot G} \reit{ng}
        \close
        \have{nc}{\enot C}\ni{c-ng2}
        \have{g2}{G}\ce{ncg, nc}
        \have{ng3}{\enot G} \reit{ng}
\close
\have{con}{G}\ne{ng-ng3}
\end{proof}
\medskip

\item $\enot(P \eif Q) \proves \enot Q$
\begin{proof}
	\hypo{p1}{\enot(P \eif Q)} \pr{}
	\open
	\hypo{as}{Q} \as{}
		\open
		\hypo{as2}{P} \as{}
		\have{q}{Q} \reit{as}
		\close
	\have{pq}{P \eif Q} \ci{as2-q}
	\have{npq}{\enot(P \eif Q)} \reit{p1}
	\close
	\have{con}{\enot Q} \ni{as-npq}
\end{proof}
\medskip


\filbreak
\item $S \eiff T \proves S \eiff (T \eor S)$ 
\begin{proof}
	\hypo{p1}{S \eiff T} \pr{}
	\open
	\hypo{as1}{S} \as{}
	\have{t}{T} \be{p1,as1}
	\have{ts}{T \eor S} \oi{t}
	\close
	\have{c1}{S \eif (T \eor S)} \ci{as1-ts}
	\open
	\hypo{as2}{T \eor S} \as{}
		\open
		\hypo{as3}{\enot S} \as{}
		\have{t2}{T} \oe{as2,as3}
		\have{s2}{S} \be{p1,t2}
		\have{ns}{\enot S} \reit{as3}
		\close
	\have{s}{S} \ne{as3-ns}
	\close
	\have{c2}{(T \eor S) \eif S} \ci{as2-s}
%	\have{con1}{(S \eif (T \eor S)) \eand ((T \eor S) \eif S)} \ai{c1,c2}
	\have{con2}{S \eiff (T \eor S)} \bi{c1,c2}
\end{proof}
\medskip


\filbreak
\item $D \eor F, D \eif G, F \eif H \proves G \eor H$ 
\begin{proof}
	\hypo{p1}{D \eor F} \pr{}
	\hypo{p2}{D \eif G} \pr{}
	\hypo{p3}{F \eif H} \pr{}
	\open
	\hypo{as}{\enot(G \eor H)} \as{}
		\open
		\hypo{as2}{\enot D} \as{}
		\have{f}{F} \oe{p1,as2}
		\have{h}{H} \ce{p3,f}
		\have{gh}{G \eor H} \oi{h}
		\have{ngh}{\enot (G \eor H)} \reit{as}
		\close
	\have{d}{D} \ne{as2-ngh}
	\have{g}{G} \ce{p2,d}
	\have{gh2}{G \eor H} \oi{g}
	\have{ngh2}{\enot (G \eor H)} \reit{as}
	\close
	\have{con}{G \eor H} \ne{as-ngh2}
\end{proof}

\filbreak
\item $(W \eor X) \eor (Y \eor Z), X\eif Y, \enot Z \proves W\eor Y$
\begin{proof}
\hypo{p1}{(W \eor X ) \eor (Y \eor Z )} \pr{}
\hypo{p2}{X \eif Y} \pr{}
\hypo{p3}{\enot Z} \pr{}
\open
	\hypo{nwy}{\enot (W \eor Y)} \as{}
		\open
		\hypo{nwx}{\enot (W \eor X)} \as{}
		\have{yz}{Y \eor Z} \oe{p1,nwx}
		\have{y}{Y} \oe{p3,yz}
		\have{wy}{W \eor Y} \oi{y}
		\have{nwy2}{\enot (W \eor Y)} \reit{nwy}
		\close
	\have{wx}{W \eor X} \ne{nwx-nwy2}
		\open
		\hypo{x}{X} \as{}
		\have{y2}{Y} \ce{p2,x}
		\have{wy}{W \eor Y} \oi{y2}
		\have{nwy3}{\enot (W \eor Y)} \reit{nwy}
		\close
	\have{nx}{\enot X} \ni{x-nwy3}
	\have{w}{W} \oe{wx,nx}
	\have{wy2}{W \eor Y} \oi{w}
	\have{nwy4}{\enot (W \eor Y)} \reit{nwy}
	\close
\have{con}{W \eor Y} \ne{nwy-nwy4}
\end{proof}

\end{earg}
\bigskip

\problempart
If $\meta{A} \proves \meta{B}$ is valid, then $(\meta{A} \eand \meta{C})\proves \meta{B}$ is valid. We know this because if $\meta{A} \proves \meta{B}$, then there is some proof with assumption $\meta{A}$ that ends with $\meta{B}$, and no undischarged assumptions other than $\meta{A}$. Now, if we start a proof with assumption $(\meta{A} \eand \meta{C})$, we can obtain $\meta{A}$ by $\eand$E. We can now copy and paste the original proof of $\meta{B}$ from $\meta{A}$, adding 1 to every line number and line number citation. The result will be a proof of $\meta{B}$ from assumption $\meta{A}$.

We also know that $(\meta{A} \eif \meta{B}) \eif \meta{A} \proves \meta{B}$ is valid. Since, there is some proof with assumption $\meta{A}$ that ends with $\meta{B}$, we can assume \meta{A}, derive \meta{B}, and then use the conditional-introduction rule to get $(\meta{A} \eif \meta{B})$. We use the condtional-elimination rule on $(\meta{A} \eif \meta{B})$ and the premise, and get \meta{B}, the conclusion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% CHAPTER 16

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% CHAPTER 17

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Proofs in Carnap}\label{s:Carnap-proofs}

Creating proofs in Carnap is not difficult. To type the connectives, use the symbols on the right in table \ref{symbols-Carnap}. 

\begin{table*}\centering\sffamily\footnotesize
\ra{1.25}
\begin{tabular}{@{}l l@{}}\toprule
\textth{TFL connective} & \textth{in Carnap} \\\midrule
	\enot & $ \sim$\\
	\eand & \&\\
	\eor & v (lowercase v)\\
	\eif & $-$$>$ (dash, greater than sign)\\
	\eiff & $<$$-$$>$\\
\bottomrule
\end{tabular}
\caption{}\label{symbols-Carnap}
\end{table*}


Carnap will number the lines automatically. After the sentence on each line, there has to be a colon (`:') before the `PR', `AS', or the rule. Carnap is flexible with the spacing on a line, but as a guideline, put a tab space between the sentence and `:PR', `:AS', or the rule (:$\eif$E, :$\eor$I, etc.). Also indent subproofs with a tab space. (Carnap will let you use more or fewer spaces, but a subproof has to be indented some amount.)

To produce a proof, you are given an interface like the one shown in figure \ref{fig:proof-1a}. As you can see, the argument is given at the top. In this case, the premises are $P \eif \enot Q$ and $R \eand P$, and the conclusion is $\enot Q$. (The premises are separated by commas. The premises and the conclusion are separated by the turnstile (`$\proves$').)

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
%\centering
\includegraphics[width=9.5cm]{textbook--1a.PNG}
\caption{}
\label{fig:proof-1a}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Begin by listing the premises, and don't forget to put `:PR' after each one. If there is a problem with a line---either the sentence isn't formed correctly, the rule you've cited isn't being used correctly, or there's some other mistake---Carnap will put \textsf{?} or {\fontencoding{U}\fontfamily{futs}\selectfont\char 66\relax}  at the end of the line. When the line is ok, you will get a `+'.
% Once you complete each line, Carnap will give you the typographically correct version on the right (figure \ref{fig:proof-1b}). 
We finish this proof using the $\eand$E and $\eif$E rules (figure \ref{fig:proof-1c}). When the proof is correct, the box containing the argument will turn green, and the proof can be submitted. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
%\centering
\includegraphics[width=9.5cm]{textbook--1b.PNG}
\caption{}
\label{fig:proof-1b}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
%\centering
\includegraphics[width=9.5cm]{textbook--1c.PNG}
\caption{}
\label{fig:proof-1c}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our next example, $(P \eor Q) \proves (\enot P \eif Q)$, requires a subproof. We begin as before. To create the subproof, put a tab space before $\enot P$ and put `:AS' at the end of the line (figure \ref{fig:proof-2a}). Since the next line is also part of the subproof, we again need a tab before the $Q$. We end the subproof (and discharge the assumption) with the $\eif$I rule. $\enot P \eif Q$ is not indented (so no tabs or spaces before the $\enot P$). That's the conclusion, and so if everything is correct, Carnap will give you the green bar and you can submit the proof (figure \ref{fig:proof-2b}). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
%\centering
\includegraphics[width=9.5cm]{textbook--2a.PNG}
\caption{}
\label{fig:proof-2a}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
%\centering
\includegraphics[width=9.5cm]{textbook--2b.PNG}
\caption{}
\label{fig:proof-2b}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}
%\begin{subfigure}[hb]{0.5\textwidth}
%\centering
%\includegraphics[angle=90, height=13cm]{textbook--2a.PNG}
%\caption{}
%\label{fig:proof-2a}
%\end{subfigure}
%\begin{subfigure}[hb]{0.5\textwidth}
%\centering
%\includegraphics[angle=90, height=13cm]{textbook--2b.PNG}
%\caption{}
%\label{fig:proof-2b}
%\end{subfigure}
% \caption{}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%



As I said at the beginning of this chapter, creating proofs in Carnap is not difficult. You do have to be careful, however. Programming a language like TFL is relatively simple because there are only a small number of rules and, to produce proofs of valid arguments, we follow those rules very strictly. But as a consequence, Carnap is not designed to understand what you are trying to do if you deviate from the rules, even if it is a minor deviation or an innocent mistake. So, some reminders:
\begin{enumerate}
\itemsep-.3mm
	\item As long as `$\enot$' is not the main logical operator, you can drop the outermost parentheses. All other parentheses have to be used. 
	\item Capitalize `PR', `AS',`E', `I' (in the rules), and all atomic sentences.
	\item Don't forget the `:' right before PR, AS, or the rule that you are citing. 
	\item There is no space between the $\eand$, $\eor$, $\eif$, $\eiff$, or $\enot$ and the `E' or `I'.  
	\item There is a space (and no punctuation) after the `E' or `I'. 
	\item There is a comma between the two lines that have to be cited for $\eand$I, $\eor$E, $\eif$E, and $\eiff$E (e.g., `$\eif$E 2,4').
	\item There is a dash between the two lines that have to be cited for $\eif$I, $\enot$I, and $\enot$E (e.g., `$\enot$E 4-6').
%	\item The $\eiff$I rule requires dashes and a comma. (Check the format on p.~\pageref{eiff-I}.)
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\chapter{Constructing proofs}
There is no simple recipe for constructing proofs, and there is no substitute for practice. Here, however, are some questions to ask yourself and some strategies to keep in mind.

\begin{earg}
\item[\ex{18-1}] Do you know all of the rules? \textbf{If you don’t have them memorized yet, then they should be written on a sheet of paper that you have next to you while you’re working.}
\medskip

\item[\ex{18-2}] Are there steps that you can take without making an assumption? If yes, is it worth taking those steps?
\medskip

\item[\ex{18-3}] If you’re not sure how to proceed, but you can do conjunction elimination, conditional elimination, disjunction elimination, or biconditional elimination, then do them just to see what happens.
\medskip

\item[\ex{18-4}] If an assumption is needed, is it for $\eif$I, $\enot$I, or $\enot$E? \textbf{Don't make an assumption if you don't know which of these rules you plan to use when you close the sub-proof.}
\medskip

\item[\ex{18-5}] If an assumption is needed, what should it be? (If you want to get $P \eif Q$, then you’re going to use $\eif$I and your assumption should be $P$.)
\medskip

\item[\ex{18-6}] If you make an assumption, then you should know what you want on either the last line or the last two lines of your subproof.
\begin{earg}
\item[a.] If you’re using $\eif$I, then you will need the consequent of the conditional on the last line of the subproof. 
\item[b.] If you’re using $\enot$I or $\enot$E, then you need a contradiction on the last two lines of your subproof, although that can be any contradiction. It doesn't have to be related to the assumption.
\end{earg}
\medskip

\item[\ex{18-3a}] Sometimes it is useful to work backwards from the conclusion. The conclusion, of course, will be the last line of your proof, and you can, if you wish, put it at the bottom of the proof anytime. For example, let's say that you need to provide a proof for this argument: $P \eif (\enot Q \eif R) \proves (P \eand \enot Q) \eif R$. You can begin this way:
\begin{proof}
	\hypo{pqr}{P \eif (\enot Q \eif R)} \pr{}
		\have[\ ]{pq}{}
		\have[\ ]{p}{}
		\have[\ ]{qr}{}
		\have[\ ]{q}{}
	\have[\ ]{con}{(P \eand \enot Q) \eif R}
\end{proof}
\medskip

Knowing that you need to arrive at a conditional, you also know that you need to use the conditional-introduction rule, what your assumption should be, and what will be on the last line of your subproof.

\begin{proof}
	\hypo{pqr}{P \eif (\enot Q \eif R)} \pr{}
	\open
		\hypo{pq}{P \eand \enot Q}\as{}
		\have[\ ]{p}{}
		\have[\ ]{qr}{}
		\have[\ ]{r}{R}
	\close
	\have[\ ]{con}{(P \eand \enot Q) \eif R}\ci{}
\end{proof}

Sketching out a proof in this way is easy to do when you are writing on paper. If you are doing it in Carnap, be careful of the spacing that you put on each blank line.

\medskip

\item[\ex{18-7}] The negation introduction and negation elimination rules are a last resort. Use them when you can't use any of the other rules.
\medskip

\item[\ex{18-8}] \textbf{Persist}. Try different things. If one approach fails, then try something else.
\end{earg}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Proof-theoretic concepts}\label{s:ProofTheoreticConcepts}

\section{Theorems}\label{s:theorems}

You are familiar with arguments that have this form:
$$\meta{A}_1, \meta{A}_2, \ldots, \meta{A}_n \proves \meta{C}$$
We may also, however, have a sentence for which it is possible to give a proof with no premises: ${} \proves \meta{C}$. In this case, we say that $\meta{C}$ is a \define{theorem}.

\begin{factboxy}{Theorem}\label{def:syntactic_tautology_in_sl}
$\meta{C}$ is a \define{theorem} iff ~ $\proves \meta{C}$
\end{factboxy}

One such sentence is `$\enot (P \eand \enot P)$'. To show that this sentence is a theorem, we give a proof that has no premises and no undischarged assumptions. To get started, we do, however, have to make an assumption. We will assume `$P \eand \enot P$'. Once we show that this assumption leads to contradiction, we can discharge it and we will have `$\enot (P \eand \enot P)$'. This is the proof:
	\begin{proof}
		\open
			\hypo{con}{P \eand \enot P}
			\have{a}{P}\ae{con}
			\have{na}{\enot P}\ae{con}
		\close
		\have{lnc}{\enot (P \eand \enot P)}\ni{con-na}
	\end{proof}
This theorem, `$\proves \enot (P \eand \enot P)$' is an instance of what is sometimes called \emph{the Law of Non-Contradiction}.

To show that a sentence is a theorem, we just have to find a suitable proof. On the other hand, it is not possible to show that a sentence is \emph{not} a theorem this same way. To show that a sentence is not a theorem with our natural deduction system, we would have to demonstrate, not just that certain proof strategies fail, but that \emph{no} proof is possible. Even if we fail in trying to give a proof for a sentence in a thousand different ways, perhaps the proof is just too long and complex for us to figure out. 


\section{Equivalent, consistent, and inconsistent}

In \S\ref{equivalence--tt}, we defined \textit{equivalent} in terms of truth tables, namely, if two sentences have the same truth value on every line of a truth table, then they are equivalent. We can also show that two sentences are equivalent using our natural deduction system. To indicate that we have shown that the two sentences are equivalent with a derivation (or actually with two derivations), we will call this equivalence \define{provably equivalent}. 

\begin{factboxy}{Provably equivalent}
Two sentences \meta{A} and \meta{B} are \define{provably equivalent} iff each can be derived from the other. I.e., $\meta{A}\proves\meta{B}$ and $\meta{B}\proves\meta{A}$.\\
(Equivalently, \meta{A} and \meta{B} are \define{provably equivalent} if $\proves \meta{A} \eiff \meta{B}$.)
\end{factboxy}
        
As in the case of showing that a sentence is a theorem, it is relatively easy to show that two sentences are provably equivalent: it just requires a pair of proofs. Showing that sentences are \emph{not} provably equivalent is not possible for t he same reason that it isn't possible to show that a sentence is not a theorem. Even if we fail to produce two proofs showing that two sentences are provably equivalent, that doesn't mean that the proofs don't exist. It just means that we've failed to figure out what they are. 

We also, in \S\ref{consistency--tt}, defined \textit{jointly inconsistent} using truth tables: sentences are jointly inconsistent if there is no line on a truth table where they are all true. Again, we can show that two or more sentences are jointly inconsistent with our natural deduction system. 

\begin{factboxy}{Provably inconsistent}
The sentences $\meta{A}_1,\meta{A}_2,\ldots, \meta{A}_n$ are \define{provably inconsistent} iff, from them, a contradiction can be derived. I.e.\ $\meta{A}_1,\meta{A}_2,\ldots, \meta{A}_n \proves (\meta{B} \eand \enot \meta{B})$.
%\tcblower
%If they are not \define{inconsistent}, then $\meta{A}_1,\meta{A}_2,\ldots, \meta{A}_n$ are \define{provably consistent}.
\end{factboxy}
        
To show that a set of sentences are provably inconsistent, we use the sentences as premises and then derive a contradiction. (Any contradiction will do.) For instance, this proof demonstrates that $P \eand Q$ and $\enot P \eor \enot Q$ are provably inconsistent.

	\begin{proof}
	\hypo{p1}{P \eand Q} \pr{}
	\hypo{p2}{\enot P \eor \enot Q} \pr{}
	\have{p}{P} \ae{p1}
	\have{np}{\enot \enot P} \dn{p}
	\have{nq}{\enot Q} \oe{p2,np}
	\have{q}{Q} \ae{p1}
	\end{proof}

Showing that some set of sentences are \textit{not} provably inconsistent is, as you might guess at this point, not possible. Doing so would require showing, not just that we have failed to derive a contradiction from a set a sentences, but that no such derivation is possible.

Table \ref{table.one-mult-proofs} summarizes what we have covered in this chapter. As we will discuss in the next chapter, when the presence (or the absence) of a logical property cannot be demonstrated using our natural deduction system, we have to resort to using a truth table.

\begin{table*}\centering\sffamily\footnotesize
\ra{1.25}
\begin{tabular}{@{}l l l@{}}\toprule
\textth{To check} & \textth{that it is} & \textth{that it is not}\\\midrule
theorem & one proof & \textit{not possible with proofs}\\
equivalent & two proofs & \textit{not possible with proofs}\\
inconsistent &  one proof  & \textit{not possible with proofs}\\
consistent & \textit{not possible with proofs} & one proof\\
\bottomrule
\end{tabular}
\caption{This table summarizes what is required to check each of these logical notions.}\label{table.one-mult-proofs}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercises for chapter on proof theoretic concepts


\section{Practice exercises}
\setcounter{ProbPart}{0}


\problempart
Give a proof for each of these theorems.
\begin{earg}
\item $\proves O \eif O$\smallskip
\item $\proves N \eor \enot N$\smallskip
\item $\proves (P \eif Q) \eor (Q \eif P)$\smallskip
\item $\proves J \eiff [J\eor (L\eand\enot L)]$\smallskip
\item $\proves ((A \eif B) \eif A) \eif A$\smallskip 
\end{earg}


\problempart
Show that each of the following pairs of sentences are provably equivalent. (To indicate that the inference from the premise to the conclusion goes from the first sentence to the second and vice versa, we use the symbols $\leftproves\proves$.)
\begin{earg}
\item $T\eif S \leftproves\proves \enot S \eif \enot T$
\item $R \eif Q \leftproves\proves \enot(R \eand \enot Q)$
\end{earg}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  answers

\section{Answers}
\setcounter{ProbPart}{0}


\problempart
\begin{earg}
\item $\proves O \eif O$
\myanswer{\begin{proof}
\open
	\hypo{o}{O} \as{}
	\have{o2}{O}\by{R}{o}
\close
\have{con}{O \eif O}\ci{o-o2}
\end{proof}}
\bigskip

\item $\proves N \eor \enot N$
\begin{proof}
\open
        \hypo{ncon}{\enot (N \eor \enot N)} \as{}
        \open
                \hypo{n}{N} \as{}
                \have{nnn1}{N \eor \enot N}\oi{n}
                \have{ncon2}{\enot (N \eor \enot N)} \reit{ncon}
        \close
        \have{nn}{\enot N} \ni{n-ncon2}
        \have{nnn2}{N \eor \enot N} \oi{nn}
        \have{ncon3}{\enot (N \eor \enot N)} \reit{ncon}
\close
\have{con}{N \eor \enot N} \ne{ncon-ncon3}
\end{proof}
\bigskip

\filbreak
\item $\proves (P \eif Q) \eor (Q \eif P)$
\begin{proof}
\open
        \hypo{ncon}{\enot ((P \eif Q) \eor (Q \eif P))} \as{}
        \open
        		\hypo{p}{P} \as{}
        		\open
        			\hypo{nq}{\enot Q} \as{}
        			\open
        				\hypo{q}{Q} \as{}
        				\have{p2}{P} \reit{p}
				\close
				\have{qp}{Q \eif P} \ci{q-p2}
				\have{pqqp}{(P \eif Q) \eor (Q \eif P)} \oi{qp}
				\have{ncon2}{\enot ((P \eif Q) \eor (Q \eif P))} \reit{ncon}
			\close
			\have{q}{Q} \ne{nq-ncon2}
		\close
		\have{pq}{P \eif Q} \ci{p-q}
		\have{pqqp2}{(P \eif Q) \eor (Q \eif P)} \oi{pq}
		\have{ncon2}{\enot ((P \eif Q) \eor (Q \eif P))} \reit{ncon}
\close
\have{con}{(P \eif Q) \eor (Q \eif P)} \ne{ncon-ncon2}
\end{proof}
\bigskip

\filbreak
\item $\proves J \eiff (J\eor (L \eand \enot L))$
\myanswer{\begin{proof}
\open
	\hypo{j1}{J} \as{}
	\have{jlnl1}{J \eor (L \eand \enot L)}\oi{j1}
\close
\have{cond1}{J \eif (J\eor (L \eand \enot L))} \ci{j1-jlnl1}
\open
	\hypo{jlnl2}{J \eor (L \eand \enot L)} \as{}
	\open
		\hypo{lnl}{L \eand \enot L} \as{}
		\have{l}{L}\ae{lnl}
		\have{nl}{\enot L}\ae{lnl}
	\close
	\have{nlnl}{\enot (L \eand \enot L)}\ne{lnl-nl}
	\have{j5}{J}\oe{jlnl2, nlnl}
\close
\have{cond2}{(J \eor (L \eand \enot L)) \eif J} \ci{jlnl2-j5}
%\have{conj}{(J \eif (J\eor (L \eand \enot L))) \eand ((J \eor (L \eand \enot L)) \eif J)} \ai{cond1,cond2}
\have{con}{J \eiff (J\eor (L \eand \enot L))}\bi{cond1,cond2}
\end{proof}}
\bigskip


\filbreak
\item $((A \eif B) \eif A) \eif A$ 
\begin{proof}
\open
	\hypo{aba}{(A \eif B) \eif A} \as{}
	\open 
		\hypo{na}{\enot A} \as{}
		\open
		\have{nab}{\enot (A \eif B)} \as{}
			\open
				\hypo{a}{A} \as{}
					\open
					\hypo{nb}{\enot B} \as{}
					\have{a2}{A} \reit{a}
					\have{na2}{\enot A} \reit{na}
				\close
				\have{b}{B} \ne{nb-na2}
			\close
		\have{ab}{A \eif B}\ci{a-b}
		\have{nab2}{\enot (A \eif B)}\reit{nab}
	\close
	\have{ab2}{A \eif B} \ne{nab-nab2}
	\have{a}{A} \ce{aba,ab2}
	\have{na3}{\enot A} \reit{na}
\close
\have{a2}{A} \ne{aba-na3}
\close
\have{con}{((A \eif B) \eif A) \eif A}\ci{aba-a2}
\end{proof}
\end{earg}


\filbreak
\problempart

\begin{earg}
\item $T\eif S \leftproves\proves \enot S \eif \enot T$
\begin{proof}
\hypo{ts}{T \eif S} \pr{}
\open
	\hypo{ns}{\enot S} \as{}
		\open
		\hypo{t}{T} \as{}
		\have{s}{S} \ce{ts,t}
		\have{ns2}{\enot S} \reit{ns}
		\close
	\have{nt}{\enot T} \ni{ns-ns2}
\close
\have{nsnt}{\enot S \eif \enot T}\ci{ns-nt}
\end{proof}

\begin{proof}
\hypo{nsnt}{\enot S \eif \enot T} \pr{}
\open
	\hypo{t}{T} \as{}
	\open
		\hypo{ns}{\enot S} \as{}
		\have{nt}{\enot T}\ce{nsnt, ns}
		\have{t2}{T}\reit{t}
	\close
	\have{s}{S}\ne{ns-t2}
\close
\have{ts}{T \eif S} \ci{t-s}
\end{proof}
\medskip


\filbreak
\item $R \eif Q \leftproves\proves \enot(R \eand \enot Q)$
\begin{proof}
\hypo{ui}{R \eif Q} \pr{}
\open
	\hypo{uni}{R \eand \enot Q} \as{}
	\have{u}{R}\ae{uni}
	\have{ni}{\enot Q}\ae{uni}
	\have{i}{Q}\ce{ui, u}
\close
\have{con}{\enot (R \eand \enot Q)}\ni{uni-i}
\end{proof}

\begin{proof}
\hypo{con}{\enot (R \eand \enot Q)} \pr{}
\open
	\hypo{u}{R} \as{}
	\open
		\hypo{ni}{\enot Q} \as{}
		\have{uni}{R \eand \enot Q}\ai{u, ni}
		\have{red}{\enot (R \eand \enot Q)}\reit{con}
	\close
	\have{i}{Q}\ne{ni-red}
\close
\have{ui}{R \eif Q}\ci{u-i}
\end{proof}

\end{earg}


