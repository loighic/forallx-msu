%!TEX root = forallxyyc.tex
\graphicspath{{figures--proofs/}}
\part{Natural deduction for TFL}
\label{ch.NDTFL}
\addtocontents{toc}{\protect\mbox{}\protect\hrulefill\par}

\chapter{Natural deduction}\label{s:NDVeryIdea}

\section{Natural deduction versus truth tables}

An argument is valid when (and only when) it is impossible for all of the premises to be true and the conclusion to be false. And we have seen that truth tables can be used to determine whether an argument is valid. In the next chapter, you will learn another method for verifying that an argument is valid. Before we turn to this new method, however, let’s review the strengths and weakness of truth tables.

\begin{earg}
\item The truth table method for determining if an argument is valid focuses directly on the definition of \textit{valid}. Each line of a complete truth table corresponds to a truth-value assignment. Thus, given an argument in TFL, truth tables reveal whether or not the conclusion is true when all of the premises true. 
\medskip

\item Truth tables also allow us to easily and rigorously set the meaning for each logical operator. As we discussed in \S\ref{s:ConnectiveDisjunction}, in English, `or’ can take the inclusive-or meaning (one or the other, or both) or the exclusive-or meaning (one or the other, but not both), and, at different times, both meanings are used in English. We can discuss which English meaning is closest to the meaning of `$\eor$’ in TFL (it's the inclusive-or), but, in the end, we just set the meaning of the symbol `$\eor$’ with this truth table:

\begin{center}
\begin{tabular}{d d | f}
\meta{A} & \meta{B} & $\meta{A}\eor\meta{B}$ \\
\hline
T & T & T\Tstrut\\
T & F & T\\
F & T & T\\
F & F & F
\end{tabular}
\end{center}

Hence, the definition for the `$\eor$’ is simple this: the operator that connects $\meta{A}$ and $\meta{B}$ to yield the truth values shown in this truth table. And, then, the same goes for the other logical operators. 
\medskip

\item To create a truth table, the number of lines needed is $2^n$, where \textit{n} is the number of different letters in the argument. So, an argument with four different sentence letters will require a 16 line truth table, one with five letters will require 32 lines, one with six different letters will require 64 lines, and so on. Hence, while a truth table can be used to determine if any argument is valid or invalid, one of the weakness of this method is that it is difficult to use when the argument contains more than four different sentence letters.
\medskip

\item But what is typically seen as the biggest weakness of using truth tables to determine if an argument is valid is that it doesn't reveal to us \textit{why} the argument is valid. It doesn’t, in other words, lay out the reasoning that demonstrates why (and how) the conclusion follows from the premises.
\end{earg}
 
As an alternative to truth tables, we have a \textit{natural deduction system}. Such a system allows us to verify that an argument is valid and to see why it is valid. We do this by making explicit the reasoning process that takes us from the premises to the conclusion. We begin with twelve basic rules---which we call \textit{rules of derivation}. (For instance, this is one of the rules: if we know that `$P \eor Q$' is true; and we also know that `\enot P' is true, then we can assert that `\textit{Q}' is true.) The rules can be combined, and with just these twelve, we hope to be able to show how we get from the premises to the conclusion for all of the valid arguments that can be represented in TFL. 

There are different natural deduction systems that can be used with TFL. But all, for the most part, reflect the ways that we naturally reason---at least insofar as the reasoning involves ‘and’, ‘or’, ‘not’, ‘if \ldots, then \ldots’, and ‘if and only if’. 


\section{Truth functional propositional logic}

We have reached a point where it is useful to summarize what TFL is. As you know, the symbols of TFL are the sentence letters that represent atomic sentences, the logical operators $\enot$, $\eand$, $\eor$, $\eif$, and $\eiff$, and parentheses. These, then, can be combined into sentences using the rules given in chapter \ref{s:TFLSentences}. And, then, in chapter \ref{s:CharacteristicTruthTables}, truth tables were used to set the meaning of the logical operators. 

Truth tables also give us a method for determining if an argument is \textit{valid}. \textit{Valid} is a concept and is not, strictly speaking a part of TFL. Rather it is a property of some arguments that can, to an extent, be studied and explicated using TFL. Similarly, as you have seen, \textit{tautology}, \textit{contradiction}, \textit{contingent}, \textit{equivalent},  \textit{jointly consistent}, and \textit{jointly inconsistent} are concepts that can be explained using TFL.

The final part of TFL is the system of natural deduction, which sets the rules for how sentences containing the logical operators can be combined or taken apart. 
% This, in effect, establishes the syntax for sentences in TFL.

\section{Fitch} 

The modern development of natural deduction dates from simultaneous but unrelated papers by Gerhard Gentzen and Stanisław Jaskowski that were published in 1934. The natural deduction system that we will use, however, is based largely on work by Frederic Fitch that was first published in 1952. Consequently, the format that is used in the next chapter for writing proofs is called \textit{Fitch notation}.
% The second name should be `Stanisław Jasśkowski,' but Yale New doesn't have the accented `s'. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\chapter{The rules of derivation}\label{s:BasicTFL}

\section{Proofs}\label{intro-proofs}

As was explained in the previous chapter, creating a \define{proof} is one way of demonstrating that an argument is valid. (And, as you know, using a truth table is the  other way.) A proof is a list of sentences. The sentence or sentences at the beginning of the list are the premises of the argument. Every other sentence in the list follows from earlier sentences by a specific rule (with one exception, which we will get to in \S \ref{s:CI-rule}). The final sentence is the conclusion of the argument.

As an illustration, consider this argument:
	$$\enot (P \eor Q) \proves \enot P \eand \enot Q$$
We start the proof by numbering the line and writing the premise:
\begin{proof}
	\hypo{a1}{\enot (P \eor Q)} \pr{}
\end{proof}
Every line in a proof is numbered so that we can refer to it later if we need to do so. We have also indicated that this is a premise by putting `PR' at the end of the line. And we have drawn a line underneath the premise. Everything written below the line will either be a sentence that can be derived from that premise, or it will be a new assumption that we introduce. The colon that is right before `PR' is, technically, optional, but it has to be used in Carnap to separate the TFL sentence from the `PR' (or the rule) that is written at the end of each line.

The conclusion of this argument is `$\enot P \eand \enot Q$'; and so we want our proof to end---on some line, we'll call it $n$---with that sentence:
\begin{proof}
	\hypo{a1}{\enot (P \eor Q)} \pr{}
	\have{a2}{\ldots}
	\have[ ]{}{\ldots}
	\have[ ]{}{\ldots}
	\have[n]{con}{\enot P \eand \enot Q}
\end{proof}
It doesn't matter how many lines it takes to arrive at the conclusion, although, generally, we prefer a shorter proof over a longer one.

Now, suppose we have this argument:
$$P\eor Q, \enot (P\eand S), \enot (Q \eand \enot T) \proves \enot S\eor T$$
This argument has three premises, and so we start by listing them, numbering each line, and drawing a line under the final premise:
\begin{proof}
	\hypo{a1}{P \eor Q} \pr{}
	\hypo{a2}{\enot (P\eand S)} \pr{}
	\hypo{a3}{\enot (Q \eand \enot T)} \pr{}
\end{proof}
This, meanwhile, will be the final line of the proof:
\begin{proof}
	\have[n]{con}{\enot S \eor T}
\end{proof}
Setting up the premises and the conclusion is, however, the easy part. The real task---and the interesting part---is determining each of the steps that get us from the premise or premises to the conclusion. 

To do that, we will use a \define{natural deduction} system. \label{intro-elim} In this system, there are two rules for each logical operator: an \define{introduction} rule, which allows us to derive a new sentence that has the logical operator as the main connective, and an \define{elimination} rule, which allows us to extract a sub-sentence from a sentence that has that logical operator as the main connective. (Table \ref{table.TFL-rules} contains a list of the rules.) These rules can then be combined to demonstrate each step that must be taken to get from the premises to the conclusion. All of the rules introduced in this chapter are also summarized on pp.~\pageref{ProofRules} - \pageref{ProofRules-end}.


\begin{table*}\centering\sffamily\footnotesize
\ra{1.25}
\begin{tabular}{@{}l l@{}}\toprule
\multicolumn{2}{@{}l}{\textth{The rules of derivation}} \\\midrule
conjunction introduction rule & conjuction elimination rule \\
disjunction introduction rule & disjunction elimination rule \\
conditional introduction rule & conditional elimination rule \\
biconditional introduction rule~~ & biconditional elimination rule \\
negation introduction rule & negation elimination rule \\
reiteration rule &\\
double negation rule &\\ 
\bottomrule
\end{tabular}
\caption{}\label{table.TFL-rules}
\end{table*}



\section{Conjunction introduction and elimination}\label{s:conj-rule}

Let's say that we know that Sarah is swimming. We also, as it happens, know that Amy is reading. We are, therefore, justified in stating, ``Sarah is swimming and Amy is reading.'' 
This reasoning process, which we all do naturally, is part of our natural deduction system. It is called the \define{conjunction introduction rule}. 
\begin{factboxy}{conjunction introduction rule}
\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[n]{b}{\meta{B}}
	\have[\ ]{c}{\meta{A}\eand\meta{B}} \ai{a, b}
\end{proof}

\small{If we have $\meta{A}$ on a line and $\meta{B}$ on a line, then we can put $\meta{A}\eand\meta{B}$ on a new line. The `$\meta{A}$' and `$\meta{B}$' can occur in either order, and the conjunction can be `$\meta{A} \eand \meta{B}$' or `$\meta{B} \eand \meta{A}$'.}
\end{factboxy}

%\tcblower

The `$m$' and `$n$' will never appear in an actual proof. In a proof, the lines are numbered $1$, $2$, $3$, etc.  The `$m$' and `$n$' are used in the statement of the rule to indicate that $\meta{A}$ and $\meta{B}$ can be on any lines in the proof.
If you look ahead, you will see that some of rules given in \S \ref{s:conj-rule} - \ref{s:bi-rules} consist of three lines and some have two. 
\begin{earg}
\item[(a)] For each of the rules that consist of three lines, you can add what is on the last line (of the rule) to your proof when, and only when, you have what is given on the first two lines (of the rule).
\item[(b)] For each of the rules that consist of two lines, you can add what is on the last line (of the rule) to your proof when, and only when, you have what is given on the first line (of the rule).
\end{earg}

Returning to the example about Sarah and Amy, we can use this symbolization key:
	\begin{ekey}
		\item[S] Sarah is swimming.
		\item[R] Amy is reading.
	\end{ekey}
Let's say that `$S$' and `$R$' are our premises (although they don't have to be to use the conjunction introduction rule), and so they are on lines 1 and 2. Then on any subsequent line---but, in this case, it will be line 3---we can get `$S \eand R$' by using the conjunction introduction rule.
\begin{proof}
	\hypo{a}{S} \pr{}
	\hypo{b}{R} \pr{}
	\have{c}{S \eand R} \ai{a, b}
\end{proof}
%Every line of our proof must either be a premise or it must be justified by some rule (or, as we will see in \S \ref{s:CI-rule} - \ref{s:neg-rules}, we can, in certain circumstances, introduce a new assumption). 
To show that this application of the conjunction introduction rule is our justification for the new `$S \eand R$' on line 3, we put `$\eand$I 1, 2' on the far right. This indicate that `$S \eand R$' was obtained by applying the conjunction introduction rule to the `$S$' on line 1 and the `$R$' on line 2. 

The conjunction introduction rule introduces a sentence with `$\eand$' as the main connective. We also have a rule that let's us extract what is on one side of a conjunction and put it on a new line. Suppose someone tells you that \textit{Jeff is eating and Mary is sleeping}. Assuming that whoever told you this is reliable, you are entitled to infer simply that \textit{Jeff is eating}. You are also entitled to infer that \textit{Mary is sleeping}. These are applications of the \define{conjunction elimination rule} (which is actually two similar rules).

\begin{factboxy}{conjunction elimination rule}
\begin{proof}
	\have[m]{ab}{\meta{A}\eand\meta{B}}
	\have[\ ]{a}{\meta{A}} \ae{ab}
\end{proof}

\begin{proof}
	\have[m]{ab}{\meta{A}\eand\meta{B}}
	\have[\ ]{b}{\meta{B}} \ae{ab}
\end{proof}

\small{If we have $\meta{A} \eand \meta{B}$ on a line, then, on a new line, we can put either $\meta{A}$ by itself or $\meta{B}$ by itself.}

\end{factboxy}

When you have a conjunction on one line of a proof, you can use the conjunction elimination rule to obtain either of the conjuncts on a new line. You can only, however, apply this rule when the `$\eand$' is the main logical operator. So, for instance, you cannot use the conjunction elimination rule to obtain `$T$' from `$S \eor (T \eand W)$' (because `$\eor$' is the main logical operator). The same holds for all of the other rules. \textbf{Each of the rules of derivation can only be applied to the main logical operator of a sentence.}\label{rule-proofs-main-operator}
\bigskip

\noindent We will now construct a couple of proofs using the two rules just introduced.  First, one for this argument: `$P \eand Q, \enot R \proves Q \eand \enot R$'.

\begin{proof}
	\hypo{p1}{P \eand Q} \pr{}
	\hypo{p2}{\enot R} \pr{}
\end{proof}\medskip

\noindent After we have listed the premises, we use the conjunction elimination rule to get `$Q$' on a line by itself.

\begin{proof}
	\hypo{p1}{P \eand Q} \pr{}
	\hypo{p2}{\enot R} \pr{}
	\have{q}{Q} \ae{p1}
\end{proof}\medskip

\noindent And then, to finish the proof, we use the conjunction introduction rule to get `$Q \eand \enot R$'.

\begin{proof}
	\hypo{p1}{P \eand Q} \pr{}
	\hypo{p2}{\enot R} \pr{}
	\have{q}{Q} \ae{p1}
	\have{c}{Q \eand \enot R} \ai{p2,q}
\end{proof}\medskip
Notice that there is nothing in this representation of the proof to indicate that the last line is the conclusion. It's only because we began with `$P~\eand~Q, \enot R \proves Q \eand \enot R$' that we know that, on line 4, we have arrived at the conclusion that we want.

Next, we will take up the proof for this argument: 
$$(N\eor P) \eand (Q \eand S) \proves (N\eor P)\eand S$$ 
After listing the premise, we can use the conjunction elimination rule twice to get `$N\eor P$' and `$Q\eand S$' on lines by themselves.
\begin{proof}
	\hypo{ab}{(N\eor P) \eand (Q\eand S)} \pr{}
	\have{a}{(N\eor P)} \ae{ab}
	\have{b}{(Q\eand S)} \ae{ab}
\end{proof}

\noindent Now that `$Q\eand S$' is on its own line, we can use the conjunction elimination rule again to get $S$ on a line by itself. 

\begin{proof}
	\hypo{ab}{(N\eor P) \eand (Q\eand S)} \pr{}
	\have{a}{(N\eor P)} \ae{ab}
	\have{b}{(Q\eand S)} \ae{ab}
	\have{b2}{S} \ae{b}
\end{proof}
In our final step, we use the conjunction introduction rule to get the conclusion, `$(N\eor P)\eand S$'.
\begin{proof}
	\hypo{ab}{(N\eor P) \eand (Q\eand S)} \pr{}
	\have{a}{(N\eor P)} \ae{ab}
	\have{b}{(Q\eand S)} \ae{ab}
	\have{b2}{S} \ae{b}
	\have{c}{(N\eor P)\eand S} \ai{a,b2}
\end{proof}
 

\section{Disjunction intro and elim}\label{s:disj-rule}

% Hopefully, grasping the conjunction introduction and conjunction elimination rules was not too difficult, and you saw that you already understood the logic of each rule. As we proceed, it may surprise you to be told that you already, more or less, understand the logic of the rest of the rules of derivation (with the possible exception of the biconditional introduction and elimination rules, since the biconditional isn't often used precisely in English). These are, after all, \textit{natural} deduction rules. Seeing them in this context makes it easy to forget this, but you should keep it in mind. We have also, already, discussed the logical operators in chapter \ref{s:TFLConnectives} and chapter \ref{s:CharacteristicTruthTables} and you should to try to integrate what you learned in those chapters with the rules of derivation.

\paragraph{Disjunction elimination}
For the disjunction rules, let's start with this example: 
\begin{ebullet}
	\item[] \textit{Sarah is swimming or Jeff is eating a burrito}. 
\end{ebullet}
When is this true? Recall from \S \ref{s:ConnectiveDisjunction} that we are using the inclusive-or. So, \textit{Sarah is swimming or Jeff is eating a burrito} is true when either
\begin{ebullet}
	\item[(a)] only \textit{Sarah is swimming} is true, or 
	\item[(b)] only \textit{Jeff is eating a burrito} is true, or 
	\item[(c)] both are true. 
\end{ebullet}
That's a lot of options, but if all we know is that Sarah is swimming \textit{or} Jeff is eating a burrito, then we don't know precisely what either one of them is doing. But let's say that someone (whom we trust completely) tells us that, actually, Sarah is \textit{not} swimming. This piece of information about Sarah, then, let's us safely infer that Jeff is eating a burrito. This reasoning process is an example of the \define{disjunction elimination rule}.

\medskip
% \begin{tcbraster}[raster columns=2, raster valign=top]
\begin{factboxy}{disjunction elimination rule}
\begin{proof}
	\have[m]{ab}{\meta{A}\eor\meta{B}}
	\have[n]{nb}{\enot\meta{B}}
	\have[\ ]{a}{\meta{A}} \oe{ab,nb}
\end{proof}

\begin{proof}
	\have[m]{ab}{\meta{A}\eor\meta{B}}
	\have[n]{na}{\enot\meta{A}}
	\have[\ ]{b}{\meta{B}} \oe{ab,nb}
\end{proof}

\small{If we have `$\meta{A}\eor\meta{B}$' on a line and, on another line, we have what is either before or after the `$\eor$' with a `$\enot$' before it (i.e., `$\enot\meta{B}$' or `$\enot\meta{A}$'), then, on a new line, we can put what is on the other side of the `$\eor$'.}

\end{factboxy}					% Don't put a blank line after this if the boxes are supposed to be next to each other.


% \end{tcbraster}


\paragraph{Disjunction introduction}
\textit{Sarah is swimming or Jeff is eating a burrito} is true when \textit{Sarah is swimming} is false (she isn't swimming) and \textit{Jeff is eating a burrito} is true. In other words, the disjunction will be true as long as one of the disjuncts is true. This feature of disjunctions lets us make an inference that we don't use often in our everyday lives. It is a very simple inference, however. Take any sentence. We'll use \textit{you are studying logic}. That's true. Since \textit{you are studying logic} is true, each one of these sentences is also true:

\begin{ebullet}
	\item[] You are studying logic, {or} you are studying German.
	\item[] You are studying logic, {or} your mother is in Fiji.
	\item[] You are studying logic, {or} a dragon is on the moon.
\end{ebullet}

\noindent The idea is that, if we know that a sentence is true, we can create a longer sentence by adding `or \textit{any sentence whatsoever}' and the disjunction will also be true. This feature of the disjunction underlies the \define{disjunction introduction rule} (which, again, is two similar rules).

\begin{factboxy}{disjunction introduction rule}\label{di-rule-box}
\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[\ ]{ab}{\meta{A}\eor\meta{B}}\oi{a}
\end{proof}

\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[\ ]{ba}{\meta{B}\eor\meta{A}}\oi{a}
\end{proof}

\small{If we have `$\meta{A}$' on a line, then on a new line we can repeat the `$\meta{A}$' and add `$\eor$' and anything else.}

\end{factboxy}
\bigskip

\noindent Now, we will construct a proof for `$P \eor Q, R \eand \enot Q \proves P \eor S$'. In this proof, we will use the conjunction elimination rule, disjunction introduction rule, and the disjunction elimination rule.

After listing the premises, we use the conjunction elimination rule to get `$\enot Q$' on a line by itself. 
\begin{proof}
	\hypo{p1}{P \eor Q} \pr{}
	\hypo{p2}{R \eand \enot Q} \pr{}
	\have{q}{\enot Q} \ae{p2}
\end{proof}
Once we have the `$\enot Q$' on a line, we can (with the `$P \eor Q$' on line 1) use the disjunction elimination rule to get `$P$' on line 4.
\begin{proof}
	\hypo{p1}{P \eor Q} \pr{}
	\hypo{p2}{R \eand \enot Q} \pr{}
	\have{q}{\enot Q} \ae{p2}
	\have{p}{P} \oe{p1,q}
\end{proof}
And then last, we use the disjunction introduction rule to get the conclusion.
\begin{proof}
	\hypo{p1}{P \eor Q} \pr{}
	\hypo{p2}{R \eand \enot Q} \pr{}
	\have{q}{\enot Q} \ae{p2}
	\have{p}{P} \oe{p1,q}
	\have{c}{P \eor S} \oi{p}
\end{proof}


%\meta{B} can be any sentence whatsoever. All that we need to use this rule is the line containing \meta{A} on some earlier line of the proof. Hence, the following is a perfectly acceptable:
%\begin{proof}
%	\hypo{m}{M} \pr{}
%	\have{mmm}{M \eor [(A\eiff B) \eif (C \eand D)]}\oi{m}
%\end{proof}

\subsection{Introduction and elimination}
When you use the rules that are in this chapter, you are applying the \textit{patterns} given in the definition of each rule. Every pattern is different, and so you have to make sure that you understand each one. But you don't, actually, have to think beyond the patterns---although you can, and some people find it helpful to think about how the rules of derivation conform to what you learned about each logical operator in the chapter on the characteristic truth tables (chapter \ref{s:CharacteristicTruthTables}).

It can also be useful to understand why we call these \textit{introduction} and \textit{elimination} rules. (Or, at least, it's useful not to misunderstand why we use these terms.) The introduction rules are given this name because, in each case, we introduce a logical operator. For instance, if a proof begins this way: 

\begin{proof}
	\hypo{p}{P} \pr{}
	\hypo{q}{Q} \pr{}
\end{proof}

\noindent then, clearly, there is not an `$\eand$' in the proof yet. The conjunction introduction rule, however, lets us introduce one:

\begin{proof}
	\have[3]{pq}{P\eand Q} \ai{p,q}
\end{proof}

The elimination rules, meanwhile, all, in a way, eliminate a logical operator. It might, however, be more useful to think of these as \textit{extraction} rules because, really, what they do is allow us to extract a part of a sentence and put it on a new line. For instance, if we have this:

\begin{proof}
	\hypo{rt}{R \eand T} \pr{}
\end{proof}

\noindent then we can use the \&E rule to take the `$R$' and put it on a line by itself:

\begin{proof}
	\have[2]{r}{R} \ae{rt}
\end{proof}

\noindent We might think of this as having eliminated the `$\eand$', and, in a way, that's what we have done. But the `$R \eand T$' is still on line 1 and can be used again in the proof. Hence, \textit{extraction rules} is a little more accurate than \textit{elimination rules}, but \textit{elimination} is the commonly used term, and so we will stick with it. 


\subsection{Double negation}\label{subsection-DN}

The \define{double negation rule} is a rule of convenience that sometimes compliments the disjunction-elimination rule. (There are also times when it will be used in the proofs that are discussed in \ref{s:theorems}, but, for the material in this chapter, it will only be used with the disjunction-elimination rule.) First, notice that the disjunction-elimination rule is very specific. To use it, we need, on one line of our proof, a sentence with the form `$\meta{A}\eor\meta{B}$', and on another line of our proof, we need one side of the disjunction (either $\meta{A}$ or $\meta{B}$) with a `$\enot$' in front of it; that is, either $\enot\meta{A}$ or $\enot\meta{B}$. 

This presents a problem if we have these two sentences somewhere in a proof:
\begin{proof}
	\have[m]{np}{\enot P \eor Q}
	\have[n]{p}{P}
\end{proof}
You might think that, given those two lines, we can put `$Q$' on a new line like this:
\begin{proof}
	\have[m]{np}{\enot P \eor Q}
	\have[n]{p}{P}
	\have[\ ]{q}{Q} \oe{np,p}
\end{proof}
In a sense, this is the right idea for the disjunction elimination rule. One side of the disjunction `$\enot P \eor Q$' has to be true, and the `$P$' on line $n$ means that `$\enot P$' is false. Hence, we should be allowed to put `$Q$' on a new line. The disjunction elimination rule, however, does not permit this. To see why, let's distinguish between \define{negation} and \define{denial}.

\begin{factboxy}{negation and denial}
The \define{negation} of a sentence is the sentence with a `not' added to it.\\ 
The \define{denial} of a sentence is the sentence with either a `not' added or a `not' removed.
\end{factboxy}

\noindent For example, 
\begin{earg}
\item[1.] the negation of `today is Tuesday' is `today is not Tuesday'. 
\end{earg}
In TFL, 
\begin{earg}
%\item[2.] the negation of $\meta{A}$ is $\enot\meta{A}$. The negation of $\enot\meta{A}$ is $\enot\enot\meta{A}$.
\item[2.] the negation of $P$ is $\enot P$. The negation of $\enot P$ is $\enot\enot P$.
\end{earg}
Meanwhile,
\begin{earg}
\item[3.] the denial of `it is not raining' is either (a) `it is raining' or (b) `it is not not raining'. 
\end{earg}
In TFL,
\begin{earg}
%\item[4.] the denial of $\enot\meta{A}$ is either $\meta{A}$ or $\enot\enot\meta{A}$.
\item[4.] the denial of $\enot P$ is either $P$ or $\enot\enot P$.
\end{earg}
To use the disjunction elimination rule, we must have the \textbf{\textit{negation}} of one side of the disjunction on another line. In the example above, we have the denial of $\enot P$, not its negation, on line \textit{n}. The \define{double negation rule} helps us correct this so that we can use the disjunction elimination rule more often. 

Before we see how the double negation rule can help us with our derivation, let's introduce the rule. The first version of the double negation rule allows us to add two \textit{not}s (i.e., \textit{not not}) to a sentence in TFL---which, of course, will not change the sentence's truth value. The second version of the double negation rule allows us to remove two \textit{not}s, although needing to do this is less common. 

\begin{factboxy-width}[width=7.5cm]{double negation rule}
\begin{proof}
	\have[m]{a1}{\meta{A}}
	\have[n]{a2}{\enot\enot\meta{A}} \dn{a1}
\end{proof}

\begin{proof}
	\have[m]{a1}{\enot\enot\meta{A}}
	\have[n]{a2}{\meta{A}} \dn{a1}
\end{proof}
\end{factboxy-width}

Let's say that this is the argument for which we need to provide a proof: `$\enot P \eor Q, P \proves Q$'. After the premises, we use the double negation rule to get `$\enot \enot P$' from line 2.

\begin{proof}
	\hypo{ab}{\enot P \eor Q} \pr{}
	\hypo{nb}{P} \pr{} 
	\have{nnb}{\enot\enot P} \dn{nb}
\end{proof}
The `$P$' on line 2 and the `$\enot \enot P$' on line 3 have exactly the same meaning. The only difference between `$P$' and `$\enot \enot P$' is their form. But now that we have `$\enot \enot P$', we have the negation of what is on the left side of the disjunction (which is `$\enot P$'). That allows us to use the disjunction elimination rule, and we can get the conclusion.

\begin{proof}
	\hypo{ab}{\enot P \eor Q} \pr{}
	\hypo{nb}{P} \pr{}
	\have{nnb}{\enot \enot P} \dn{nb}
	\have{q}{Q} \oe{ab,nnb}
\end{proof}

Remember that, in this chapter, you will only use the double negation rule right before you use the disjunction elimination rule, and you will only use it some of the time with the disjunction elimination rule.

\begin{factboxy-side}{when not to use (left) and when to use (right) the double negation rule}\label{DN-box}
\begin{proof}
	\have[m]{ab}{\meta{A}\eor\meta{B}}
	\have[n]{nb}{\enot\meta{B}}
	\have[\ ]{a}{\meta{A}} \oe{ab,nb}
\end{proof}
\tcblower
\begin{proof}
	\have[m]{ab}{\meta{A}\eor\enot\meta{B}}
	\have[n]{b}{\meta{B}}
	\have[p]{nnb}{\enot\enot\meta{B}} \dn{b}
	\have[\ ]{a}{\meta{A}} \oe{ab,nnb}
\end{proof}
\end{factboxy-side}


%\tcbsidebyside[sidebyside adapt=both,
%enhanced,center,
%title=when to use the double negation rule,
%attach boxed title to top center={yshift=-2mm},
%coltitle=black,boxed title style={colback=red!25},
%segmentation style=solid,colback=red!5,colframe=red!50
%]{%
%\begin{proof}
%	\have[m]{ab}{\meta{A}\eor\meta{B}}
%	\have[n]{nb}{\enot\meta{B}}
%	\have[\ ]{a}{\meta{A}} \oe{ab,nb}
%\end{proof}
%}{%
%\begin{proof}
%	\have[m]{ab}{\meta{A}\eor\enot\meta{B}}
%	\have[n]{b}{\meta{B}}
%	\have[p]{nnb}{\enot\enot\meta{B}} \dn{b}
%	\have[\ ]{a}{\meta{A}} \oe{ab,nnb}
%\end{proof}
%}


\section{Conditional elimination}

For the conditional, we will cover the elimination rule now and the introduction rule in \S \ref{s:CI-rule}. Consider the following argument:
	\begin{earg}
	\item	If the envelope is on the table, then Aleksander is in the safe house.
	\item The envelope is on the table.
	\item Therefore, Aleksander is in the safe house.
	\end{earg}
In this argument---which is valid---we have a conditional and then, on a separate line, the antecedent of that conditional (`the envelope is on the table'). This allows us to safely infer the consequent (`Aleksander is in the safe house'). In short, if we have a conditional and we know that the antecedent of the conditional is true, then we know that the consequent has to be true. (See also the discussion of the conditional on p.~\pageref{characteristic-tt-conditional}.) Deriving the consequent of the conditional in this way is an application of the \define{conditional elimination rule}.
This rule is also sometimes called \emph{modus ponens}. When we use the rule, the conditional and the antecedent of the conditional can be separated from one another, and they can appear in any order.

\medskip

%\begin{tcbraster}[raster columns=2, raster valign=top]

\begin{factboxy}{conditional elimination rule}\label{ce-rule}
\begin{proof}
	\have[m]{ab}{\meta{A}\eif\meta{B}}
	\have[n]{a}{\meta{A}}
	\have[\ ]{b}{\meta{B}} \ce{ab,a}
\end{proof}

\small{If we have `$\meta{A}\eif\meta{B}$' on a line and, on a different line, we have just `$\meta{A}$' (i.e., the antecedent of the conditional), then, on a new line, we can put `$\meta{B}$' (i.e., the consequent of the conditional).} 

\end{factboxy}


\begin{factboxy}{biconditional elimination rule}\label{be-rule}
\small{If we have `$\meta{A}\eiff\meta{B}$' on a line and, on a different line, we have `$\meta{A}$', then, on a new line, we can put `$\meta{B}$'.}

\begin{proof}
	\have[m]{ab}{\meta{A}\eiff\meta{B}}
	\have[n]{a}{\meta{A}}
	\have[\ ]{b}{\meta{B}} \be{ab,a}
\end{proof}

\small{Or, if we have `$\meta{A}\eiff\meta{B}$' on a line and, on a different line, we have `$\meta{B}$', then, on a new line, we can put `$\meta{A}$'.} 

\begin{proof}
	\have[m]{ab}{\meta{A}\eiff\meta{B}}
	\have[n]{a}{\meta{B}}
	\have[\ ]{b}{\meta{A}} \be{ab,a}
\end{proof}

\end{factboxy}

%\end{tcbraster}


\section{Biconditional intro and elim}\label{s:bi-rules}

The \define{biconditional elimination rule} is similar to the conditional elimination rule but a bit more flexible. If you have a biconditional on one line and the left side of the biconditional on another line, you can put the right side on a new line. Or, if you have the right side, you can put the left side on a new line. Notice the difference between the conditional elimination rule and the biconditional elimination rule. There are two ways to use the biconditional elimination rule. There is only one way to use the conditional elimination rule.

In chapter \ref{s:CharacteristicTruthTables}, we said that the biconditional is ``the conjunction of a conditional running in each direction.'' This is the basis for the \define{biconditional introduction rule}. If we have both conditionals, $\meta{A} \eif \meta{B}$ and $\meta{B} \eif \meta{A}$ on separate lines in our proof, then we can put $\meta{A} \eiff \meta{B}$ on a new line.

\begin{factboxy}{biconditional introduction rule}
\begin{proof}\label{eiff-I}
    \have[m]{ab}{\meta{A}\eif\meta{B}} 
    \have[n]{ba}{\meta{B}\eif\meta{A}}
	\have[\ ]{b}{\meta{A}\eiff\meta{B}} \bi{ab,ba}
\end{proof}
\begin{proof}
    \have[m]{ab}{\meta{A}\eif\meta{B}} 
    \have[n]{ba}{\meta{B}\eif\meta{A}}
	\have[\ ]{b}{\meta{B}\eiff\meta{A}} \bi{ab,ba}
\end{proof}

\small{If we have `$\meta{A}\eif\meta{B}$' and `$\meta{B}\eif\meta{A}$' on two lines of our proof, then, on a new line, we can put either `$\meta{A}\eiff\meta{B}$' or `$\meta{B}\eiff\meta{A}$'.}

\end{factboxy}



%The order of the $\meta{A}$ and $\meta{B}$ in $\meta{A}\eiff\meta{B}$ has to match their order in the first conditional in the conjunction, but since, typically, that line will have to be generated using the conjunction introduction rule, those conjuncts  can be put in either order then. \textbf{Also, notice that when we cite the rule, we use $\eiff$ex, not $\eiff$I.}


\section{Some examples}

We will now look at some proofs that use the rules that are covered in \S\S \ref{s:conj-rule} -- \ref{s:bi-rules}. There are also practice exercises using these rules on p.~\pageref{practice-proofs-1}. 

\begin{earg}

%\pagebreak

\item[\ex{14.6.3}] For a proof of `$P \eif Q, R \eand P \proves Q \eand R$', we use the conjunction introduction rule, the conjunction elimination rule, and the conditional elimination rule.
\begin{proof}
	\hypo{p1}{P \eif Q} \pr{}
	\hypo{p2}{R \eand P} \pr{}
	\have{p}{P} \ae{p2}
	\have{r}{R} \ae{p2}
	\have{q}{Q} \ce{p1,p}
	\have{c}{Q \eand R} \ai{r,q}
\end{proof}\medskip


%\pagebreak

\item[\ex{14.6.4}] For a proof of `$R \eiff T, P \eor T, \enot P \proves R$', we use the disjunction elimination rule and the biconditional elimination rule.
\begin{proof}
	\hypo{p1}{R \eiff T} \pr{}
	\hypo{p2}{P \eor T} \pr{}
	\hypo{p3}{\enot P} \pr{}
	\have{t}{T} \oe{p2,p3}
	\have{c}{R} \be{p1,t}
\end{proof}\medskip


\item[\ex{14.6.2b}] For `$C \eand (D \eor \enot F), F \eand G \proves C \eand (D \eor H)$', we use all five of the rules introduced in \S\S \ref{s:conj-rule} and \ref{s:disj-rule}.
\begin{proof}
	\hypo{p1}{C \eand (D \eor \enot F)} \pr{}
	\hypo{p2}{F \eand G} \pr{}
	\have{c}{C} \ae{p1}
	\have{dnf}{D \eor \enot F} \ae{p1}
	\have{f}{F} \ae{p2}
	\have{nnf}{\enot \enot F} \dn{f}
	\have{d}{D} \oe{dnf, nnf}
	\have{dh}{D \eor H} \oi{d}
	\have{con}{C \eand (D \eor H)} \ai{c,dh}
\end{proof}\medskip


\begin{minipage}{10cm}
\item[\ex{14.6.5}] For a proof of `$(R \eand T) \eif Q, T \eand S, R \proves Q$', we use the conjunction elimination, conjunction introduction, and conditional elimination rules.
\begin{proof}
	\hypo{p1}{(R \eand T) \eif Q} \pr{}
	\hypo{p2}{T \eand S} \pr{}
	\hypo{p3}{R} \pr{}
	\have{t}{T} \ae{p2}
	\have{rt}{R \eand T} \ai{p3,t}
	\have{c}{Q} \ce{p1,rt}
\end{proof}\medskip
\end{minipage}

\item[\ex{14.6.6}] For a proof of `$P \eiff (R \eor S), T \eif R, Q \eand T \proves P$', we use the conjunction elimination, conditional elimination, disjunction introduction, and biconditional introduction rules.
\begin{proof}
	\hypo{p1}{P \eiff (R \eor S)} \pr{}
	\hypo{p2}{T \eif R} \pr{}
	\hypo{p3}{Q \eand T} \pr{}
	\have{t}{T} \ae{p3}
	\have{r}{R} \ce{p2,t}
	\have{rs}{R \eor S} \oi{r}
	\have{c}{P} \ce{p1,r}
\end{proof}\medskip

\item[\ex{14.6.7}] And last, a proof for this argument: $$(S \eif T) \eor \enot R, (T \eif S) \eor Q, R \eand \enot Q \proves T \eiff S$$ requires the conjunction elimination rule, the double negation rule, the disjunction elimination rule, and the biconditional introduction rule.
\begin{proof}
	\hypo{p1}{(S \eif T) \eor \enot R} \pr{}
	\hypo{p2}{(T \eif S) \eor Q} \pr{}
	\hypo{p3}{R \eand \enot Q} \pr{}
	\have{r}{R} \ae{p3}
	\have{q}{\enot Q} \ae{p3}
	\have{nnr}{\enot \enot R} \dn{r}
	\have{st}{S \eif T} \oe{p1,nnr}
	\have{ts}{T \eif S} \oe{p2,q}
%	\have{con}{(T \eif S) \eand (S \eif T)} \ai{st,ts}
	\have{c}{T \eiff S} \bi{st,ts}
\end{proof}\medskip
\end{earg}


\section{Conditional introduction}\label{s:CI-rule}

The \define{conditional introduction rule} is a little bit more complicated than the conditional elimination rule, but, with some thought (and some practice), it is easily grasped. We'll start with this symbolization key for the sentence letters \textit{G} and \textit{L}:
	\begin{ekey}
		\item[G] Kate's German class meets today.
		\item[L] Kate's logic class meets today.
	\end{ekey}
And this is our argument: 
$$G \eor L \proves \enot G \eif L$$
We will go through the proof for this argument, and in the process explain the conditional introduction rule. We start by listing the premise.
	\begin{proof}
		\hypo{r}{G \eor L} \pr{}
	\end{proof}
Next, we need to make a new assumption: `Kate's German class is \textit{not} meeting today'. We might say that we're making this assumption ``for the sake of argument'' or to see where it leads. To indicate that this is an assumption that we have supplied, we put `$\enot G$' on line 2 this way:
	\begin{proof}
		\hypo{r}{G \eor L} \pr{}
		\open
			\hypo{l}{\enot G} \as{}
	\end{proof}
You will notice right away that the `$\enot G$' is indented. Whenever we make an assumption ourselves, we must indent it and the lines that follow. This creates a \define{subproof} that is set off from the rest of the proof. The assumption is cited with `AS', and we put a line under the assumption just as we do with the final premise.
With this assumption in place, we next use the disjunction elimination rule to get $L$ on line 3.
	\begin{proof}
		\hypo{r}{G \eor L} \pr{}
		\open
			\hypo{l}{\enot G} \as{}
			\have{rl}{L}\oe{r,l}
	\end{proof}

The idea for the first three lines of this proof are, first, we know that \textit{Kate's German class meets today or her logic class meets today}. (Or, at least, we are assuming that `$ G \eor L$' is true because that is the premise that we were given). Next, on line 2, we are, in effect, asking, ``What if her German class is not meeting today?'' That is, what will follow if we make this assumption? Well, one thing that will follow is that Kate's logic class must be meeting today.  
%In other words, using the disjunction elimination rule, we can put `$L$' on line 3. 

So, on line 2, we have asked, What if \textit{Kate's German class is not meeting today}? On line 3, we have one answer: \textit{Kate's logic class is meeting today}. Therefore, on line 4, we can use the conditional introduction rule to put these two together as \textit{if Kate's German class is not meeting today, then Kate's logic class is meeting today}.
	\begin{proof}
		\hypo{r}{G \eor L} \pr{}
		\open
			\hypo{l}{\enot G} \as{}
			\have{rl}{L}\oe{r,l}
			\close
		\have{con}{\enot G \eif L}\ci{l-rl}
	\end{proof}
For this final step, we have gone back to the original vertical line of the proof. 

When we use the conditional introduction rule, the assumption that we make will always be the antecedent of the conditional. The last line of the subproof, meanwhile, will always be the consequent of the conditional.

\begin{factboxy}{conditional introduction rule}
	\begin{proof}
		\open
			\hypo[i]{a}{\meta{A}} \as{}
			\have[j]{b}{\meta{B}}
		\close
		\have[\ ]{ab}{\meta{A}\eif\meta{B}}\ci{a-b}
	\end{proof}

\small{We begin by making an assumption: `$\meta{A}$'. We then derive `$\meta{B}$'. Once that is done, we know that \textit{if} $\meta{A}$, \textit{then} $\meta{B}$, and we can put the conditional on the line after the subproof.

There can be as many or as few lines as needed between lines $i$ and $j$.

The lines cited are the range for the subproof, beginning with the line where the assumption is.} 
\end{factboxy}


\begin{notebox}
To simplify matters at the beginning of section \ref{intro-proofs}, I only use the term \textit{premises} to refer to the premises of an argument. A premise, however, is just a type of assumption. It is an assumption because we are taking it as given, and so it requires no justification---just like the assumption that we make at the beginning of a subproof. 
\end{notebox}


\paragraph{Subproofs} Lines $i$ through $j$ are called a \define{subproof}. These are the rules for subproofs:

\begin{earg}
\item[1.] Once a subproof has been closed, none of the lines in the subproof can be used again. (The conditional $\meta{A}\eif\meta{B}$ can be used later in the proof because it is outside of the subproof.)
\item[2.] A subproof is closed by the application of the conditional introduction rule---or, as you will see shortly, the negation introduction or the negation elimination rules.
\item[3.] When we close a subproof, the assumption made at the beginning of the subproof has been \textit{discharged}.
\item[4.] A proof is not complete until every assumption that we have made (and so not counting the premises) is discharged.
\end{earg}


\section{Some more examples}

Each of these examples uses the conditional introduction rule.

\begin{earg}
\item We will start with a proof for this argument:
	$$P \eif Q, Q \eif R \proves P \eif R$$
We start by listing both of our premises. Next, since we want `($P \eif R$)', we assume the antecedent of that conditional. 
\begin{proof}
	\hypo{pq}{P \eif Q} \pr{}
	\hypo{qr}{Q \eif R} \pr{}
	\open
		\hypo{p}{P} \as{}
	\close
\end{proof}
Now, even though it is an assumption that we've introduced, since `$P$' is on a line by itself (and the subproof has not yet been closed), we can use it for our next step. With `$P$' and the `$P \eif Q$' on line 1, we can use the conditional elimination rule to get `$Q$'. 
\begin{proof}
	\hypo{pq}{P \eif Q} \pr{}
	\hypo{qr}{Q \eif R} \pr{}
	\open
		\hypo{p}{P}\as{}
		\have{q}{Q}\ce{pq,p}
%		\have{r}{R}\ce{qr,q}
	\close
%	\have{pr}{P \eif R}\ci{p-r}
\end{proof}
With the `$Q$' on line 4 and `$Q \eif R$' on line 2, we can use the conditional elimination rule again; this time to get `$R$'. So, by assuming `$P$', we were able to get `$R$'. Last, we apply the conditional introduction rule, which discharges our assumption and completes the proof.
\label{HSproof}
\begin{proof}
	\hypo{pq}{P \eif Q} \pr{}
	\hypo{qr}{Q \eif R} \pr{}
	\open
		\hypo{p}{P}\as{}
		\have{q}{Q}\ce{pq,p}
		\have{r}{R}\ce{qr,q}
	\close
	\have{pr}{P \eif R}\ci{p-r}
\end{proof}
\bigskip

\item Next, let's construct a proof for this argument: `$F \eif (G \eand H) \proves F \eif G$'. We proceed this way:

\begin{proof}
	\hypo{fgh}{F\eif(G\eand H)} \pr{}
	\open
		\hypo{f}{F}\as{}
		\have{gh}{G \eand H}\ce{fgh,f}
		\have{g}{G}\ae{gh}
	\close
	\have{fg}{F \eif G}\ci{f-g}
\end{proof}
\bigskip

\item As you know, the biconditional elimination rule is similar to the conditional elimination rule. (But they are not the same. See p.~\pageref{ce-rule} to compare them.) We should also, however, be able to start with a biconditional, say `$M \eiff P$' and derive either of the conditionals: `$M \eif P$' or `$P \eif M$'. This is easily done with the conditional introduction rule.
\begin{proof}
	\hypo{bc}{M \eiff P} \pr{}
	\open
		\hypo{b}{M}\as{}
		\have{c}{P}\be{bc,b}
	\close
	\have{bc2}{M \eif P}\ci{b-c}
\end{proof}
\smallskip
\noindent And, with a similar proof, we can also derive `$P \eif M$'.
\bigskip

\item In the proof for `$\enot P \eor (R \eand Q) \proves P \eif Q$', we will use the conditional introduction rule as well as double negation rule and disjunction-elimination rule. 

\begin{proof}
	\hypo{p1}{\enot P \eor (R \eand Q)} \pr{}	
	\open
		\hypo{p}{P}\as{}
		\have{dn}{\enot\enot P} \dn{p}
		\have{rq}{R \eand Q} \oe{p1,dn}
		\have{q}{Q} \ae{rq}
	\close
	\have{c}{P \eif Q}\ci{p-q}
\end{proof}

%\begin{notebox}
%In fact, in some logic textbooks, this is biconditional elimination rule:
%\begin{proof}\label{eiff-I}
%    \have[m]{ab}{\meta{A} \eiff \meta{B}} 
%	\have[\ ]{b}{\meta{A}\eif\meta{B}} \be{ab}
%\end{proof}
%\begin{proof}\label{eiff-I}
%    \have[m]{ab}{\meta{A} \eiff \meta{B}}
%	\have[\ ]{b}{\meta{B}\eif\meta{A}}\be{ab}
%\end{proof}
%\end{notebox}

\end{earg}


\section{Negation introduction and elimination}\label{s:neg-rules}

Here is a simple mathematical argument in English:
\begin{earg}
\item[1.] Assume that there is some greatest natural number. Call it $G$.
\item[2.] That number plus one is also a natural number.
\item[3.] $G+1$ is greater than $G$.
\item[4.] Thus, $G$ is the greatest natural number (accordingt to 1), and there is a natural number greater than $G$ (according to 3).
\item[5.] The previous line is a contradiction.
\item[6.] Therefore, the assumption that we made on line 1 is false. There is no greatest natural number.
\end{earg}
This type of argument is traditionally called a \emph{reductio}. Its full Latin name is \emph{reductio ad absurdum}, which means `reduction to absurdity' (although \textit{absurdity} in the sense that we generally use the word today isn't part of this). In a reductio, we assume something for the sake of argument---for example, that there is a greatest natural number. Then we show that the assumption leads to two contradictory sentences---for example, `$G$ is the greatest natural number' and `$G$ is not the greatest natural number.' In this way, we have shown that the original assumption must be false, which means that the denial of the assumption is true. 

Our two negation rules (which are basically the same rule) formalize this reasoning process.

\medskip

%\begin{tcbraster}[raster columns=2, raster valign=top]

\begin{factboxy}{negation introduction rule}
\begin{proof}
\open
	\hypo[m]{na}{\meta{A}}\as{}
	\have[n]{b}{\meta{B}}
	\have[p]{nb}{\enot\meta{B}}
\close
\have[\ ]{a}{\enot\meta{A}}\ni{na-nb}
\end{proof}

\small{Assume $\meta{A}$. Derive a contradiction. (That is, get $\meta{B}$ and $\enot\meta{B}$ on the last two lines of the subproof). Exit the subproof, and put $\enot\meta{A}$ on the first line after the subproof.
\smallskip

There can be as many or as few lines as needed between lines $m$ and $n$, but $n$ and $p$ have to be consecutive lines. $\meta{B}$ and $\enot\meta{B}$ can be any contradiction that it is possible to derive; they can be in either order; and $\meta{A}$ can be one half of the contradiction (although it doesn't have to be).}

\end{factboxy}

\begin{factboxy}{negation elimination rule}
\begin{proof}
\open
	\hypo[m]{na}{\enot\meta{A}}\as{}
	\have[n]{b}{\meta{B}}
	\have[p]{nb}{\enot\meta{B}}
\close
\have[\ ]{a}{\meta{A}}\ne{na-nb}
\end{proof}

\small{Assume $\enot\meta{A}$. Derive a contradiction. (That is, get $\meta{B}$ and $\enot\meta{B}$ on the last two lines of the subproof). Exit the subproof, and put $\meta{A}$ on the first line after the subproof.}

\end{factboxy}

%\end{tcbraster}
\medskip

\noindent Notice that, just as we do when using the conditional introduction rule, we begin by making an assumption. The subproof that follows is indented, and the assumption that we made must be discharged by applying either the negation introduction rule or the negation elimination rule. 

When using either of the negation rules, the last two lines of the subproof must be an explicit contradiction: \meta{B} on one line and its negation, $\enot\meta{B}$, on the next line (or vice versa). Those two lines cannot be separated. When you cite the rule, however, the lines that you give are the lines for the whole subproof (starting with the assumption), not just the two lines containing the contradiction. 

%To see how the negation ... rule works, suppose we want to prove the law of non-contradiction: $\enot(G \eand \enot G)$. We can prove this without any premises by immediately starting a subproof. We want to apply {\enot}I to the subproof, so we assume $(G \eand \enot G)$. We then get an explicit contradiction by {\eand}E. The proof looks like this:

%\begin{proof}
%	\open
%		\hypo{gng}{G\eand \enot G}\as{}
%		\have{g}{G}\ae{gng}
%		\have{ng}{\enot G}\ae{gng}
%	\close
%	\have{ngng}{\enot(G \eand \enot G)}\ni{gng-ng}
%\end{proof}

\subsection{Reiteration}

To get a contradiction on the last two lines of a subproof, we will usually have to move a sentence that is on an earlier line to the last or second-to-last line of the subproof. This is done with the \define{reiteration rule}. Just as the double negation rule is a rule of convenience that sometimes compliments the disjunction elimination rule, the reiteration rule is a rule of convenience that compliments the negation elimination and negation introduction rules.  

\begin{factboxy-width}[width=7.5cm]{reiteration rule}
\begin{proof}
	\have[m]{a1}{\meta{A}}
	\have[n]{a2}{\meta{A}} \reit{a1}
\end{proof}
\end{factboxy-width}

To demonstrate both the negation elimination rule and the reiteration rule, we will go through the proof for this argument: `$\enot P \eif \enot Q, Q \proves P$'. Looking at the argument, you'll notice that our conclusion is `$P$', but we cannot get `$P$' by using $\eand$E, $\eor$E, $\eif$E, or $\eiff$E. This means that we will need to use one of our negation rules.

After the premises, we make the assumption that we need for negation elimination. Since, ultimately, we want `$P$', we will assume `$\enot P$' so that, once we discharge that assumption (and close the subproof), we will have the `$P$' that we are after.

\begin{proof}
	\hypo{p1}{\enot P \eif \enot Q} \pr{}	
	\hypo{p2}{Q} \pr{}
	\open
		\hypo{np}{\enot P}\as{}
\end{proof}
We then use the conditional elimination rule to get $\enot Q$ on line 4. 
\begin{proof}
	\hypo{p1}{\enot P \eif \enot Q} \pr{}	
	\hypo{p2}{Q} \pr{}
	\open
		\hypo{np}{\enot P}\as{}
		\have{nq}{\enot Q}\ce{p1,np}
%		\have{ng}{Q}\reit{p2}
\end{proof}
The $Q$ on line 2 and $\enot Q$ on line 4 are a contradiction, but to use the negation elimination rule we need to have `$Q$' on line 5. To get it there, we use the reiteration rule. 

\begin{proof}
	\hypo{p1}{\enot P \eif \enot Q} \pr{}	
	\hypo{p2}{Q} \pr{}
	\open
		\hypo{np}{\enot P}\as{}
		\have{nq}{\enot Q}\ce{p1,np}
		\have{ng}{Q}\reit{p2}
\end{proof}
Now that `$\enot Q$' and `$Q$' are on consecutive lines, we can use the negation elimination rule to discharge the assumption that we made, and that gives us the conclusion we are after: `$P$'.

\begin{proof}
	\hypo{p1}{\enot P \eif \enot Q} \pr{}	
	\hypo{p2}{Q} \pr{}
	\open
		\hypo{np}{\enot P}\as{}
		\have{nq}{\enot Q}\ce{p1,np}
		\have{ng}{Q}\reit{p2}
	\close
	\have{c}{P}\ne{np-ng}
\end{proof}

We just used the negation elimination rule. The negation introduction rule is, essentially, the same. Whether you use the negation introduction rule or negation elimination rule is just a function of whether you want `$\enot \meta{A}$' or `$\meta{A}$' on the line after the subproof.


%\newpage

\section{Even more examples}

The negation introduction rule or the negation elimination rule is used in each of these proofs.

\begin{earg}

\item $P \eif Q, \enot Q \proves \enot P$

\begin{proof}
	\hypo{p1}{P \eif Q} \pr{}
	\hypo{p2}{\enot Q} \pr{}	
	\open
		\hypo{p}{P} \as{}
		\have{q}{Q} \ce{p1,p}
		\have{nq}{\enot Q} \reit{p2}
	\close
	\have{c}{\enot P} \ni{p-nq}
\end{proof}
\medskip


\item $P \eif \enot Q \proves \enot(P \eand Q)$

\begin{proof}
	\hypo{p1}{P \eif \enot Q} \pr{}	
	\open
		\hypo{pq}{P \eand Q} \as{}
		\have{p}{P} \ae{pq}
		\have{nq}{\enot Q} \ce{p1,p}
		\have{q}{Q} \ae{pq}
	\close
	\have{c}{\enot(P \eand Q)} \ni{pq-q}
\end{proof}
\medskip

\begin{minipage}{10cm}
\item $Q \eand R, \enot(Q \eand S) \proves \enot S$

\begin{proof}
	\hypo{p1}{Q \eand R} \pr{}	
	\hypo{p2}{\enot(Q \eand S)} \pr{}
	\open
		\hypo{a}{S} \as{}
		\have{b}{Q} \ae{p1}
		\have{ba}{Q \eand S} \ai{a,b}
		\have{p2b}{\enot(Q \eand S)} \reit{p2}
	\close
	\have{c}{\enot S} \ni{a-p2b}
\end{proof}
\medskip
\end{minipage}


\item The proof for `$\enot P \proves P \eif Q$' requires two subproofs. First, we assume `$P$' so that we can use the conditional introduction rule at the end of the proof. Then, we assume `$\enot Q$' so that we can use the negation elimination rule and get `$Q$' on the last line of the first subproof.

\begin{proof}
	\hypo{p1}{\enot P} \pr{}	
	\open
		\hypo{p}{P}	\as{}
		\open
			\hypo{nq}{\enot Q} \as{}
			\have{p2}{P} \reit{p}
			\have{np}{\enot P} \reit{p1}
		\close
		\have{q}{Q} \ne{nq-np}
	\close
	\have{c}{P \eif Q} \ci{p-q}
\end{proof}
\medskip


\begin{minipage}{10cm}
\item The proof for  `$Q \eor S, Q \eif T, S \eif T \proves T$' also requires two subproofs. 

\begin{proof}
	\hypo{p1}{Q \eor S} \pr{}
	\hypo{p2}{Q \eif T} \pr{}
	\hypo{p3}{S \eif T} \pr{}
	\open
		\hypo{nt}{\enot T} \as{}
			\open
				\hypo{nq}{\enot Q} \as{}
				\have{s}{S} \oe{p1,nq}
				\have{t}{T} \ce{p3,s}
				\have{nt2}{\enot T} \reit{nt}
			\close
		\have{q}{Q} \ne{nq-nt2}
		\have{t2}{T} \ce{p2,q}
		\have{nt3}{\enot T} \reit{nt}
	\close
	\have{c}{T} \ne{nt-nt3}
\end{proof}
\end{minipage}

\end{earg}



\section{Invalid arguments}

In this chapter, we have taken it for granted that each argument that we have encountered has been valid. The purpose of providing a proof is (1) to confirm that it is valid and (2) to show why it is valid---that is, to lay out each step that takes us from the premises to the conclusion. If an argument is invalid, however, we are stuck. It is impossible to provide a correct proof of an invalid argument using the rules given in this chapter. At the same time, not being able to provide a proof for an argument doesn't mean that the argument is invalid. Perhaps the proof is just too complicated for us to figure out. 

In chapter \ref{s:NDVeryIdea}, we discussed some reasons to prefer natural deduction to truth tables for checking that an argument is valid. To show that an argument is invalid, however, creating a truth table is not merely a superior method, it is our only option.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercises for chapter on proofs

\section{Practice exercises}
\setcounter{ProbPart}{0}

\begin{small}

\problempart\label{practice-proofs-1}
Give a proof for each argument using the rules from \S\S \ref{s:conj-rule} -- \ref{s:bi-rules}.
\begin{earg}
\item $\enot P \eif (Q \eor P), \enot P \proves Q$\\
\textit{Note:} After you list the premises on lines 1 and 2, notice that $\enot P$ is the antecedent of the sentence on line 1 and $\enot P$ is by itself on line 2. This means using the conditional elimination rule is an option.
\medskip
\item $P \eif (Q \eor \enot P), P \proves Q$\\
\textit{Note:} This problem is similar to the one right above, but it's not exactly the same. If you're not sure how to do this one, look at the subsection on the double negation rule (pp.~\pageref{subsection-DN} - \pageref{DN-box}).
\medskip
\item $D \eand H, H \eiff J  \proves J \eor N$\smallskip 

\item $R \eand S, (S \eor Q) \eif T \proves T$\\
\textit{Note:} Since $(S \eor Q)$ is the antecedent of the conditional on line 2, to use the conditional elimination rule, you need to have $(S \eor Q)$ on a line by itself. Since $(S \eor Q)$ is a disjunction, you will need to use the disjunction introduction rule to get it. (Check how that rule works if you don't remember: p.~\pageref{di-rule-box}.)
\medskip

\item $G \eand (H \eand J), (H \eor M) \eif K \proves K$\smallskip
\item $P \eand (Q \eor R), P \eif \enot R \proves Q$\smallskip
\item $(P \eor \enot Q) \eiff R, R \eand Q \proves P \eand R$\smallskip
\item $(R \eand T) \eif Q, R \eor \enot P, P \eand T \proves Q$\\
\textit{Note:} After you list the premises for this proof, you'll see that the conclusion is the consequent of the conditional on line 1. To use the conditional elimination rule, you need to use the two other sentences (on lines 2 and 3) to get the antecedent on a line by itself. That will take several steps. Think about whether you need to use the double negation rule for one of those steps. 
\medskip

\item $S \eif T, Q \eand \enot R, \enot R \eiff (T \eif S) \proves S \eiff T $\smallskip
\item $(L \eor M) \eif N, P \eiff N, L \proves L \eand P$\smallskip
\item $(P \eand R) \eiff (S \eor T), P \eif Q, T \proves Q$\smallskip
\item $S \eif T, T \eif S, T \eiff (T \eiff S) \proves S$\smallskip
\end{earg}


\problempart
Give a proof for each argument.
\begin{earg}
\item $P \eif (Q \eif R) \proves (P \eand Q) \eif R$\smallskip
\item $Q \eif R \proves (Q \eand S) \eif (R \eor T)$\smallskip 
\item $M \eand (\enot N \eif \enot M) \proves (N \eand M) \eor \enot M$\smallskip
\item $(Z\eand K)\eiff(Y\eand M), D\eand(D\eif M) \proves Y\eif Z$\smallskip
\item $C\eif(E\eand G), \enot C \eif G \proves G$\smallskip
\item $\enot(P \eif Q) \proves \enot Q$\smallskip
\item $S \eiff T \proves S \eiff (T \eor S)$\smallskip 
\item $D \eor F, D \eif G, F \eif H \proves G \eor H$\smallskip
%\item $(W \eor X) \eor (Y \eor Z), X\eif Y, \enot Z \proves W\eor Y$
\end{earg}


%\problempart
%If you know that a proof can be given for $\meta{A}\proves\meta{Q}$ (that is, you know that the argument is valid), then is it possible to know if $(\meta{A} \eand \meta{C}) \proves \meta{B}$ is valid? Is it possible to know if $(\meta{A} \eif \meta{B}) \eif \meta{B} \proves \meta{B}$ is valid? Explain your answers.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  answers


\section{Answers}
\setcounter{ProbPart}{0}

\problempart
\begin{earg}
\item $\enot P \eif (Q \eor P), \enot P \proves Q$
\begin{proof}
	\hypo{p1}{\enot P \eif (Q \eor P)} \pr{}
	\hypo{p2}{\enot P} \pr{}
	\have{qp}{Q \eor P} \ce{p1,p2}
	\have{q}{Q} \oe{p2,qp}
\end{proof}
\medskip

\item $P \eif (Q \eor \enot P), P \proves Q$\\
\begin{proof}
	\hypo{p1}{P \eif (Q \eor \enot P)} \pr{}
	\hypo{p2}{P} \pr{}
	\have{qp}{Q \eor \enot P} \ce{p1,p2}
	\have{nnp}{\enot\enot P} \dn{p2} 
	\have{q}{Q} \oe{qp,nnp}
\end{proof}
\medskip

%\filbreak

\item $D \eand H, H \eiff J  \proves J \eor N$ 
\begin{proof}
	\hypo{p1}{D \eand H} \pr{}
	\hypo{p2}{H \eiff J} \pr{}
	\have{h}{H} \ae{p1}
	\have{j}{J} \be{p2,h}
	\have{jn}{J \eor N} \oi{j}
\end{proof}
\medskip


\item $R \eand S, (S \eor Q) \eif T \proves T$
\begin{proof}
	\hypo{p1}{R \eand S} \pr{}
	\hypo{p2}{(S \eor Q) \eif T} \pr{}
	\have{s}{S} \ae{p1}
	\have{svq}{S \eor Q} \oi{s}
	\have{t}{T} \ce{p2,svq}
\end{proof}
\medskip
%\filbreak

\item $G \eand (H \eand J), (H \eor M) \eif K \proves K$
\smallskip

\textit{Note:} In this proof, just like in the previous one, you need to use the disjunction introduction rule to get an antecedent---in this case, `$(H~\eor~M)$'---on a line by itself (so that you can then use the conditional elimination rule). This is a good trick to remember. 
\begin{proof}
	\hypo{p1}{G \eand (H \eand J)} \pr{}
	\hypo{p2}{(H \eor M) \eif K} \pr{}
	\have{hj}{H \eand J} \ae{p1}
	\have{h}{H} \ae{hj}
	\have{hj2}{H \eor M} \oi{h}
	\have{k}{K} \ci{p2,hj2}
\end{proof}
\medskip


\item $P \eand (Q \eor R), P \eif \enot R \proves Q$
\begin{proof}
	\hypo{p1}{P \eand (Q \eor R)} \pr{}
	\hypo{p2}{P \eif \enot R} \pr{}
	\have{p}{P} \ae{p1}
	\have{nr}{\enot R} \ci{p2,p}
	\have{qr}{Q \eor R} \ae{p1}
	\have{q}{Q} \oe{nr,qr}
\end{proof}
\medskip

\noindent\begin{minipage}{0.99\textwidth}
\item $(P \eor \enot Q) \eiff R, R \eand Q \proves P \eand R$\smallskip
\begin{proof}
\hypo{p1}{(P \eor \enot Q) \eiff R} \pr{}
\hypo{p2}{R \eand Q} \pr{}
\have{r}{R} \ae{p2}
\have{pq}{P \eor \enot Q} \be{p1,r}
\have{q}{Q} \ae{p2}
\have{nnq}{\enot\enot Q} \dn{q}
\have{p}{P} \oe{pq,nnq}
\have{con}{P \eand R} \ai{r,p}
\end{proof}
\medskip
\end{minipage}


\item $(R \eand T) \eif Q, R \eor \enot P, P \eand T \proves Q$\smallskip
\begin{proof}
\hypo{p1}{(R \eand T) \eif Q} \pr{}
\hypo{p2}{R \eor \enot P} \pr{}
\hypo{p3}{P \eand T} \pr{}
\have{p}{P} \ae{p3}
\have{nnp}{\enot\enot P} \dn{p}
\have{r}{R} \oe{p2,nnp}
\have{t}{T} \ae{p3}
\have{rt}{R \eand T} \ai{r,t}
\have{con}{Q} \ce{p1, rt}
\end{proof}
\medskip


\item $S \eif T, Q \eand \enot R, \enot R \eiff (T \eif S) \proves S \eiff T $\smallskip
\begin{proof}
\hypo{p1}{S \eif T} \pr{}
\hypo{p2}{Q \eand \enot R} \pr{}
\hypo{p3}{\enot R \eiff (T \eif S)} \pr{}
\have{nr}{\enot R} \ae{p2}
\have{ts}{T \eif S} \be{p3,nr}
\have{con}{S \eiff T} \bi{p1,ts}
\end{proof}
\medskip


\item $(L \eor M) \eif N, P \eiff N, L \proves L \eand P$\smallskip
\begin{proof}
\hypo{p1}{(L \eor M) \eif N} \pr{}
\hypo{p2}{P \eiff N} \pr{}
\hypo{p3}{L} \pr{}
\have{lm}{L \eor M} \oi{p3}
\have{n}{N} \ce{p1,lm}
\have{p}{P} \be{p2,n}
\have{con}{L \eand P} \ai{p3,p}
\end{proof}
\medskip

\item $(P \eand R) \eiff (S \eor T), P \eif Q, T \proves Q$
\begin{proof}
\hypo{p1}{(P \eand R) \eiff (S \eor T)} \pr{}
\hypo{p2}{P \eif Q} \pr{}
\hypo{p3}{T} \pr{}
\have{svt}{S \eor T} \oi{p3}
\have{pr}{P \eand R} \be{p1,svt}
\have{p}{P} \ae{pr}
\have{q}{Q} \ce{p2,p}
\end{proof}
\medskip


\item $S \eif T, T \eif S, T \eiff (T \eiff S) \proves S$
\begin{proof}
\hypo{p1}{S \eif T} \pr{}
\hypo{p2}{T \eif S} \pr{}
\hypo{p3}{T \eiff (T \eiff S)} \pr{}
\have{ts}{T \eiff S} \bi{p1,p2}
\have{t}{T} \be{p3,ts}
\have{s}{S} \ce{p2,t}
\end{proof}

\medskip

\end{earg}


%%% Part B

\filbreak

\problempart
\begin{earg}
\item $P \eif (Q \eif R) \proves (P \eand Q) \eif R$
\begin{proof}
	\hypo{p1}{P \eif (Q \eif R)} \pr{}
	\open
	\hypo{as}{P \eand Q} \as{}
	\have{p}{P} \ae{as}
	\have{qr}{Q \eif R} \ce{p1,p}
	\have{q}{Q} \ae{as}
	\have{r}{R} \ce{qr,q}
	\close
	\have{con}{(P \eand Q) \eif R} \ci{as-r}
\end{proof}
\medskip


\item $Q \eif R \proves (Q \eand S) \eif (R \eor T)$ 
\begin{proof}
	\hypo{p1}{Q \eif R} \pr{}
	\open
	\hypo{as}{Q \eand S} \as{}
	\have{q}{Q} \ae{as}
	\have{r}{R} \ce{p1,q}
	\have{rt}{R \eor T} \oi{r}
	\close
	\have{con}{(Q \eand S) \eif (R \eor T)} \ci{as-rt}
\end{proof}
\medskip



\item $M \eand (\enot N \eif \enot M) \proves (N \eand M) \eor \enot M$
\begin{proof}
\hypo{MnNnM}{M \eand (\enot N \eif \enot M)} \pr{}
\have{M}{M}\ae{MnNnM}
\have{nNnM}{\enot N \eif \enot M}\ae{MnNnM}
\open
	\hypo{nN}{\enot N} \as{}
	\have{nM}{\enot M}\ce{nNnM, nN}
	\have{M2}{M} \reit{M}
\close
\have{N}{N} \ne{nN-M2}
\have{NM}{N \eand M}\ai{M,N}
\have{con}{(N \eand M) \eor \enot M}\oi{NM}	
\end{proof}
\medskip

%\filbreak

\item $(Z\eand K)\eiff(Y\eand M), D\eand(D\eif M) \proves Y\eif Z$
\begin{proof}
\hypo{zkym}{(Z \eand K) \eiff (Y \eand M)} \pr{}
\hypo{ddm}{D \eand (D \eif M)} \pr{}
\have{d}{D}\ae{ddm}
\have{dm}{D \eif M}\ae{ddm}
\have{m}{M}\ce{d,dm}
\open
	\hypo{y}{Y} \as{}
	\have{ym}{Y \eand M}\ai{m,y}
	\have{zk}{Z \eand K}\be{zkym, ym}
	\have{z}{Z}\ae{zk}
\close
\have{yz}{Y \eif Z}\ci{y-z}
\end{proof}
\medskip


\item $C\eif(E\eand G), \enot C \eif G \proves G$
\begin{proof}
\hypo{ceg}{C \eif (E \eand G)} \pr{}
\hypo{ncg}{\enot C \eif G} \pr{}
\open
        \hypo{ng}{\enot G} \as{}
        \open
                \hypo{c}{C} \as{}
                \have{eg}{E \eand G}\ce{ceg, c}
                \have{g1}{G}\ae{eg}
                \have{ng2}{\enot G} \reit{ng}
        \close
        \have{nc}{\enot C}\ni{c-ng2}
        \have{g2}{G}\ce{ncg, nc}
        \have{ng3}{\enot G} \reit{ng}
\close
\have{con}{G}\ne{ng-ng3}
\end{proof}
\medskip

\noindent\begin{minipage}{0.99\textwidth}
\item $\enot(P \eif Q) \proves \enot Q$
\begin{proof}
	\hypo{p1}{\enot(P \eif Q)} \pr{}
	\open
	\hypo{as}{Q} \as{}
		\open
		\hypo{as2}{P} \as{}
		\have{q}{Q} \reit{as}
		\close
	\have{pq}{P \eif Q} \ci{as2-q}
	\have{npq}{\enot(P \eif Q)} \reit{p1}
	\close
	\have{con}{\enot Q} \ni{as-npq}
\end{proof}
\medskip
\end{minipage}

\item $S \eiff T \proves S \eiff (T \eor S)$ 
\begin{proof}
	\hypo{p1}{S \eiff T} \pr{}
	\open
	\hypo{as1}{S} \as{}
	\have{t}{T} \be{p1,as1}
	\have{ts}{T \eor S} \oi{t}
	\close
	\have{c1}{S \eif (T \eor S)} \ci{as1-ts}
	\open
	\hypo{as2}{T \eor S} \as{}
		\open
		\hypo{as3}{\enot S} \as{}
		\have{t2}{T} \oe{as2,as3}
		\have{s2}{S} \be{p1,t2}
		\have{ns}{\enot S} \reit{as3}
		\close
	\have{s}{S} \ne{as3-ns}
	\close
	\have{c2}{(T \eor S) \eif S} \ci{as2-s}
%	\have{con1}{(S \eif (T \eor S)) \eand ((T \eor S) \eif S)} \ai{c1,c2}
	\have{con2}{S \eiff (T \eor S)} \bi{c1,c2}
\end{proof}
\medskip


\noindent\begin{minipage}{0.99\textwidth}
\item $D \eor F, D \eif G, F \eif H \proves G \eor H$ 
\begin{proof}
	\hypo{p1}{D \eor F} \pr{}
	\hypo{p2}{D \eif G} \pr{}
	\hypo{p3}{F \eif H} \pr{}
	\open
	\hypo{as}{\enot(G \eor H)} \as{}
		\open
		\hypo{as2}{\enot D} \as{}
		\have{f}{F} \oe{p1,as2}
		\have{h}{H} \ce{p3,f}
		\have{gh}{G \eor H} \oi{h}
		\have{ngh}{\enot (G \eor H)} \reit{as}
		\close
	\have{d}{D} \ne{as2-ngh}
	\have{g}{G} \ce{p2,d}
	\have{gh2}{G \eor H} \oi{g}
	\have{ngh2}{\enot (G \eor H)} \reit{as}
	\close
	\have{con}{G \eor H} \ne{as-ngh2}
\end{proof}
\end{minipage}

\begin{comment}
\filbreak
\item $(W \eor X) \eor (Y \eor Z), X\eif Y, \enot Z \proves W\eor Y$
\begin{proof}
\hypo{p1}{(W \eor X ) \eor (Y \eor Z )} \pr{}
\hypo{p2}{X \eif Y} \pr{}
\hypo{p3}{\enot Z} \pr{}
\open
	\hypo{nwy}{\enot (W \eor Y)} \as{}
		\open
		\hypo{nwx}{\enot (W \eor X)} \as{}
		\have{yz}{Y \eor Z} \oe{p1,nwx}
		\have{y}{Y} \oe{p3,yz}
		\have{wy}{W \eor Y} \oi{y}
		\have{nwy2}{\enot (W \eor Y)} \reit{nwy}
		\close
	\have{wx}{W \eor X} \ne{nwx-nwy2}
		\open
		\hypo{x}{X} \as{}
		\have{y2}{Y} \ce{p2,x}
		\have{wy}{W \eor Y} \oi{y2}
		\have{nwy3}{\enot (W \eor Y)} \reit{nwy}
		\close
	\have{nx}{\enot X} \ni{x-nwy3}
	\have{w}{W} \oe{wx,nx}
	\have{wy2}{W \eor Y} \oi{w}
	\have{nwy4}{\enot (W \eor Y)} \reit{nwy}
	\close
\have{con}{W \eor Y} \ne{nwy-nwy4}
\end{proof}
\end{comment}

\end{earg}

\end{small}

%\problempart
%If $\meta{A} \proves \meta{B}$ is valid, then $(\meta{A} \eand \meta{C})\proves \meta{B}$ is valid. We know this because if $\meta{A} \proves \meta{B}$, then there is some proof with assumption $\meta{A}$ that ends with $\meta{B}$, and no undischarged assumptions other than $\meta{A}$. Now, if we start a proof with assumption $(\meta{A} \eand \meta{C})$, we can obtain $\meta{A}$ by $\eand$E. We can now copy and paste the original proof of $\meta{B}$ from $\meta{A}$, adding 1 to every line number and line number citation. The result will be a proof of $\meta{B}$ from assumption $\meta{A}$.

%We also know that $(\meta{A} \eif \meta{B}) \eif \meta{A} \proves \meta{B}$ is valid. Since, there is some proof with assumption $\meta{A}$ that ends with $\meta{B}$, we can assume \meta{A}, derive \meta{B}, and then use the conditional-introduction rule to get $(\meta{A} \eif \meta{B})$. We use the condtional-elimination rule on $(\meta{A} \eif \meta{B})$ and the premise, and get \meta{B}, the conclusion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% CHAPTER 16

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% CHAPTER 17

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Proofs in Carnap}\label{s:Carnap-proofs}

Creating proofs in Carnap is not difficult. To type the connectives, use the symbols on the right in table \ref{symbols-Carnap}. 

\begin{table*}\centering\sffamily\footnotesize
\ra{1.25}
\begin{tabular}{@{}l l@{}}\toprule
\textth{TFL operator}\quad\quad & \textth{in Carnap} \\\midrule
\quad	\enot 	&\quad $ \sim$\\
\quad	\eand 	&\quad \&\\
\quad	\eor 	&\quad v (lowercase v)\\
\quad	\eif 	&\quad $-$$>$ (dash, greater than sign)\\
\quad	\eiff 	&\quad $<$$-$$>$\\
\bottomrule
\end{tabular}
\caption{}\label{symbols-Carnap}
\end{table*}


Carnap will number the lines automatically. After the TFL  sentence on each line, there has to be a colon (`:') before the `PR', `AS', or the rule. Carnap is flexible with the spacing on a line, but as a guideline, put a tab space between the sentence and `:PR', `:AS', or the rule (:$\eif$E, :$\eor$I, etc.). Also indent subproofs with a tab space. (Carnap will let you use more or fewer spaces, but a subproof has to be indented some amount.)

To create a proof, you are given an interface like the one shown in figure \ref{fig:proof-1a}. As you can see, the argument is given at the top. In this case, the premises are $P \eif \enot Q$ and $R \eand P$, and the conclusion is $\enot Q$. (The premises are separated by commas. The premises and the conclusion are separated by the turnstile (`$\proves$').)

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
%\centering
\includegraphics[width=9.5cm]{textbook--1a.PNG}
\caption{}
\label{fig:proof-1a}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Begin by listing the premises, and don't forget to put `:PR' after each one. If there is a problem with a line---the sentence isn't formed correctly, the rule you've cited isn't being used correctly, or there's some other mistake---Carnap will put \textsf{?} or {\fontencoding{U}\fontfamily{futs}\selectfont\char 49\relax} at the end of the line. When the line is ok, you will get a `+'. 
% \danger \warning
% Once you complete each line, Carnap will give you the typographically correct version on the right (figure \ref{fig:proof-1b}). 
We finish this proof using the $\eand$E and $\eif$E rules (figure \ref{fig:proof-1c}). When the proof is correct, the box containing the argument will turn green, and the proof can be submitted. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
%\centering
\includegraphics[width=9.5cm]{textbook--1b.PNG}
\caption{}
\label{fig:proof-1b}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
%\centering
\includegraphics[width=9.5cm]{textbook--1c.PNG}
\caption{}
\label{fig:proof-1c}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our next example, $(P \eor Q) \proves (\enot P \eif Q)$, requires a subproof. We begin as before. To create the subproof, put a tab space before $\enot P$ and put `:AS' at the end of the line (figure \ref{fig:proof-2a}). Since the next line is also part of the subproof, we again need a tab before the $Q$. We end the subproof (and discharge the assumption) with the $\eif$I rule. $\enot P \eif Q$ is not indented (so no tabs or spaces before the $\enot P$). That's the conclusion, and so if everything is correct, Carnap will give you the green bar and you can submit the proof (figure \ref{fig:proof-2b}). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
%\centering
\includegraphics[width=9.5cm]{textbook--2a.PNG}
\caption{}
\label{fig:proof-2a}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
%\centering
\includegraphics[width=9.5cm]{textbook--2b.PNG}
\caption{}
\label{fig:proof-2b}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}
%\begin{subfigure}[hb]{0.5\textwidth}
%\centering
%\includegraphics[angle=90, height=13cm]{textbook--2a.PNG}
%\caption{}
%\label{fig:proof-2a}
%\end{subfigure}
%\begin{subfigure}[hb]{0.5\textwidth}
%\centering
%\includegraphics[angle=90, height=13cm]{textbook--2b.PNG}
%\caption{}
%\label{fig:proof-2b}
%\end{subfigure}
% \caption{}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%



Although creating proofs in Carnap is not difficult, you do have to be careful. Creating a program that can verify proofs that use only the rules of derivation given in chapter \ref{s:BasicTFL} is relatively simple because there are only a small number of rules and, to produce proofs of valid arguments, we follow those rules very strictly. But, as a consequence, Carnap is not designed to understand what you are trying to do if you deviate from the rules, even if it is a minor deviation or an innocent mistake. So, some reminders:
\begin{enumerate}
\itemsep-.3mm
	\item As long as `$\enot$' is not the main logical operator, you can drop the outermost parentheses. All other parentheses have to be used. 
	\item Capitalize `PR', `AS',`E', `I' (in the rules), and all atomic sentences.
	\item Don't forget the `:' right before PR, AS, or the rule that you are citing. 
	\item There is no space between the $\eand$, $\eor$, $\eif$, $\eiff$, or $\enot$ and the `E' or `I'.  
	\item There is a space (and no punctuation) after the `E' or `I'. 
	\item There is a comma between the two lines that have to be cited for $\eand$I, $\eor$E, $\eif$E, and $\eiff$E (e.g., `$\eif$E 2,4').
	\item There is a dash between the two lines that have to be cited for $\eif$I, $\enot$I, and $\enot$E (e.g., `$\enot$E 4-6').
%	\item The $\eiff$I rule requires dashes and a comma. (Check the format on p.~\pageref{eiff-I}.)
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\chapter{Some strategies}
There is no simple recipe for constructing proofs, and there is no substitute for practice. Here, however, are some questions to ask yourself and some strategies to keep in mind.

\begin{earg}
\item[\ex{18-1}] Do you know all of the rules? \textbf{If you don’t have them memorized yet, then they should be written on a sheet of paper that you have next to you while you’re working.}
\medskip

\item[\ex{18-2}] Are there steps that you can take without making an assumption? If yes, is it worth taking those steps?
\medskip

\item[\ex{18-3}] If you’re not sure how to proceed, but you can do conjunction elimination, conditional elimination, disjunction elimination, or biconditional elimination, then do them just to see what happens.
\end{earg}

\noindent The theme for 4 -- 7 is ``think ahead.'' Some amount of trial and error is often necessary, but, especially when you are constructing a proof that will contain a subproof, it's important to think about how each step that you take will affect the later parts of your proof.

\begin{earg}
\item[\ex{18-4}] If an assumption is needed, is it for $\eif$I, $\enot$I, or $\enot$E? \textbf{Don't make an assumption if you don't know which of these rules you plan to use when you close the subproof.}
\medskip

\item[\ex{18-5}] If an assumption is needed, what should it be? (If you want to get $P \eif Q$, then you’re going to use $\eif$I and your assumption should be $P$.)
\medskip

\item[\ex{18-6}] If you make an assumption, then you should know what you want on either the last line or the last two lines of your subproof.
\begin{earg}
\item[a.] If you’re using $\eif$I, then you will need the consequent of the conditional on the last line of the subproof. 
\item[b.] If you’re using $\enot$I or $\enot$E, then you need a contradiction on the last two lines of your subproof, although that can be any contradiction. It doesn't have to be related to the assumption.
\end{earg}
\medskip

\item[\ex{18-3a}] Sometimes it is useful to work backwards from the conclusion. The conclusion, of course, will be the last line of your proof, and you can, if you wish, put it at the bottom of the proof anytime. For example, let's say that you need to provide a proof for this argument: $P \eif (\enot Q \eif R) \proves (P \eand \enot Q) \eif R$. You can begin this way:
\begin{proof}
	\hypo{pqr}{P \eif (\enot Q \eif R)} \pr{}
		\have[\ ]{pq}{}
		\have[\ ]{p}{}
		\have[\ ]{qr}{}
		\have[\ ]{q}{}
	\have[\ ]{con}{(P \eand \enot Q) \eif R}
\end{proof}
\medskip

Knowing that you need to arrive at a conditional, you also know these three things: (1) you need to use the conditional-introduction rule, (2) what your assumption should be, and (3) what will be on the last line of your subproof.

\begin{proof}
	\hypo{pqr}{P \eif (\enot Q \eif R)} \pr{}
	\open
		\hypo{pq}{P \eand \enot Q}\as{}
		\have[\ ]{p}{}
		\have[\ ]{qr}{}
		\have[\ ]{r}{R}
	\close
	\have[\ ]{con}{(P \eand \enot Q) \eif R}\ci{}
\end{proof}

Sketching out a proof in this way is easy to do when you are writing on paper. If you are doing it in Carnap, be careful about the spacing that you put on each blank line.

\medskip

\item[\ex{18-7}] The negation introduction and negation elimination rules are a last resort. Use them when you can't use any of the other rules. When you do use them, always have in mind that, when you complete the subproof, you will have the opposite of the assumption. Hence, a good guideline is to make the assumption the opposite of the conclusion. 

(If you have to make two assumptions---and both assumptions will be discharged with one of these rules---this guideline only applies to the first assumption. Determining the best choice for a second assumption sometimes takes a little trial and error.)  
\medskip

\item[\ex{18-8}] \textbf{Persist}. Try different things. If one approach fails, then try something else.
\end{earg}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Proof-theoretic concepts}\label{s:ProofTheoreticConcepts}

\section{Theorems}\label{s:theorems}

You are familiar with arguments that have this form:
$$\meta{A}_1, \meta{A}_2, \ldots, \meta{A}_n \proves \meta{C}$$
We may also, however, have a sentence for which it is possible to give a proof with no premises: ${} \proves \meta{C}$. In this case, we say that $\meta{C}$ is a \define{theorem}.

\begin{factboxy}{Theorem}\label{def:syntactic_tautology_in_sl}
$\meta{C}$ is a \define{theorem} if and only if ~ $\proves \meta{C}$
\end{factboxy}

One such sentence is `$\enot (P \eand \enot P)$'. To show that this sentence is a theorem, we give a proof that has no premises and no undischarged assumptions. To get started, we do, however, have to make an assumption. We will assume `$P \eand \enot P$'. Once we show that this assumption leads to contradiction, we can discharge it and we will have `$\enot (P \eand \enot P)$'. This is the proof:
	\begin{proof}
		\open
			\hypo{con}{P \eand \enot P}
			\have{a}{P}\ae{con}
			\have{na}{\enot P}\ae{con}
		\close
		\have{lnc}{\enot (P \eand \enot P)}\ni{con-na}
	\end{proof}
This theorem, `$\proves \enot (P \eand \enot P)$' is an instance of what is sometimes called \emph{the law of non-contradiction}.

To show that a sentence is a theorem, we just have to find a suitable proof. On the other hand, it is not possible to show that a sentence is \emph{not} a theorem this same way. To show that a sentence is not a theorem with our natural deduction system, we would have to demonstrate, not just that certain proof strategies fail, but that \emph{no} proof is possible. Even if we fail in trying to give a proof for a sentence in a thousand different ways, perhaps the proof is just too long and complex for us to figure out. 


\section{Equivalent, consistent, and inconsistent}

In \S\ref{equivalence--tt}, we defined \textit{equivalent} in terms of truth tables, namely, if two sentences have the same truth value on every line of a truth table, then they are equivalent. We can also show that two sentences are equivalent using our natural deduction system. To indicate that we have shown that the two sentences are equivalent with a derivation (or actually with two derivations), we will call this equivalence \define{provably equivalent}. 

\begin{factboxy}{Provably equivalent}
Two sentences \meta{A} and \meta{B} are \define{provably equivalent} iff each can be derived from the other. I.e., $\meta{A}\proves\meta{B}$ and $\meta{B}\proves\meta{A}$.\\
(Equivalently, \meta{A} and \meta{B} are \define{provably equivalent} if $\proves \meta{A} \eiff \meta{B}$.)
\end{factboxy}
        
As in the case of showing that a sentence is a theorem, it is relatively easy to show that two sentences are provably equivalent: it just requires a pair of proofs. Showing that sentences are \emph{not} provably equivalent is not possible for t he same reason that it isn't possible to show that a sentence is not a theorem. Even if we fail to produce two proofs showing that two sentences are provably equivalent, that doesn't mean that the proofs don't exist. It just means that we've failed to figure out what they are. 

We also, in \S\ref{consistency--tt}, defined \textit{jointly inconsistent} using truth tables: sentences are jointly inconsistent if there is no line on a truth table where they are all true. Again, we can show that two or more sentences are jointly inconsistent with our natural deduction system. 

\begin{factboxy}{Provably inconsistent}
The sentences $\meta{A}_1,\meta{A}_2,\ldots, \meta{A}_n$ are \define{provably inconsistent} iff, from them, a contradiction can be derived. I.e.\ $\meta{A}_1,\meta{A}_2,\ldots, \meta{A}_n \proves (\meta{B} \eand \enot \meta{B})$.
%\tcblower
%If they are not \define{inconsistent}, then $\meta{A}_1,\meta{A}_2,\ldots, \meta{A}_n$ are \define{provably consistent}.
\end{factboxy}
        
To show that a set of sentences are provably inconsistent, we use the sentences as premises and then derive a contradiction. (Any contradiction will do.) For instance, this proof demonstrates that $P \eand Q$ and $\enot P \eor \enot Q$ are provably inconsistent.

	\begin{proof}
	\hypo{p1}{P \eand Q} \pr{}
	\hypo{p2}{\enot P \eor \enot Q} \pr{}
	\have{p}{P} \ae{p1}
	\have{np}{\enot \enot P} \dn{p}
	\have{nq}{\enot Q} \oe{p2,np}
	\have{q}{Q} \ae{p1}
	\have{con}{Q \eand \enot Q} \ai{nq,q}
	\end{proof}

Showing that some set of sentences are \textit{not} provably inconsistent is, as you might guess at this point, not possible. Doing so would require showing, not just that we have failed to derive a contradiction from a set a sentences, but that no such derivation is possible.

Table \ref{table.one-mult-proofs} summarizes what we have covered in this chapter. As we will discuss in the next chapter, when the presence (or the absence) of a logical property cannot be demonstrated using our natural deduction system, we have to resort to using a truth table.

\begin{table*}\centering\sffamily\footnotesize
\ra{1.25}
\begin{tabular}{@{}l l l@{}}\toprule
\textth{To check} & \textth{that it is} & \textth{that it is not}\\\midrule
theorem & one proof & \textit{not possible with proofs}\\
equivalent & two proofs & \textit{not possible with proofs}\\
inconsistent &  one proof  & \textit{not possible with proofs}\\
consistent & \textit{not possible with proofs} & one proof\\
\bottomrule
\end{tabular}
\caption{This table summarizes what is required to check each of these logical notions.}\label{table.one-mult-proofs}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercises for chapter on proof theoretic concepts

\newpage

\section{Practice exercises}
\setcounter{ProbPart}{0}

\begin{small}

\problempart
Give a proof for each of these theorems.
\begin{earg}
\item $\proves O \eif O$\smallskip
\item $\proves S \eif (S \eor R)$\smallskip
\item $\proves N \eor \enot N$\smallskip
\item $\proves \enot((R \eor T) \eand (\enot R  \eand \enot T))$\smallskip
\item $\proves (R \eiff M) \eif (M \eif R)$\smallskip
%\item $\proves (P \eif Q) \eor (Q \eif P)$\smallskip
% \item $\proves ((A \eif B) \eif A) \eif A$\smallskip 
\end{earg}


\problempart
Show that each of the following pairs of sentences are provably equivalent. (To indicate that the inference from the premise to the conclusion goes from the first sentence to the second and vice versa, we use the symbols $\leftproves\proves$.)
\begin{earg}
\item $T\eif S \leftproves\proves \enot S \eif \enot T$
\item $R \eif Q \leftproves\proves \enot(R \eand \enot Q)$
\end{earg}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  answers


\section{Answers}
\setcounter{ProbPart}{0}

\problempart


\begin{earg}

\noindent\begin{minipage}{0.99\textwidth}
\item $\proves O \eif O$
\begin{proof}
\open
	\hypo{o}{O} \as{}
	\have{o2}{O}\reit{o}
\close
\have{con}{O \eif O}\ci{o-o2}
\end{proof}
\bigskip
\end{minipage}

\noindent\begin{minipage}{0.99\textwidth}
\item $\proves S \eif (S \eor R)$
\begin{proof}
\open
	\hypo{s}{S} \as{}
	\have{s2}{S} \reit{s}
	\have{sr}{S \eor R} \oi{s2}
\close
\have{con}{S \eif (S \eor R)}\ci{s-sr}
\end{proof}
\medskip
\end{minipage}


\item $\proves N \eor \enot N$
\begin{proof}
\open
        \hypo{ncon}{\enot (N \eor \enot N)} \as{}
        \open
                \hypo{n}{N} \as{}
                \have{nnn1}{N \eor \enot N}\oi{n}
                \have{ncon2}{\enot (N \eor \enot N)} \reit{ncon}
        \close
        \have{nn}{\enot N} \ni{n-ncon2}
        \have{nnn2}{N \eor \enot N} \oi{nn}
        \have{ncon3}{\enot (N \eor \enot N)} \reit{ncon}
\close
\have{con}{N \eor \enot N} \ne{ncon-ncon3}
\end{proof}
\medskip


\noindent\begin{minipage}{0.99\textwidth}
\item $\proves \enot((R \eor T) \eand (\enot R  \eand \enot T))$
\begin{proof}
\open
        \hypo{ncon}{(R \eor T) \eand (\enot R  \eand \enot T)} \as{}
        \have{rt}{R \eor T} \ae{ncon}
        \have{nrnt}{\enot R  \eand \enot T} \ae{ncon}
        \have{nr}{\enot R} \ae{nrnt}
        \have{nt}{\enot T} \ae{nrnt}
        \have{t}{T} \oe{rt,nr}
\close
\have{con}{\enot((R \eor T) \eand (\enot R  \eand \enot T))} \ni{ncon-t}
\end{proof}
\medskip
\end{minipage}

\noindent\begin{minipage}{0.99\textwidth}
\item $\proves (R \eiff M) \eif (M \eif R)$
\begin{proof}
\open
        \hypo{rm}{R \eiff M} \as{}
        	\open
        	\hypo{m}{M} \as{}
        	\have{r}{R} \be{rm,m}
        \close
        \have{mr}{M \eif R} \ci{m-r}
\close
\have{con}{(R \eiff M) \eif (M \eif R)} \ci{rm-mr}
\end{proof}
\medskip
\end{minipage}

\begin{comment}
\begin{minipage}{10cm}
\item $\proves (P \eif Q) \eor (Q \eif P)$
\begin{proof}
\open
        \hypo{ncon}{\enot ((P \eif Q) \eor (Q \eif P))} \as{}
        \open
        		\hypo{p}{P} \as{}
        		\open
        			\hypo{nq}{\enot Q} \as{}
        			\open
        				\hypo{q}{Q} \as{}
        				\have{p2}{P} \reit{p}
				\close
				\have{qp}{Q \eif P} \ci{q-p2}
				\have{pqqp}{(P \eif Q) \eor (Q \eif P)} \oi{qp}
				\have{ncon2}{\enot ((P \eif Q) \eor (Q \eif P))} \reit{ncon}
			\close
			\have{q}{Q} \ne{nq-ncon2}
		\close
		\have{pq}{P \eif Q} \ci{p-q}
		\have{pqqp2}{(P \eif Q) \eor (Q \eif P)} \oi{pq}
		\have{ncon2}{\enot ((P \eif Q) \eor (Q \eif P))} \reit{ncon}
\close
\have{con}{(P \eif Q) \eor (Q \eif P)} \ne{ncon-ncon2}
\end{proof}
\bigskip
\end{minipage}
\end{comment}

%\filbreak
%\item $((A \eif B) \eif A) \eif A$ 
%\begin{proof}
%\open
%	\hypo{aba}{(A \eif B) \eif A} \as{}
%	\open 
%		\hypo{na}{\enot A} \as{}
%		\open
%		\have{nab}{\enot (A \eif B)} \as{}
%			\open
%				\hypo{a}{A} \as{}
%					\open
%					\hypo{nb}{\enot B} \as{}
%					\have{a2}{A} \reit{a}
%					\have{na2}{\enot A} \reit{na}
%				\close
%				\have{b}{B} \ne{nb-na2}
%			\close
%		\have{ab}{A \eif B}\ci{a-b}
%		\have{nab2}{\enot (A \eif B)}\reit{nab}
%	\close
%	\have{ab2}{A \eif B} \ne{nab-nab2}
%	\have{a}{A} \ce{aba,ab2}
%	\have{na3}{\enot A} \reit{na}
%\close
%\have{a2}{A} \ne{aba-na3}
%\close
%\have{con}{((A \eif B) \eif A) \eif A}\ci{aba-a2}
%\end{proof}

\end{earg}


\problempart

\begin{earg}
\item $T\eif S \leftproves\proves \enot S \eif \enot T$
\begin{proof}
\hypo{ts}{T \eif S} \pr{}
\open
	\hypo{ns}{\enot S} \as{}
		\open
		\hypo{t}{T} \as{}
		\have{s}{S} \ce{ts,t}
		\have{ns2}{\enot S} \reit{ns}
		\close
	\have{nt}{\enot T} \ni{ns-ns2}
\close
\have{nsnt}{\enot S \eif \enot T}\ci{ns-nt}
\end{proof}

\begin{proof}
\hypo{nsnt}{\enot S \eif \enot T} \pr{}
\open
	\hypo{t}{T} \as{}
	\open
		\hypo{ns}{\enot S} \as{}
		\have{nt}{\enot T}\ce{nsnt, ns}
		\have{t2}{T}\reit{t}
	\close
	\have{s}{S}\ne{ns-t2}
\close
\have{ts}{T \eif S} \ci{t-s}
\end{proof}
\bigskip

\noindent\begin{minipage}{0.99\textwidth}
\item $R \eif Q \leftproves\proves \enot(R \eand \enot Q)$
\begin{proof}
\hypo{ui}{R \eif Q} \pr{}
\open
	\hypo{uni}{R \eand \enot Q} \as{}
	\have{u}{R}\ae{uni}
	\have{ni}{\enot Q}\ae{uni}
	\have{i}{Q}\ce{ui, u}
\close
\have{con}{\enot (R \eand \enot Q)}\ni{uni-i}
\end{proof}
\end{minipage}

\begin{proof}
\hypo{con}{\enot (R \eand \enot Q)} \pr{}
\open
	\hypo{u}{R} \as{}
	\open
		\hypo{ni}{\enot Q} \as{}
		\have{uni}{R \eand \enot Q}\ai{u, ni}
		\have{red}{\enot (R \eand \enot Q)}\reit{con}
	\close
	\have{i}{Q}\ne{ni-red}
\close
\have{ui}{R \eif Q}\ci{u-i}
\end{proof}

\end{earg}

\end{small}
